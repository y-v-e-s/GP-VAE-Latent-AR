{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VAE-AR vs VAE-GP — Summary\n\nWe compare two VAE architectures that differ only in how temporal structure is encoded in the latent space.  \nVAE-GP relies on a smooth Gaussian-process prior, capturing global correlations but no causal direction, while VAE-AR induces an explicit latent autoregressive structure through Gaussian conditioning.\n\nEmpirically, both models achieve similar perplexities, but their behavior diverges at long horizons: the non-AR GP variant tends to collapse, whereas the latent-autoregressive model maintains coherent generations.  \nThis suggests that while smooth latent geometry is sufficient for local modeling, **causal structure in the latent space is crucial for long-range sequence stability**, even with a non-autoregressive decoder.\n","metadata":{}},{"cell_type":"markdown","source":"# VAE-AR vs VAE-GP  \n## Latent Autoregression versus Smooth Gaussian-Process Priors\n\nThis repository compares two closely related variational autoencoder (VAE) architectures that differ **only in the way temporal structure is modeled in the latent space**:\n\n- **VAE-AR**: a latent autoregressive model induced by a causal Gaussian-process prior.\n- **VAE-GP (non-AR)**: a VAE with a smooth Gaussian-process prior but no latent autoregression.\n\nThe goal is not to rank models by a single metric, but to understand **where their behaviors differ, what each model is good at, and which assumptions they encode about sequential structure**.\n\n---\n\n## 1. Conceptual difference\n\n### VAE-AR (Latent Autoregression)\n\n- The latent variables form a **causal sequence**:\n  \\[\n  p(z_{1:L}) = \\prod_{t=1}^L p(z_t \\mid z_{<t})\n  \\]\n- Autoregression is **not implemented by a neural recurrence**, but emerges from **Gaussian conditioning** under a GP prior.\n- Temporal directionality is an explicit **probabilistic property of the latent path**.\n\n**Interpretation:**  \nSequential structure is carried by the *latent trajectory itself*, independently of the decoder.\n\n---\n\n### VAE-GP (Non-Autoregressive)\n\n- The latent variables are jointly distributed under a **smooth Gaussian-process prior**:\n  \\[\n  p(z_{1:L}) = \\mathcal{N}(0, K)\n  \\]\n- No causal factorization is enforced in the latent space.\n- Correlations exist, but they are **global and non-directional**.\n\n**Interpretation:**  \nThe model captures *global smoothness*, but not causal or step-by-step dependence.\n\n---\n\n## 2. Empirical comparison (WikiText-2)\n\n| Aspect | VAE-AR | VAE-GP (non-AR) |\n|------|-------|----------------|\n| Validation NLL | comparable | comparable |\n| Perplexity | comparable | comparable |\n| KL divergence | **high** | **low** |\n| Latent autocorrelation | **causal, directional** | smooth, non-causal |\n| Mutual information (latent) | higher | lower |\n| Short-range generation | good | slightly better |\n| Long-range generation | **stable** | prone to collapse |\n| Training cost | similar | similar |\n| Inference speed | slightly slower | slightly faster |\n\n---\n\n## 3. What really differs\n\n### Perplexity is *not* the discriminating factor\nBoth models achieve similar token-level perplexities.  \nThis shows that **local likelihood metrics are insufficient** to assess sequential coherence.\n\n### Latent structure is the key difference\n- **VAE-GP** minimizes KL by keeping latents close to a smooth prior.\n- **VAE-AR** accepts a higher KL cost to encode **causal latent trajectories**.\n\nThis trade-off directly affects long-horizon behavior.\n\n---\n\n## 4. Strengths and limitations\n\n### VAE-AR — strengths\n- Enforces **latent causality** without token-level autoregression.\n- Produces **stable long-range generations**.\n- Explicitly models temporal structure in latent space.\n\n### VAE-AR — limitations\n- Higher KL cost.\n- Slightly more complex inference.\n- Perplexity remains higher than highly optimized Transformers.\n\n---\n\n### VAE-GP — strengths\n- Simple, smooth latent geometry.\n- Lower KL divergence.\n- Efficient for short-range or interpolation tasks.\n\n### VAE-GP — limitations\n- No causal latent mechanism.\n- Long-range generations may collapse despite good local metrics.\n\n---\n\n## 5. Takeaway\n\n**VAE-AR and VAE-GP do not solve the same problem.**\n\n- VAE-GP models *smoothness*.\n- VAE-AR models *sequential causality*.\n\nThis comparison supports the idea that **part of sequence modeling can be shifted from token-level autoregression to the probabilistic geometry of latent space**, but only if that geometry encodes causal structure.\n\n---\n\n## 6. Status\n\nThis comparison is intended as a **proof-of-concept study**.\nThe models are deliberately constrained to highlight structural effects rather than to compete with large-scale Transformer baselines.\n\n","metadata":{}},{"cell_type":"code","source":"# VAE on WikiText-2 blocks — AR PRIOR ONLY\n# \"NO-DRAMA\" + MANY METRICS + PERF/GPU\n# Run this cell alone in a fresh notebook runtime for clean GPU measurements.\n\nimport math\nimport os\nimport time\nimport json\nimport random\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, Tuple, List, Any, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n\n# Environment / warnings\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\n\n\n# Config\n\n@dataclass\nclass CFG:\n    seed: int = 0\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Data\n    dataset_name: str = \"wikitext\"\n    dataset_config: str = \"wikitext-2-raw-v1\"\n    tokenizer_name: str = \"gpt2\"\n    block_size: int = 256\n    batch_size: int = 16\n    num_workers: int = 2\n    pin_memory: bool = True\n\n    # Model\n    vocab_size: int = 50257  # overwritten after tokenizer load\n    d_model: int = 384\n    n_layers: int = 6\n    n_heads: int = 6\n    dropout: float = 0.1\n\n    # Latent\n    z_dim: int = 64\n    n_z_samples: int = 1  # MC samples for KL estimate\n\n    # Training\n    lr: float = 3e-4\n    weight_decay: float = 0.01\n    max_steps: int = 6000\n    warmup_steps: int = 300\n    grad_clip: float = 1.0\n    eval_every: int = 400\n    log_every: int = 50\n    amp: bool = True\n\n    # KL anneal\n    beta_start: float = 0.0\n    beta_end: float = 1.0\n    beta_warmup_steps: int = 1500\n\n    # AR prior\n    ar_init_rho: float = 0.95\n    ar_sigma: float = 0.5\n\n    # Eval controls\n    eval_max_batches: int = 80\n    eval_train_batches: int = 20\n    eval_gen_prompts: int = 12\n    eval_gen_max_new: int = 96\n    eval_gen_top_k: int = 50\n\n    # ECE calibration\n    ece_bins: int = 15\n\n    # Bits-back diagnostics\n    kldim_eps: float = 0.01  # threshold for \"inactive\" dims (per-token)\n\n    # MI proxy (batch-based, subsampled for cost)\n    mi_max_components: int = 512   # max components in mixture for q(z)\n    mi_max_points: int = 256       # max sample points z to evaluate\n    mi_seed: int = 0\n\n    # Rate–Distortion points\n    rd_betas: Tuple[float, ...] = (0.0, 0.1, 0.5, 1.0, 2.0)\n\n    # Logging / saving\n    out_dir: str = \"runs\"\n    run_name: str = \"wt2_latentAR\"\n\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\n\n# Dataset\n\nclass LMBlocks(Dataset):\n    def __init__(self, token_ids: List[int], block_size: int):\n        self.block_size = block_size\n        n = (len(token_ids) - 1) // block_size\n        self.data = token_ids[: n * block_size + 1]\n\n    def __len__(self):\n        return (len(self.data) - 1) // self.block_size\n\n    def __getitem__(self, idx):\n        i = idx * self.block_size\n        x = torch.tensor(self.data[i : i + self.block_size], dtype=torch.long)\n        y = torch.tensor(self.data[i + 1 : i + self.block_size + 1], dtype=torch.long)\n        return x, y\n\n\ndef tokenize_split_streaming(tokenizer, texts: List[str], chunk_chars: int = 200_000) -> List[int]:\n    ids: List[int] = []\n    buf: List[str] = []\n    cur_len = 0\n    sep = \"\\n\\n\"\n\n    for t in texts:\n        if not t:\n            continue\n        add = t + sep\n        buf.append(add)\n        cur_len += len(add)\n        if cur_len >= chunk_chars:\n            chunk = \"\".join(buf)\n            ids.extend(tokenizer.encode(chunk))\n            buf = []\n            cur_len = 0\n\n    if buf:\n        chunk = \"\".join(buf)\n        ids.extend(tokenizer.encode(chunk))\n\n    return ids\n\n\ndef load_wt2_blocks(cfg: CFG, tokenizer) -> Tuple[DataLoader, DataLoader]:\n    ds = load_dataset(cfg.dataset_name, cfg.dataset_config)\n\n    train_ids = tokenize_split_streaming(tokenizer, ds[\"train\"][\"text\"])\n    val_ids = tokenize_split_streaming(tokenizer, ds[\"validation\"][\"text\"])\n\n    train_set = LMBlocks(train_ids, cfg.block_size)\n    val_set = LMBlocks(val_ids, cfg.block_size)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=cfg.batch_size,\n        shuffle=True,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory,\n        drop_last=True,\n        persistent_workers=(cfg.num_workers > 0),\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=cfg.batch_size,\n        shuffle=False,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory,\n        drop_last=False,\n        persistent_workers=(cfg.num_workers > 0),\n    )\n    return train_loader, val_loader\n\n\n\n# Transformer blocks\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, d_model, n_heads, n_layers, dropout):\n        super().__init__()\n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.enc = nn.TransformerEncoder(layer, num_layers=n_layers)\n\n    def forward(self, x, src_key_padding_mask=None):\n        return self.enc(x, src_key_padding_mask=src_key_padding_mask)\n\n\nclass TransformerDecoderLM(nn.Module):\n    def __init__(self, d_model, n_heads, n_layers, dropout):\n        super().__init__()\n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.dec = nn.TransformerEncoder(layer, num_layers=n_layers)\n\n    def forward(self, x):\n        _, T, _ = x.shape\n        attn_mask = torch.full((T, T), float(\"-inf\"), device=x.device)\n        attn_mask = torch.triu(attn_mask, diagonal=1)\n        return self.dec(x, mask=attn_mask)\n\n\n\n# Priors: AR only\n\nclass PriorBase(nn.Module):\n    def log_p(self, z: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError\n\n    def log_p_per_dim(self, z: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError\n\n    @torch.no_grad()\n    def sample(self, B: int, T: int, Dz: int, device: str) -> torch.Tensor:\n        raise NotImplementedError\n\n\nclass PriorAR(PriorBase):\n    \"\"\"\n    p(z1)=N(0,I), p(zt|z_{t-1})=N(rho z_{t-1}, sigma^2 I)\n    rho learnable scalar.\n    \"\"\"\n    def __init__(self, Dz: int, rho_init: float, sigma: float):\n        super().__init__()\n        self.logit_rho = nn.Parameter(torch.logit(torch.tensor(float(rho_init))))\n        self.sigma = float(sigma)\n        self.Dz = Dz\n\n    def rho(self):\n        return torch.sigmoid(self.logit_rho).clamp(1e-4, 0.9999)\n\n    def log_p(self, z):\n        B, T, Dz = z.shape\n        lp1 = (-0.5 * (z[:, 0] ** 2 + math.log(2 * math.pi))).sum(dim=1)\n        if T == 1:\n            return lp1\n        rho = self.rho()\n        sigma2 = self.sigma ** 2\n        resid = z[:, 1:] - rho * z[:, :-1]\n        lp = (-0.5 * ((resid**2) / sigma2 + math.log(2 * math.pi * sigma2))).sum(dim=(1, 2))\n        return lp1 + lp\n\n    def log_p_per_dim(self, z):\n        B, T, Dz = z.shape\n        lp1 = (-0.5 * (z[:, 0] ** 2 + math.log(2 * math.pi)))  # (B,Dz)\n        if T == 1:\n            return lp1\n        rho = self.rho()\n        sigma2 = self.sigma ** 2\n        resid = z[:, 1:] - rho * z[:, :-1]  # (B,T-1,Dz)\n        lp = -0.5 * ((resid**2) / sigma2 + math.log(2 * math.pi * sigma2))  # (B,T-1,Dz)\n        return lp1 + lp.sum(dim=1)\n\n    @torch.no_grad()\n    def sample(self, B, T, Dz, device):\n        rho = float(self.rho().item())\n        z = torch.zeros(B, T, Dz, device=device)\n        z[:, 0] = torch.randn(B, Dz, device=device)\n        for t in range(1, T):\n            z[:, t] = rho * z[:, t - 1] + self.sigma * torch.randn(B, Dz, device=device)\n        return z\n\n\n\n# VAE model\n\nclass VAETextLM(nn.Module):\n    def __init__(self, cfg: CFG, prior: PriorBase):\n        super().__init__()\n        self.cfg = cfg\n        self.prior = prior\n\n        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_emb = nn.Embedding(cfg.block_size, cfg.d_model)\n        self.drop = nn.Dropout(cfg.dropout)\n\n        self.encoder = TransformerEncoder(cfg.d_model, cfg.n_heads, cfg.n_layers, cfg.dropout)\n\n        self.to_mu = nn.Linear(cfg.d_model, cfg.z_dim)\n        self.to_logvar = nn.Linear(cfg.d_model, cfg.z_dim)\n\n        self.z_proj = nn.Linear(cfg.z_dim, cfg.d_model)\n        self.decoder = TransformerDecoderLM(cfg.d_model, cfg.n_heads, cfg.n_layers, cfg.dropout)\n\n        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n\n    def encode(self, x_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        B, T = x_ids.shape\n        tok = self.tok_emb(x_ids)\n        pos = self.pos_emb(torch.arange(T, device=x_ids.device))[None, :, :]\n        h = self.drop(tok + pos)\n        h = self.encoder(h)\n        mu = self.to_mu(h)\n        logvar = self.to_logvar(h).clamp(-12.0, 6.0)\n        return mu, logvar\n\n    def reparam(self, mu, logvar, n_samples: int):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn((n_samples,) + mu.shape, device=mu.device)\n        return mu[None] + eps * std[None]\n\n    def log_q_total(self, z, mu, logvar) -> torch.Tensor:\n        var = torch.exp(logvar)\n        log2pi = math.log(2 * math.pi)\n        diff2 = (z - mu[None]) ** 2\n        lq = -0.5 * (diff2 / var[None] + logvar[None] + log2pi)\n        return lq.sum(dim=(2, 3))\n\n    def log_q_per_dim(self, z_btD: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n        var = torch.exp(logvar)\n        log2pi = math.log(2 * math.pi)\n        diff2 = (z_btD - mu) ** 2\n        lq = -0.5 * (diff2 / var + logvar + log2pi)\n        return lq.sum(dim=1)\n\n    def decode_logits(self, x_ids: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n        B, T = x_ids.shape\n        tok = self.tok_emb(x_ids)\n        pos = self.pos_emb(torch.arange(T, device=x_ids.device))[None, :, :]\n        h = self.drop(tok + pos + self.z_proj(z))\n        h = self.decoder(h)\n        return self.lm_head(h)\n\n    def forward(self, x_ids: torch.Tensor, y_ids: torch.Tensor, beta: float) -> Dict[str, torch.Tensor]:\n        B, T = x_ids.shape\n        mu, logvar = self.encode(x_ids)\n        zS = self.reparam(mu, logvar, self.cfg.n_z_samples)\n\n        nll_list, logq_list, logp_list = [], [], []\n        last_logits = None\n        last_z = None\n\n        for s in range(self.cfg.n_z_samples):\n            z = zS[s]\n            logits = self.decode_logits(x_ids, z)\n            last_logits = logits\n            last_z = z\n\n            nll = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                y_ids.view(-1),\n                reduction=\"none\",\n            ).view(B, T).sum(dim=1)\n            nll_list.append(nll)\n\n            logq = self.log_q_total(zS[s:s+1], mu, logvar)[0]\n            logq_list.append(logq)\n\n            logp = self.prior.log_p(z)\n            logp_list.append(logp)\n\n        nll = torch.stack(nll_list, dim=0).mean(dim=0)\n        logq = torch.stack(logq_list, dim=0).mean(dim=0)\n        logp = torch.stack(logp_list, dim=0).mean(dim=0)\n\n        kl = (logq - logp)\n        elbo = -(nll + beta * kl)\n\n        nll_tok = nll.mean() / T\n        kl_tok = kl.mean() / T\n        elbo_tok = elbo.mean() / T\n\n        ppl = torch.exp(nll_tok.detach())\n        bits_per_tok = (nll_tok.detach() / math.log(2.0))\n\n        loss = -(elbo_tok)\n        return {\n            \"loss\": loss,\n            \"elbo_tok\": elbo_tok.detach(),\n            \"nll_tok\": nll_tok.detach(),\n            \"kl_tok\": kl_tok.detach(),\n            \"ppl\": ppl.detach(),\n            \"bits_per_tok\": bits_per_tok.detach(),\n            \"mu\": mu,\n            \"logvar\": logvar,\n            \"z_last\": last_z,\n            \"logits_last\": last_logits,\n        }\n\n    @torch.no_grad()\n    def generate(self, prompt_ids: torch.Tensor, max_new_tokens: int = 80, top_k: int = 0) -> torch.Tensor:\n        self.eval()\n        device = prompt_ids.device\n        B, Tp = prompt_ids.shape\n        total_T = min(self.cfg.block_size, Tp + max_new_tokens)\n\n        out = prompt_ids.clone()\n        z = self.prior.sample(B, total_T, self.cfg.z_dim, device=device)\n\n        for _ in range(max_new_tokens):\n            Tcur = out.size(1)\n            if Tcur >= total_T:\n                break\n            x = out[:, :Tcur]\n            zcur = z[:, :Tcur]\n            logits = self.decode_logits(x, zcur)\n            next_logits = logits[:, -1, :]\n\n            if top_k and top_k > 0:\n                vals, idx = torch.topk(next_logits, k=top_k, dim=-1)\n                probs = torch.zeros_like(next_logits).scatter_(-1, idx, F.softmax(vals, dim=-1))\n            else:\n                probs = F.softmax(next_logits, dim=-1)\n\n            next_id = torch.multinomial(probs, num_samples=1)\n            out = torch.cat([out, next_id], dim=1)\n\n        return out\n\n\n\n# Perf / GPU utilities\n\ndef _now():\n    return time.perf_counter()\n\ndef sync_if_cuda(device: str):\n    if isinstance(device, str) and device.startswith(\"cuda\") and torch.cuda.is_available():\n        torch.cuda.synchronize()\n\ndef cuda_mem_reset(device: str):\n    if isinstance(device, str) and device.startswith(\"cuda\") and torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.synchronize()\n\n@torch.no_grad()\ndef cuda_mem_snapshot_mb(device: str) -> Dict[str, float]:\n    if not (isinstance(device, str) and device.startswith(\"cuda\") and torch.cuda.is_available()):\n        return {\"peak_alloc_MB\": float(\"nan\"), \"peak_reserved_MB\": float(\"nan\")}\n    peak_alloc = torch.cuda.max_memory_allocated() / (1024**2)\n    peak_reserved = torch.cuda.max_memory_reserved() / (1024**2)\n    return {\"peak_alloc_MB\": float(peak_alloc), \"peak_reserved_MB\": float(peak_reserved)}\n\nclass RunningMean:\n    def __init__(self):\n        self.sum = 0.0\n        self.n = 0\n    def add(self, x: float):\n        if math.isfinite(x):\n            self.sum += float(x)\n            self.n += 1\n    def mean(self) -> float:\n        return self.sum / max(1, self.n)\n\n\n\n# Metric helpers\n\n@torch.no_grad()\ndef logits_entropy_and_topk_mass(logits: torch.Tensor, top_k: int = 50) -> Dict[str, float]:\n    probs = F.softmax(logits, dim=-1)\n    logp = torch.log(probs.clamp_min(1e-12))\n    ent = -(probs * logp).sum(dim=-1)\n    ent_mean = ent.mean().item()\n\n    if top_k and top_k > 0:\n        V = probs.size(-1)\n        vals, _ = torch.topk(probs, k=min(top_k, V), dim=-1)\n        topk_mass = vals.sum(dim=-1).mean().item()\n    else:\n        topk_mass = float(\"nan\")\n\n    return {\"tok_entropy_mean\": float(ent_mean), \"topk_mass_mean\": float(topk_mass)}\n\n@torch.no_grad()\ndef ece_from_logits(logits: torch.Tensor, targets: torch.Tensor, n_bins: int = 15) -> float:\n    probs = F.softmax(logits, dim=-1)\n    conf, pred = probs.max(dim=-1)\n    acc = (pred == targets).float()\n\n    conf = conf.reshape(-1)\n    acc = acc.reshape(-1)\n\n    bins = torch.linspace(0, 1, n_bins + 1, device=logits.device)\n    ece = torch.zeros((), device=logits.device)\n    for i in range(n_bins):\n        lo, hi = bins[i], bins[i + 1]\n        mask = (conf > lo) & (conf <= hi) if i > 0 else (conf >= lo) & (conf <= hi)\n        if mask.any():\n            ece = ece + (mask.float().mean()) * (acc[mask].mean() - conf[mask].mean()).abs()\n    return float(ece.item())\n\n@torch.no_grad()\ndef posterior_collapse_ratio(mu: torch.Tensor, logvar: torch.Tensor) -> Dict[str, float]:\n    std = torch.exp(0.5 * logvar)\n    mu_abs_mean = mu.abs().mean(dim=(0, 1))\n    std_mean = std.mean(dim=(0, 1))\n    collapsed = ((mu_abs_mean < 0.02) & ((std_mean - 1.0).abs() < 0.05)).float().mean().item()\n    return {\n        \"post_std_mean\": float(std.mean().item()),\n        \"post_mu_abs_mean\": float(mu.abs().mean().item()),\n        \"collapse_dim_frac\": float(collapsed),\n    }\n\n@torch.no_grad()\ndef latent_autocorr(x: torch.Tensor, lag: int = 1) -> float:\n    if x.size(1) <= lag:\n        return float(\"nan\")\n    a = x[:, :-lag, :].reshape(-1, x.size(-1))\n    b = x[:, lag:, :].reshape(-1, x.size(-1))\n    a = a - a.mean(dim=0, keepdim=True)\n    b = b - b.mean(dim=0, keepdim=True)\n    cov = (a * b).mean(dim=0)\n    va = (a * a).mean(dim=0).clamp_min(1e-8)\n    vb = (b * b).mean(dim=0).clamp_min(1e-8)\n    return float((cov / torch.sqrt(va * vb)).mean().item())\n\n@torch.no_grad()\ndef kl_per_dim_stats(model: VAETextLM, mu: torch.Tensor, logvar: torch.Tensor, z: torch.Tensor, eps: float) -> Dict[str, float]:\n    B, T, Dz = z.shape\n    logq_bd = model.log_q_per_dim(z, mu, logvar)      # (B,Dz)\n    logp_bd = model.prior.log_p_per_dim(z)            # (B,Dz)\n    kld_bd = (logq_bd - logp_bd) / max(1, T)          # per-token\n    kld_d = kld_bd.mean(dim=0).detach().cpu().numpy()\n    return {\n        \"kldim_mean\": float(np.mean(kld_d)),\n        \"kldim_median\": float(np.median(kld_d)),\n        \"kldim_max\": float(np.max(kld_d)),\n        \"kldim_frac_below_eps\": float(np.mean(kld_d < eps)),\n    }\n\n@torch.no_grad()\ndef mi_proxy_batch(mu: torch.Tensor, logvar: torch.Tensor, z: torch.Tensor, max_components: int, max_points: int, seed: int = 0) -> float:\n    B, T, Dz = z.shape\n    C = B * T\n    if C == 0:\n        return float(\"nan\")\n    rng = np.random.default_rng(seed)\n\n    mu_c = mu.reshape(C, Dz)\n    lv_c = logvar.reshape(C, Dz)\n    z_c  = z.reshape(C, Dz)\n\n    comp_idx = np.arange(C)\n    pt_idx = np.arange(C)\n    if C > max_components:\n        comp_idx = rng.choice(comp_idx, size=max_components, replace=False)\n    if C > max_points:\n        pt_idx = rng.choice(pt_idx, size=max_points, replace=False)\n\n    mu_s = mu_c[torch.as_tensor(comp_idx, device=mu.device)]\n    lv_s = lv_c[torch.as_tensor(comp_idx, device=mu.device)]\n    var_s = torch.exp(lv_s).clamp_min(1e-12)\n\n    z_pts  = z_c [torch.as_tensor(pt_idx, device=mu.device)]\n    mu_pts = mu_c[torch.as_tensor(pt_idx, device=mu.device)]\n    lv_pts = lv_c[torch.as_tensor(pt_idx, device=mu.device)]\n    var_pts = torch.exp(lv_pts).clamp_min(1e-12)\n\n    log2pi = math.log(2 * math.pi)\n    lq_cond = -0.5 * (((z_pts - mu_pts) ** 2) / var_pts + lv_pts + log2pi).sum(dim=-1)\n    eq_logq_z_given_x = lq_cond.mean()\n\n    z_exp = z_pts[:, None, :]\n    mu_exp = mu_s[None, :, :]\n    lv_exp = lv_s[None, :, :]\n    var_exp = var_s[None, :, :]\n\n    lcomp = -0.5 * (((z_exp - mu_exp) ** 2) / var_exp + lv_exp + log2pi).sum(dim=-1)\n    lmix = torch.logsumexp(lcomp, dim=1) - math.log(lcomp.size(1))\n    eq_logq_z = lmix.mean()\n\n    return float((eq_logq_z_given_x - eq_logq_z).item())\n\ndef rd_points(nll_tok: float, kl_tok: float, betas: Tuple[float, ...]) -> Dict[str, float]:\n    out = {}\n    for b in betas:\n        out[f\"rd_elbo_tok_beta_{b:g}\"] = -(nll_tok + b * kl_tok)\n    out[\"rd_nll_tok\"] = float(nll_tok)\n    out[\"rd_kl_tok\"] = float(kl_tok)\n    return out\n\n\ndef distinct_ngrams(token_seq: List[int], n: int) -> float:\n    if len(token_seq) < n or n <= 0:\n        return 0.0\n    total = 0\n    uniq = set()\n    for i in range(len(token_seq) - n + 1):\n        uniq.add(tuple(token_seq[i:i+n]))\n        total += 1\n    return (len(uniq) / total) if total > 0 else 0.0\n\ndef repetition_rate(token_seq: List[int], n: int) -> float:\n    if len(token_seq) < n or n <= 0:\n        return 0.0\n    total = 0\n    counts = {}\n    for i in range(len(token_seq) - n + 1):\n        ng = tuple(token_seq[i:i+n])\n        counts[ng] = counts.get(ng, 0) + 1\n        total += 1\n    repeated = sum(c for c in counts.values() if c > 1)\n    return repeated / total if total > 0 else 0.0\n\n\n@torch.no_grad()\ndef generation_metrics(model: VAETextLM, tokenizer, device: str, n_prompts: int, max_new: int, top_k: int) -> Dict[str, Any]:\n    model.eval()\n    prompts = [\n        \"The meaning of life is\",\n        \"In the middle of the night\",\n        \"The government announced that\",\n        \"A new theory suggests\",\n        \"Once upon a time\",\n        \"The experiment shows\",\n        \"In a shocking discovery\",\n        \"The book describes\",\n        \"Scientists found that\",\n        \"The president said\",\n        \"In the future,\",\n        \"The story begins\",\n    ]\n\n    cuda_mem_reset(device)\n    sync_if_cuda(device)\n    t0 = _now()\n    gen_new_tokens_total = 0\n\n    per_prompt: List[Dict[str, Any]] = []\n    decoded_samples: List[str] = []\n\n    for i in range(n_prompts):\n        p = prompts[i % len(prompts)]\n        p_ids = tokenizer(p, return_tensors=\"pt\").input_ids.to(device)\n        if p_ids.size(1) > model.cfg.block_size // 2:\n            p_ids = p_ids[:, : model.cfg.block_size // 2]\n\n        out_ids = model.generate(p_ids, max_new_tokens=max_new, top_k=top_k)\n        seq = out_ids[0].tolist()\n\n        prompt_len = int(p_ids.size(1))\n        new_tokens = max(0, len(seq) - prompt_len)\n        gen_new_tokens_total += new_tokens\n\n        st = {\n            \"prompt\": p,\n            \"prompt_len\": prompt_len,\n            \"total_len\": len(seq),\n            \"new_tokens\": new_tokens,\n            \"rep2\": repetition_rate(seq, 2),\n            \"rep3\": repetition_rate(seq, 3),\n            \"distinct2\": distinct_ngrams(seq, 2),\n            \"distinct3\": distinct_ngrams(seq, 3),\n        }\n\n        txt = tokenizer.decode(seq[: min(len(seq), 250)], skip_special_tokens=True)\n        st[\"sample\"] = txt\n        per_prompt.append(st)\n\n        if i < 3:\n            decoded_samples.append(txt)\n\n    sync_if_cuda(device)\n    dt = max(1e-9, (_now() - t0))\n    mem = cuda_mem_snapshot_mb(device)\n\n    rep2 = float(np.mean([p[\"rep2\"] for p in per_prompt])) if per_prompt else 0.0\n    rep3 = float(np.mean([p[\"rep3\"] for p in per_prompt])) if per_prompt else 0.0\n    distinct2 = float(np.mean([p[\"distinct2\"] for p in per_prompt])) if per_prompt else 0.0\n    distinct3 = float(np.mean([p[\"distinct3\"] for p in per_prompt])) if per_prompt else 0.0\n    lengths = [p[\"total_len\"] for p in per_prompt]\n    newlens = [p[\"new_tokens\"] for p in per_prompt]\n\n    return {\n        \"gen_distinct2\": distinct2,\n        \"gen_distinct3\": distinct3,\n        \"gen_rep2\": rep2,\n        \"gen_rep3\": rep3,\n        \"gen_len_mean\": float(np.mean(lengths)) if lengths else 0.0,\n        \"gen_len_std\": float(np.std(lengths)) if lengths else 0.0,\n        \"gen_new_tokens_mean\": float(np.mean(newlens)) if newlens else 0.0,\n        \"gen_new_tokens_total\": int(gen_new_tokens_total),\n\n        \"time_gen_seconds_total\": float(dt),\n        \"time_gen_tokens_per_s\": float(gen_new_tokens_total / dt),\n        \"gpu_gen_peak_alloc_MB\": mem[\"peak_alloc_MB\"],\n        \"gpu_gen_peak_reserved_MB\": mem[\"peak_reserved_MB\"],\n\n        \"gen_per_prompt\": per_prompt,\n        \"gen_sample_0\": decoded_samples[0] if len(decoded_samples) > 0 else \"\",\n        \"gen_sample_1\": decoded_samples[1] if len(decoded_samples) > 1 else \"\",\n        \"gen_sample_2\": decoded_samples[2] if len(decoded_samples) > 2 else \"\",\n    }\n\n\n@torch.no_grad()\ndef eval_many_metrics(\n    model: VAETextLM,\n    loader: DataLoader,\n    device: str,\n    beta: float,\n    max_batches: int,\n    ece_bins: int,\n    gen_do: bool,\n    tokenizer=None,\n    gen_prompts: int = 12,\n    gen_max_new: int = 96,\n    gen_top_k: int = 50,\n    rd_betas: Tuple[float, ...] = (0.0, 1.0),\n    measure_perf: bool = True,\n    kldim_eps: float = 0.01,\n    mi_max_components: int = 512,\n    mi_max_points: int = 256,\n    mi_seed: int = 0,\n) -> Dict[str, Any]:\n    model.eval()\n\n    acc = {\n        \"nll_tok\": 0.0,\n        \"kl_tok\": 0.0,\n        \"elbo_tok\": 0.0,\n        \"tok_entropy_mean\": 0.0,\n        \"topk_mass_mean\": 0.0,\n        \"ece\": 0.0,\n        \"post_std_mean\": 0.0,\n        \"post_mu_abs_mean\": 0.0,\n        \"collapse_dim_frac\": 0.0,\n        \"mu_ac1\": 0.0,\n        \"mu_ac5\": 0.0,\n        \"z_ac1\": 0.0,\n        \"z_ac5\": 0.0,\n        \"kldim_mean\": 0.0,\n        \"kldim_median\": 0.0,\n        \"kldim_max\": 0.0,\n        \"kldim_frac_below_eps\": 0.0,\n        \"mi_proxy\": 0.0,\n    }\n    n = 0\n\n    step_ms = RunningMean()\n    tokens_per_s = RunningMean()\n    total_tokens = 0\n\n    if measure_perf:\n        cuda_mem_reset(device)\n\n    for i, (x, y) in enumerate(loader):\n        if i >= max_batches:\n            break\n        x = x.to(device)\n        y = y.to(device)\n\n        if measure_perf:\n            sync_if_cuda(device)\n            t0 = _now()\n\n        out = model(x, y, beta=beta)\n\n        if measure_perf:\n            sync_if_cuda(device)\n            dt = max(1e-9, (_now() - t0))\n            step_ms.add(1000.0 * dt)\n            B, T = x.shape\n            tok = int(B * T)\n            total_tokens += tok\n            tokens_per_s.add(tok / dt)\n\n        nll_tok = float(out[\"nll_tok\"])\n        kl_tok = float(out[\"kl_tok\"])\n        elbo_tok = float(out[\"elbo_tok\"])\n        if not (math.isfinite(nll_tok) and math.isfinite(kl_tok) and math.isfinite(elbo_tok)):\n            continue\n\n        logits = out[\"logits_last\"]\n        mu = out[\"mu\"].detach()\n        logvar = out[\"logvar\"].detach()\n        zlast = out[\"z_last\"].detach()\n\n        ent_topk = logits_entropy_and_topk_mass(logits, top_k=gen_top_k)\n        ece = ece_from_logits(logits, y, n_bins=ece_bins)\n        post = posterior_collapse_ratio(mu, logvar)\n\n        mu_ac1 = latent_autocorr(mu, lag=1)\n        mu_ac5 = latent_autocorr(mu, lag=5)\n        z_ac1 = latent_autocorr(zlast, lag=1)\n        z_ac5 = latent_autocorr(zlast, lag=5)\n\n        kld = kl_per_dim_stats(model, mu, logvar, zlast, eps=kldim_eps)\n        mi = mi_proxy_batch(mu, logvar, zlast, max_components=mi_max_components, max_points=mi_max_points, seed=mi_seed)\n\n        acc[\"nll_tok\"] += nll_tok\n        acc[\"kl_tok\"] += kl_tok\n        acc[\"elbo_tok\"] += elbo_tok\n        acc[\"tok_entropy_mean\"] += ent_topk[\"tok_entropy_mean\"]\n        acc[\"topk_mass_mean\"] += ent_topk[\"topk_mass_mean\"]\n        acc[\"ece\"] += ece\n        acc[\"post_std_mean\"] += post[\"post_std_mean\"]\n        acc[\"post_mu_abs_mean\"] += post[\"post_mu_abs_mean\"]\n        acc[\"collapse_dim_frac\"] += post[\"collapse_dim_frac\"]\n        acc[\"mu_ac1\"] += mu_ac1\n        acc[\"mu_ac5\"] += mu_ac5\n        acc[\"z_ac1\"] += z_ac1\n        acc[\"z_ac5\"] += z_ac5\n        acc[\"kldim_mean\"] += kld[\"kldim_mean\"]\n        acc[\"kldim_median\"] += kld[\"kldim_median\"]\n        acc[\"kldim_max\"] += kld[\"kldim_max\"]\n        acc[\"kldim_frac_below_eps\"] += kld[\"kldim_frac_below_eps\"]\n        acc[\"mi_proxy\"] += mi\n        n += 1\n\n    if n == 0:\n        return {k: float(\"nan\") for k in acc.keys()}\n\n    for k in acc:\n        acc[k] /= n\n\n    acc[\"ppl\"] = math.exp(acc[\"nll_tok\"])\n    acc[\"bits_per_tok\"] = acc[\"nll_tok\"] / math.log(2.0)\n    acc.update(rd_points(acc[\"nll_tok\"], acc[\"kl_tok\"], rd_betas))\n\n    if measure_perf:\n        mem = cuda_mem_snapshot_mb(device)\n        acc[\"time_eval_step_ms_mean\"] = step_ms.mean()\n        acc[\"time_eval_tokens_per_s_mean\"] = tokens_per_s.mean()\n        acc[\"gpu_eval_peak_alloc_MB\"] = mem[\"peak_alloc_MB\"]\n        acc[\"gpu_eval_peak_reserved_MB\"] = mem[\"peak_reserved_MB\"]\n        acc[\"eval_total_tokens_measured\"] = int(total_tokens)\n\n    if gen_do and tokenizer is not None:\n        acc.update(\n            generation_metrics(model, tokenizer, device=device, n_prompts=gen_prompts, max_new=gen_max_new, top_k=gen_top_k)\n        )\n\n    return acc\n\n\n\n# Schedules\n\ndef lr_schedule(step, cfg: CFG):\n    if step < cfg.warmup_steps:\n        return cfg.lr * (step / max(1, cfg.warmup_steps))\n    progress = (step - cfg.warmup_steps) / max(1, cfg.max_steps - cfg.warmup_steps)\n    return cfg.lr * (0.1 + 0.9 * 0.5 * (1.0 + math.cos(math.pi * progress)))\n\ndef beta_schedule(step, cfg: CFG):\n    if step >= cfg.beta_warmup_steps:\n        return cfg.beta_end\n    a = step / max(1, cfg.beta_warmup_steps)\n    return cfg.beta_start + a * (cfg.beta_end - cfg.beta_start)\n\n\n\n# Train (AR only)\n\ndef train_ar(cfg: CFG, train_loader, val_loader, tokenizer) -> Dict[str, Any]:\n    device = cfg.device\n    prior = PriorAR(cfg.z_dim, rho_init=cfg.ar_init_rho, sigma=cfg.ar_sigma)\n    model = VAETextLM(cfg, prior).to(device)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    use_amp = bool(cfg.amp and device.startswith(\"cuda\"))\n    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n\n    os.makedirs(cfg.out_dir, exist_ok=True)\n    run_stamp = int(time.time())\n    log_path = os.path.join(cfg.out_dir, f\"{cfg.run_name}_vae_ar_{run_stamp}.jsonl\")\n\n    best_val = float(\"inf\")\n    best: Dict[str, Any] = {}\n\n    t0 = time.time()\n    pbar = tqdm(total=cfg.max_steps, desc=\"train[vae_ar]\")\n\n    it = iter(train_loader)\n\n    train_step_ms = RunningMean()\n    train_tokens_per_s = RunningMean()\n    train_tokens_total = 0\n    cuda_mem_reset(device)\n\n    for step in range(1, cfg.max_steps + 1):\n        try:\n            x, y = next(it)\n        except StopIteration:\n            it = iter(train_loader)\n            x, y = next(it)\n\n        x, y = x.to(device), y.to(device)\n\n        beta = beta_schedule(step, cfg)\n        lr = lr_schedule(step, cfg)\n        for pg in opt.param_groups:\n            pg[\"lr\"] = lr\n\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        if use_amp:\n            sync_if_cuda(device)\n        t_step0 = _now()\n\n        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n            out = model(x, y, beta=beta)\n            loss = out[\"loss\"]\n\n        scaler.scale(loss).backward()\n        if cfg.grad_clip > 0:\n            scaler.unscale_(opt)\n            nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n        scaler.step(opt)\n        scaler.update()\n\n        if use_amp:\n            sync_if_cuda(device)\n        dt = max(1e-9, (_now() - t_step0))\n        train_step_ms.add(1000.0 * dt)\n        B, T = x.shape\n        tok = int(B * T)\n        train_tokens_total += tok\n        train_tokens_per_s.add(tok / dt)\n\n        if step % cfg.log_every == 0:\n            pbar.set_postfix({\n                \"loss\": f\"{loss.item():.4f}\",\n                \"nll_tok\": f\"{float(out['nll_tok']):.4f}\",\n                \"kl_tok\": f\"{float(out['kl_tok']):.4f}\",\n                \"ppl\": f\"{float(out['ppl']):.2f}\",\n                \"beta\": f\"{beta:.2f}\",\n                \"lr\": f\"{lr:.1e}\",\n            })\n\n        if step % cfg.eval_every == 0 or step == 1:\n            mem_train = cuda_mem_snapshot_mb(device)\n            train_perf = {\n                \"time_train_step_ms_mean_window\": train_step_ms.mean(),\n                \"time_train_tokens_per_s_mean_window\": train_tokens_per_s.mean(),\n                \"gpu_train_peak_alloc_MB_window\": mem_train[\"peak_alloc_MB\"],\n                \"gpu_train_peak_reserved_MB_window\": mem_train[\"peak_reserved_MB\"],\n                \"train_tokens_measured_window\": int(train_tokens_total),\n            }\n\n            train_eval = eval_many_metrics(\n                model, train_loader, device=device, beta=1.0,\n                max_batches=min(cfg.eval_train_batches, cfg.eval_max_batches),\n                ece_bins=cfg.ece_bins,\n                gen_do=False,\n                rd_betas=cfg.rd_betas,\n                measure_perf=True,\n                kldim_eps=cfg.kldim_eps,\n                mi_max_components=cfg.mi_max_components,\n                mi_max_points=cfg.mi_max_points,\n                mi_seed=cfg.mi_seed,\n                gen_top_k=cfg.eval_gen_top_k,\n            )\n            val_eval = eval_many_metrics(\n                model, val_loader, device=device, beta=1.0,\n                max_batches=cfg.eval_max_batches,\n                ece_bins=cfg.ece_bins,\n                gen_do=True, tokenizer=tokenizer,\n                gen_prompts=cfg.eval_gen_prompts,\n                gen_max_new=cfg.eval_gen_max_new,\n                gen_top_k=cfg.eval_gen_top_k,\n                rd_betas=cfg.rd_betas,\n                measure_perf=True,\n                kldim_eps=cfg.kldim_eps,\n                mi_max_components=cfg.mi_max_components,\n                mi_max_points=cfg.mi_max_points,\n                mi_seed=cfg.mi_seed,\n            )\n\n            val_nll = val_eval.get(\"nll_tok\", float(\"inf\"))\n            if math.isfinite(val_nll) and (val_nll < best_val):\n                best_val = float(val_nll)\n                best = {\n                    \"variant\": \"vae_ar\",\n                    \"step\": step,\n                    \"wall_s\": time.time() - t0,\n                    \"best_val_nll_tok\": best_val,\n                    \"train_perf_window\": train_perf,\n                    \"train\": train_eval,\n                    \"val\": val_eval,\n                }\n\n            rec = {\n                \"variant\": \"vae_ar\",\n                \"step\": step,\n                \"wall_s\": time.time() - t0,\n                \"lr\": float(lr),\n                \"beta\": float(beta),\n                \"train_perf_window\": train_perf,\n                \"train\": train_eval,\n                \"val\": val_eval,\n                \"best_val_nll_tok_so_far\": float(best_val),\n            }\n            with open(log_path, \"a\", encoding=\"utf-8\") as f:\n                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\n            print(\n                f\"[eval] vae_ar step={step:5d} \"\n                f\"train_tok/s={train_eval.get('time_eval_tokens_per_s_mean', float('nan')):.1f} \"\n                f\"val_tok/s={val_eval.get('time_eval_tokens_per_s_mean', float('nan')):.1f} \"\n                f\"gen_tok/s={val_eval.get('time_gen_tokens_per_s', float('nan')):.1f} \"\n                f\"val_ppl={val_eval.get('ppl', float('nan')):.2f} \"\n                f\"val_nll={val_eval.get('nll_tok', float('nan')):.4f} val_kl={val_eval.get('kl_tok', float('nan')):.4f} \"\n                f\"ece={val_eval.get('ece', float('nan')):.3f} ent={val_eval.get('tok_entropy_mean', float('nan')):.3f} \"\n                f\"mi={val_eval.get('mi_proxy', float('nan')):.3f} \"\n                f\"kldim_med={val_eval.get('kldim_median', float('nan')):.3f} \"\n                f\"train_peakMB={train_perf.get('gpu_train_peak_alloc_MB_window', float('nan')):.1f}\"\n            )\n\n            train_step_ms = RunningMean()\n            train_tokens_per_s = RunningMean()\n            train_tokens_total = 0\n            cuda_mem_reset(device)\n\n        pbar.update(1)\n\n    pbar.close()\n\n    if not best:\n        best = {\"variant\": \"vae_ar\", \"step\": cfg.max_steps, \"wall_s\": time.time() - t0, \"best_val_nll_tok\": best_val}\n    best[\"log_path\"] = log_path\n    return best\n\n\ndef main():\n    cfg = CFG()\n    set_seed(cfg.seed)\n\n    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n    tokenizer.model_max_length = int(1e9)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    cfg.vocab_size = len(tokenizer)\n\n    train_loader, val_loader = load_wt2_blocks(cfg, tokenizer)\n\n    best = train_ar(cfg, train_loader, val_loader, tokenizer)\n\n    os.makedirs(cfg.out_dir, exist_ok=True)\n    out_json = os.path.join(cfg.out_dir, f\"{cfg.run_name}_BEST_{int(time.time())}.json\")\n    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"cfg\": asdict(cfg), \"best\": best}, f, indent=2, ensure_ascii=False)\n\n    print(f\"\\nSaved BEST summary JSON: {out_json}\")\n    print(f\"Best log JSONL: {best.get('log_path','')}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:16:46.946602Z","iopub.execute_input":"2025-12-15T07:16:46.946879Z","iopub.status.idle":"2025-12-15T07:42:00.315758Z","shell.execute_reply.started":"2025-12-15T07:16:46.946857Z","shell.execute_reply":"2025-12-15T07:42:00.314630Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4121a804dbba4bfc827c71f13ebbc3f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"596e4c77b3b44fd08ad7e9bd43b3898b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3d6a4fc6e849ee9627d42527ec36cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d129032663d2494889204cb9c320e7e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a84a6d65f2d24fb688ce9fadf1842b59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e0e64f6b56d4501aa292eea43a10f4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba99210fd11d41ada25ebb1d30953b4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a44258aff7a435f91b460a5e39a385d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc7e2bc8c4f04c9db0f8146299028798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48777ca9c55c409584fd667dc0af636b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd47eeefd0c543a999ce654bb01d9e83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d49b195d94f455b997cf14c13cf5cc5"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\ntrain[vae_ar]:   0%|          | 1/6000 [00:15<25:02:07, 15.02s/it]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step=    1 train_tok/s=64052.0 val_tok/s=65908.0 gen_tok/s=276.9 val_ppl=111782.79 val_nll=11.6243 val_kl=648.1745 ece=0.002 ent=10.022 mi=49.299 kldim_med=9.817 train_peakMB=3511.3\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:   7%|▋         | 400/6000 [01:51<6:18:10,  4.05s/it, loss=12.2158, nll_tok=6.4745, kl_tok=21.5299, ppl=648.41, beta=0.27, lr=3.0e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step=  400 train_tok/s=66070.3 val_tok/s=66008.1 gen_tok/s=287.8 val_ppl=798.93 val_nll=6.6833 val_kl=20.9481 ece=0.018 ent=6.394 mi=0.301 kldim_med=0.327 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  13%|█▎        | 800/6000 [03:28<5:44:56,  3.98s/it, loss=17.0916, nll_tok=6.1767, kl_tok=20.4656, ppl=481.39, beta=0.53, lr=2.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step=  800 train_tok/s=66113.6 val_tok/s=66118.6 gen_tok/s=305.5 val_ppl=566.15 val_nll=6.3389 val_kl=20.2432 ece=0.022 ent=6.004 mi=0.105 kldim_med=0.316 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  20%|██        | 1200/6000 [05:05<5:16:18,  3.95s/it, loss=21.9683, nll_tok=5.9479, kl_tok=20.0255, ppl=382.96, beta=0.80, lr=2.8e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 1200 train_tok/s=66086.8 val_tok/s=66174.9 gen_tok/s=310.9 val_ppl=447.52 val_nll=6.1037 val_kl=19.7930 ece=0.022 ent=5.759 mi=0.044 kldim_med=0.309 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  27%|██▋       | 1600/6000 [06:42<4:52:11,  3.98s/it, loss=25.1890, nll_tok=5.6690, kl_tok=19.5200, ppl=289.74, beta=1.00, lr=2.7e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 1600 train_tok/s=66151.9 val_tok/s=66090.1 gen_tok/s=304.8 val_ppl=390.57 val_nll=5.9676 val_kl=19.4372 ece=0.022 ent=5.632 mi=0.032 kldim_med=0.304 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  33%|███▎      | 2000/6000 [08:19<4:23:38,  3.95s/it, loss=24.3795, nll_tok=5.3694, kl_tok=19.0101, ppl=214.73, beta=1.00, lr=2.4e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 2000 train_tok/s=66230.1 val_tok/s=66170.3 gen_tok/s=311.0 val_ppl=353.85 val_nll=5.8689 val_kl=19.0480 ece=0.028 ent=5.225 mi=0.028 kldim_med=0.298 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  40%|████      | 2400/6000 [09:57<3:59:51,  4.00s/it, loss=23.6080, nll_tok=4.9795, kl_tok=18.6285, ppl=145.40, beta=1.00, lr=2.2e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 2400 train_tok/s=65896.5 val_tok/s=65920.1 gen_tok/s=306.8 val_ppl=331.72 val_nll=5.8043 val_kl=18.6132 ece=0.024 ent=5.235 mi=0.029 kldim_med=0.291 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  47%|████▋     | 2800/6000 [11:34<3:33:55,  4.01s/it, loss=23.3647, nll_tok=5.0411, kl_tok=18.3235, ppl=154.65, beta=1.00, lr=1.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 2800 train_tok/s=65994.0 val_tok/s=65938.0 gen_tok/s=302.6 val_ppl=317.58 val_nll=5.7607 val_kl=18.2731 ece=0.031 ent=4.905 mi=0.026 kldim_med=0.285 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  53%|█████▎    | 3200/6000 [13:12<3:08:01,  4.03s/it, loss=22.7308, nll_tok=4.9556, kl_tok=17.7752, ppl=141.96, beta=1.00, lr=1.6e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 3200 train_tok/s=65985.1 val_tok/s=65989.8 gen_tok/s=298.8 val_ppl=309.96 val_nll=5.7365 val_kl=17.9617 ece=0.030 ent=4.813 mi=0.025 kldim_med=0.281 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  60%|██████    | 3600/6000 [14:50<2:41:02,  4.03s/it, loss=22.3814, nll_tok=4.7536, kl_tok=17.6278, ppl=116.00, beta=1.00, lr=1.3e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 3600 train_tok/s=65804.8 val_tok/s=65872.5 gen_tok/s=300.6 val_ppl=301.18 val_nll=5.7077 val_kl=17.6996 ece=0.033 ent=4.718 mi=0.025 kldim_med=0.277 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  67%|██████▋   | 4000/6000 [16:28<2:13:19,  4.00s/it, loss=22.0798, nll_tok=4.4993, kl_tok=17.5805, ppl=89.95, beta=1.00, lr=1.0e-04] ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 4000 train_tok/s=65972.9 val_tok/s=66002.2 gen_tok/s=306.5 val_ppl=300.70 val_nll=5.7061 val_kl=17.4905 ece=0.037 ent=4.618 mi=0.027 kldim_med=0.273 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  73%|███████▎  | 4400/6000 [18:06<1:47:10,  4.02s/it, loss=21.8012, nll_tok=4.4362, kl_tok=17.3650, ppl=84.45, beta=1.00, lr=7.9e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 4400 train_tok/s=65913.4 val_tok/s=65889.1 gen_tok/s=302.8 val_ppl=301.00 val_nll=5.7071 val_kl=17.3283 ece=0.041 ent=4.491 mi=0.024 kldim_med=0.271 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  80%|████████  | 4800/6000 [19:44<1:21:19,  4.07s/it, loss=21.5226, nll_tok=4.3356, kl_tok=17.1870, ppl=76.37, beta=1.00, lr=5.8e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 4800 train_tok/s=65712.7 val_tok/s=65836.7 gen_tok/s=293.0 val_ppl=299.34 val_nll=5.7016 val_kl=17.1759 ece=0.039 ent=4.484 mi=0.026 kldim_med=0.268 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  87%|████████▋ | 5200/6000 [21:22<53:49,  4.04s/it, loss=21.3161, nll_tok=4.3390, kl_tok=16.9771, ppl=76.63, beta=1.00, lr=4.3e-05]  ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 5200 train_tok/s=65796.9 val_tok/s=65949.7 gen_tok/s=298.9 val_ppl=301.90 val_nll=5.7101 val_kl=17.0925 ece=0.047 ent=4.362 mi=0.024 kldim_med=0.267 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  93%|█████████▎| 5600/6000 [23:00<26:48,  4.02s/it, loss=21.4050, nll_tok=4.4308, kl_tok=16.9742, ppl=84.00, beta=1.00, lr=3.3e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 5600 train_tok/s=65874.8 val_tok/s=65855.0 gen_tok/s=303.2 val_ppl=301.89 val_nll=5.7101 val_kl=17.0195 ece=0.045 ent=4.401 mi=0.025 kldim_med=0.266 train_peakMB=3981.0\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]: 100%|██████████| 6000/6000 [24:38<00:00,  4.06it/s, loss=21.1580, nll_tok=4.1740, kl_tok=16.9840, ppl=64.98, beta=1.00, lr=3.0e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 6000 train_tok/s=65738.4 val_tok/s=65848.4 gen_tok/s=306.6 val_ppl=302.95 val_nll=5.7136 val_kl=16.9583 ece=0.044 ent=4.379 mi=0.024 kldim_med=0.265 train_peakMB=3981.0\n\nSaved BEST summary JSON: runs/wt2_latentAR_BEST_1765784520.json\nBest log JSONL: runs/wt2_latentAR_vae_ar_1765783041.jsonl\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# GP-VAE on WikiText-2 blocks — GP PRIOR ONLY\n# \"NO-DRAMA\" + MANY METRICS + PERF/GPU\n\nimport math\nimport os\nimport time\nimport json\nimport random\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, Tuple, List, Any, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n\n# Environment / warnings\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\n\n\n# Config\n\n@dataclass\nclass CFG:\n    seed: int = 0\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Data\n    dataset_name: str = \"wikitext\"\n    dataset_config: str = \"wikitext-2-raw-v1\"\n    tokenizer_name: str = \"gpt2\"\n    block_size: int = 256\n    batch_size: int = 16\n    num_workers: int = 2\n    pin_memory: bool = True\n\n    # Model\n    vocab_size: int = 50257  # overwritten after tokenizer load\n    d_model: int = 384\n    n_layers: int = 6\n    n_heads: int = 6\n    dropout: float = 0.1\n\n    # Latent\n    z_dim: int = 64\n    n_z_samples: int = 1  # MC samples for KL estimate\n\n    # Training\n    lr: float = 3e-4\n    weight_decay: float = 0.01\n    max_steps: int = 6000\n    warmup_steps: int = 300\n    grad_clip: float = 1.0\n    eval_every: int = 400\n    log_every: int = 50\n    amp: bool = True\n\n    # KL anneal\n    beta_start: float = 0.0\n    beta_end: float = 1.0\n    beta_warmup_steps: int = 1500\n\n    # GP prior (RBF kernel)\n    gp_lengthscale: float = 25.0\n    gp_sigma: float = 1.0\n    gp_jitter: float = 1e-3\n\n    # Eval controls\n    eval_max_batches: int = 80\n    eval_train_batches: int = 20\n    eval_gen_prompts: int = 12\n    eval_gen_max_new: int = 96\n    eval_gen_top_k: int = 50\n\n    # ECE calibration\n    ece_bins: int = 15\n\n    # Bits-back diagnostics\n    kldim_eps: float = 0.01\n\n    # MI proxy\n    mi_max_components: int = 512\n    mi_max_points: int = 256\n    mi_seed: int = 0\n\n    # Rate–Distortion points\n    rd_betas: Tuple[float, ...] = (0.0, 0.1, 0.5, 1.0, 2.0)\n\n    # Logging / saving\n    out_dir: str = \"runs\"\n    run_name: str = \"wt2_latentGP\"\n\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\n\n# Dataset\n\nclass LMBlocks(Dataset):\n    def __init__(self, token_ids: List[int], block_size: int):\n        self.block_size = block_size\n        n = (len(token_ids) - 1) // block_size\n        self.data = token_ids[: n * block_size + 1]\n\n    def __len__(self):\n        return (len(self.data) - 1) // self.block_size\n\n    def __getitem__(self, idx):\n        i = idx * self.block_size\n        x = torch.tensor(self.data[i : i + self.block_size], dtype=torch.long)\n        y = torch.tensor(self.data[i + 1 : i + self.block_size + 1], dtype=torch.long)\n        return x, y\n\n\ndef tokenize_split_streaming(tokenizer, texts: List[str], chunk_chars: int = 200_000) -> List[int]:\n    ids: List[int] = []\n    buf: List[str] = []\n    cur_len = 0\n    sep = \"\\n\\n\"\n\n    for t in texts:\n        if not t:\n            continue\n        add = t + sep\n        buf.append(add)\n        cur_len += len(add)\n        if cur_len >= chunk_chars:\n            chunk = \"\".join(buf)\n            ids.extend(tokenizer.encode(chunk))\n            buf = []\n            cur_len = 0\n\n    if buf:\n        chunk = \"\".join(buf)\n        ids.extend(tokenizer.encode(chunk))\n\n    return ids\n\n\ndef load_wt2_blocks(cfg: CFG, tokenizer) -> Tuple[DataLoader, DataLoader]:\n    ds = load_dataset(cfg.dataset_name, cfg.dataset_config)\n\n    train_ids = tokenize_split_streaming(tokenizer, ds[\"train\"][\"text\"])\n    val_ids = tokenize_split_streaming(tokenizer, ds[\"validation\"][\"text\"])\n\n    train_set = LMBlocks(train_ids, cfg.block_size)\n    val_set = LMBlocks(val_ids, cfg.block_size)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=cfg.batch_size,\n        shuffle=True,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory,\n        drop_last=True,\n        persistent_workers=(cfg.num_workers > 0),\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=cfg.batch_size,\n        shuffle=False,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory,\n        drop_last=False,\n        persistent_workers=(cfg.num_workers > 0),\n    )\n    return train_loader, val_loader\n\n\n\n# Transformer blocks\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, d_model, n_heads, n_layers, dropout):\n        super().__init__()\n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.enc = nn.TransformerEncoder(layer, num_layers=n_layers)\n\n    def forward(self, x, src_key_padding_mask=None):\n        return self.enc(x, src_key_padding_mask=src_key_padding_mask)\n\n\nclass TransformerDecoderLM(nn.Module):\n    def __init__(self, d_model, n_heads, n_layers, dropout):\n        super().__init__()\n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.dec = nn.TransformerEncoder(layer, num_layers=n_layers)\n\n    def forward(self, x):\n        _, T, _ = x.shape\n        attn_mask = torch.full((T, T), float(\"-inf\"), device=x.device)\n        attn_mask = torch.triu(attn_mask, diagonal=1)\n        return self.dec(x, mask=attn_mask)\n\n\n\n# Prior: GP only\n\nclass PriorBase(nn.Module):\n    def log_p(self, z: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError\n\n    def log_p_per_dim(self, z: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError\n\n    @torch.no_grad()\n    def sample(self, B: int, T: int, Dz: int, device: str) -> torch.Tensor:\n        raise NotImplementedError\n\n\nclass PriorGP(PriorBase):\n    \"\"\"\n    GP prior per latent dim: z_{:, :, d} ~ N(0, K_T) with RBF kernel over time.\n    Kernel built dynamically per T.\n    \"\"\"\n    def __init__(self, lengthscale: float, sigma: float, jitter: float):\n        super().__init__()\n        self.log_lengthscale = nn.Parameter(torch.log(torch.tensor(float(lengthscale))))\n        self.log_sigma = nn.Parameter(torch.log(torch.tensor(float(sigma))))\n        self.jitter = float(jitter)\n\n    def _kernel(self, T: int, device):\n        t = torch.arange(T, device=device).float()\n        dt2 = (t[:, None] - t[None, :]) ** 2\n        ell = torch.exp(self.log_lengthscale).clamp(1e-3, 1e6)\n        sig = torch.exp(self.log_sigma).clamp(1e-6, 1e6)\n        K = (sig**2) * torch.exp(-0.5 * dt2 / (ell**2))\n        return K + self.jitter * torch.eye(T, device=device)\n\n    def _chol(self, K: torch.Tensor) -> torch.Tensor:\n        try:\n            return torch.linalg.cholesky(K)\n        except RuntimeError:\n            T = K.size(0)\n            return torch.linalg.cholesky(K + (10.0 * self.jitter) * torch.eye(T, device=K.device))\n\n    def log_p(self, z):\n        B, T, Dz = z.shape\n        K = self._kernel(T, z.device)\n        L = self._chol(K)\n\n        z_td = z.transpose(1, 2).contiguous().view(B * Dz, T, 1)\n        alpha = torch.cholesky_solve(z_td, L)\n        quad = (z_td * alpha).sum(dim=(1, 2))\n        logdet = 2.0 * torch.log(torch.diagonal(L)).sum()\n        const = T * math.log(2 * math.pi)\n        lp = -0.5 * (quad + logdet + const)\n        return lp.view(B, Dz).sum(dim=1)\n\n    def log_p_per_dim(self, z):\n        B, T, Dz = z.shape\n        K = self._kernel(T, z.device)\n        L = self._chol(K)\n        logdet = 2.0 * torch.log(torch.diagonal(L)).sum()\n        const = T * math.log(2 * math.pi)\n\n        z_td = z.transpose(1, 2).contiguous().view(B * Dz, T, 1)\n        alpha = torch.cholesky_solve(z_td, L)\n        quad = (z_td * alpha).sum(dim=(1, 2))\n        lp = -0.5 * (quad + logdet + const)\n        return lp.view(B, Dz)\n\n    @torch.no_grad()\n    def sample(self, B, T, Dz, device):\n        K = self._kernel(T, device)\n        L = self._chol(K)\n        eps = torch.randn(B, Dz, T, device=device)\n        z = torch.matmul(eps, L.T)\n        return z.permute(0, 2, 1).contiguous()\n\n\n\n# VAE model\n\nclass VAETextLM(nn.Module):\n    def __init__(self, cfg: CFG, prior: PriorBase):\n        super().__init__()\n        self.cfg = cfg\n        self.prior = prior\n\n        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_emb = nn.Embedding(cfg.block_size, cfg.d_model)\n        self.drop = nn.Dropout(cfg.dropout)\n\n        self.encoder = TransformerEncoder(cfg.d_model, cfg.n_heads, cfg.n_layers, cfg.dropout)\n\n        self.to_mu = nn.Linear(cfg.d_model, cfg.z_dim)\n        self.to_logvar = nn.Linear(cfg.d_model, cfg.z_dim)\n\n        self.z_proj = nn.Linear(cfg.z_dim, cfg.d_model)\n        self.decoder = TransformerDecoderLM(cfg.d_model, cfg.n_heads, cfg.n_layers, cfg.dropout)\n\n        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n\n    def encode(self, x_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        B, T = x_ids.shape\n        tok = self.tok_emb(x_ids)\n        pos = self.pos_emb(torch.arange(T, device=x_ids.device))[None, :, :]\n        h = self.drop(tok + pos)\n        h = self.encoder(h)\n        mu = self.to_mu(h)\n        logvar = self.to_logvar(h).clamp(-12.0, 6.0)\n        return mu, logvar\n\n    def reparam(self, mu, logvar, n_samples: int):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn((n_samples,) + mu.shape, device=mu.device)\n        return mu[None] + eps * std[None]\n\n    def log_q_total(self, z, mu, logvar) -> torch.Tensor:\n        var = torch.exp(logvar)\n        log2pi = math.log(2 * math.pi)\n        diff2 = (z - mu[None]) ** 2\n        lq = -0.5 * (diff2 / var[None] + logvar[None] + log2pi)\n        return lq.sum(dim=(2, 3))\n\n    def log_q_per_dim(self, z_btD: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n        var = torch.exp(logvar)\n        log2pi = math.log(2 * math.pi)\n        diff2 = (z_btD - mu) ** 2\n        lq = -0.5 * (diff2 / var + logvar + log2pi)\n        return lq.sum(dim=1)\n\n    def decode_logits(self, x_ids: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n        B, T = x_ids.shape\n        tok = self.tok_emb(x_ids)\n        pos = self.pos_emb(torch.arange(T, device=x_ids.device))[None, :, :]\n        h = self.drop(tok + pos + self.z_proj(z))\n        h = self.decoder(h)\n        return self.lm_head(h)\n\n    def forward(self, x_ids: torch.Tensor, y_ids: torch.Tensor, beta: float) -> Dict[str, torch.Tensor]:\n        B, T = x_ids.shape\n        mu, logvar = self.encode(x_ids)\n        zS = self.reparam(mu, logvar, self.cfg.n_z_samples)\n\n        nll_list, logq_list, logp_list = [], [], []\n        last_logits = None\n        last_z = None\n\n        for s in range(self.cfg.n_z_samples):\n            z = zS[s]\n            logits = self.decode_logits(x_ids, z)\n            last_logits = logits\n            last_z = z\n\n            nll = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                y_ids.view(-1),\n                reduction=\"none\",\n            ).view(B, T).sum(dim=1)\n            nll_list.append(nll)\n\n            logq = self.log_q_total(zS[s:s+1], mu, logvar)[0]\n            logq_list.append(logq)\n\n            logp = self.prior.log_p(z)\n            logp_list.append(logp)\n\n        nll = torch.stack(nll_list, dim=0).mean(dim=0)\n        logq = torch.stack(logq_list, dim=0).mean(dim=0)\n        logp = torch.stack(logp_list, dim=0).mean(dim=0)\n\n        kl = (logq - logp)\n        elbo = -(nll + beta * kl)\n\n        nll_tok = nll.mean() / T\n        kl_tok = kl.mean() / T\n        elbo_tok = elbo.mean() / T\n\n        ppl = torch.exp(nll_tok.detach())\n        bits_per_tok = (nll_tok.detach() / math.log(2.0))\n\n        loss = -(elbo_tok)\n        return {\n            \"loss\": loss,\n            \"elbo_tok\": elbo_tok.detach(),\n            \"nll_tok\": nll_tok.detach(),\n            \"kl_tok\": kl_tok.detach(),\n            \"ppl\": ppl.detach(),\n            \"bits_per_tok\": bits_per_tok.detach(),\n            \"mu\": mu,\n            \"logvar\": logvar,\n            \"z_last\": last_z,\n            \"logits_last\": last_logits,\n        }\n\n    @torch.no_grad()\n    def generate(self, prompt_ids: torch.Tensor, max_new_tokens: int = 80, top_k: int = 0) -> torch.Tensor:\n        self.eval()\n        device = prompt_ids.device\n        B, Tp = prompt_ids.shape\n        total_T = min(self.cfg.block_size, Tp + max_new_tokens)\n\n        out = prompt_ids.clone()\n        z = self.prior.sample(B, total_T, self.cfg.z_dim, device=device)\n\n        for _ in range(max_new_tokens):\n            Tcur = out.size(1)\n            if Tcur >= total_T:\n                break\n            x = out[:, :Tcur]\n            zcur = z[:, :Tcur]\n            logits = self.decode_logits(x, zcur)\n            next_logits = logits[:, -1, :]\n\n            if top_k and top_k > 0:\n                vals, idx = torch.topk(next_logits, k=top_k, dim=-1)\n                probs = torch.zeros_like(next_logits).scatter_(-1, idx, F.softmax(vals, dim=-1))\n            else:\n                probs = F.softmax(next_logits, dim=-1)\n\n            next_id = torch.multinomial(probs, num_samples=1)\n            out = torch.cat([out, next_id], dim=1)\n\n        return out\n\n\n\n# Perf / GPU utilities\n\ndef _now():\n    return time.perf_counter()\n\ndef sync_if_cuda(device: str):\n    if isinstance(device, str) and device.startswith(\"cuda\") and torch.cuda.is_available():\n        torch.cuda.synchronize()\n\ndef cuda_mem_reset(device: str):\n    if isinstance(device, str) and device.startswith(\"cuda\") and torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.synchronize()\n\n@torch.no_grad()\ndef cuda_mem_snapshot_mb(device: str) -> Dict[str, float]:\n    if not (isinstance(device, str) and device.startswith(\"cuda\") and torch.cuda.is_available()):\n        return {\"peak_alloc_MB\": float(\"nan\"), \"peak_reserved_MB\": float(\"nan\")}\n    peak_alloc = torch.cuda.max_memory_allocated() / (1024**2)\n    peak_reserved = torch.cuda.max_memory_reserved() / (1024**2)\n    return {\"peak_alloc_MB\": float(peak_alloc), \"peak_reserved_MB\": float(peak_reserved)}\n\nclass RunningMean:\n    def __init__(self):\n        self.sum = 0.0\n        self.n = 0\n    def add(self, x: float):\n        if math.isfinite(x):\n            self.sum += float(x)\n            self.n += 1\n    def mean(self) -> float:\n        return self.sum / max(1, self.n)\n\n\n\n# Metric helpers (same as AR)\n\n@torch.no_grad()\ndef logits_entropy_and_topk_mass(logits: torch.Tensor, top_k: int = 50) -> Dict[str, float]:\n    probs = F.softmax(logits, dim=-1)\n    logp = torch.log(probs.clamp_min(1e-12))\n    ent = -(probs * logp).sum(dim=-1)\n    ent_mean = ent.mean().item()\n    if top_k and top_k > 0:\n        V = probs.size(-1)\n        vals, _ = torch.topk(probs, k=min(top_k, V), dim=-1)\n        topk_mass = vals.sum(dim=-1).mean().item()\n    else:\n        topk_mass = float(\"nan\")\n    return {\"tok_entropy_mean\": float(ent_mean), \"topk_mass_mean\": float(topk_mass)}\n\n@torch.no_grad()\ndef ece_from_logits(logits: torch.Tensor, targets: torch.Tensor, n_bins: int = 15) -> float:\n    probs = F.softmax(logits, dim=-1)\n    conf, pred = probs.max(dim=-1)\n    acc = (pred == targets).float()\n    conf = conf.reshape(-1)\n    acc = acc.reshape(-1)\n    bins = torch.linspace(0, 1, n_bins + 1, device=logits.device)\n    ece = torch.zeros((), device=logits.device)\n    for i in range(n_bins):\n        lo, hi = bins[i], bins[i + 1]\n        mask = (conf > lo) & (conf <= hi) if i > 0 else (conf >= lo) & (conf <= hi)\n        if mask.any():\n            ece = ece + (mask.float().mean()) * (acc[mask].mean() - conf[mask].mean()).abs()\n    return float(ece.item())\n\n@torch.no_grad()\ndef posterior_collapse_ratio(mu: torch.Tensor, logvar: torch.Tensor) -> Dict[str, float]:\n    std = torch.exp(0.5 * logvar)\n    mu_abs_mean = mu.abs().mean(dim=(0, 1))\n    std_mean = std.mean(dim=(0, 1))\n    collapsed = ((mu_abs_mean < 0.02) & ((std_mean - 1.0).abs() < 0.05)).float().mean().item()\n    return {\n        \"post_std_mean\": float(std.mean().item()),\n        \"post_mu_abs_mean\": float(mu.abs().mean().item()),\n        \"collapse_dim_frac\": float(collapsed),\n    }\n\n@torch.no_grad()\ndef latent_autocorr(x: torch.Tensor, lag: int = 1) -> float:\n    if x.size(1) <= lag:\n        return float(\"nan\")\n    a = x[:, :-lag, :].reshape(-1, x.size(-1))\n    b = x[:, lag:, :].reshape(-1, x.size(-1))\n    a = a - a.mean(dim=0, keepdim=True)\n    b = b - b.mean(dim=0, keepdim=True)\n    cov = (a * b).mean(dim=0)\n    va = (a * a).mean(dim=0).clamp_min(1e-8)\n    vb = (b * b).mean(dim=0).clamp_min(1e-8)\n    return float((cov / torch.sqrt(va * vb)).mean().item())\n\n@torch.no_grad()\ndef kl_per_dim_stats(model: VAETextLM, mu: torch.Tensor, logvar: torch.Tensor, z: torch.Tensor, eps: float) -> Dict[str, float]:\n    B, T, Dz = z.shape\n    logq_bd = model.log_q_per_dim(z, mu, logvar)\n    logp_bd = model.prior.log_p_per_dim(z)\n    kld_bd = (logq_bd - logp_bd) / max(1, T)\n    kld_d = kld_bd.mean(dim=0).detach().cpu().numpy()\n    return {\n        \"kldim_mean\": float(np.mean(kld_d)),\n        \"kldim_median\": float(np.median(kld_d)),\n        \"kldim_max\": float(np.max(kld_d)),\n        \"kldim_frac_below_eps\": float(np.mean(kld_d < eps)),\n    }\n\n@torch.no_grad()\ndef mi_proxy_batch(mu: torch.Tensor, logvar: torch.Tensor, z: torch.Tensor, max_components: int, max_points: int, seed: int = 0) -> float:\n    B, T, Dz = z.shape\n    C = B * T\n    if C == 0:\n        return float(\"nan\")\n    rng = np.random.default_rng(seed)\n\n    mu_c = mu.reshape(C, Dz)\n    lv_c = logvar.reshape(C, Dz)\n    z_c  = z.reshape(C, Dz)\n\n    comp_idx = np.arange(C)\n    pt_idx = np.arange(C)\n    if C > max_components:\n        comp_idx = rng.choice(comp_idx, size=max_components, replace=False)\n    if C > max_points:\n        pt_idx = rng.choice(pt_idx, size=max_points, replace=False)\n\n    mu_s = mu_c[torch.as_tensor(comp_idx, device=mu.device)]\n    lv_s = lv_c[torch.as_tensor(comp_idx, device=mu.device)]\n    var_s = torch.exp(lv_s).clamp_min(1e-12)\n\n    z_pts  = z_c [torch.as_tensor(pt_idx, device=mu.device)]\n    mu_pts = mu_c[torch.as_tensor(pt_idx, device=mu.device)]\n    lv_pts = lv_c[torch.as_tensor(pt_idx, device=mu.device)]\n    var_pts = torch.exp(lv_pts).clamp_min(1e-12)\n\n    log2pi = math.log(2 * math.pi)\n    lq_cond = -0.5 * (((z_pts - mu_pts) ** 2) / var_pts + lv_pts + log2pi).sum(dim=-1)\n    eq_logq_z_given_x = lq_cond.mean()\n\n    z_exp = z_pts[:, None, :]\n    mu_exp = mu_s[None, :, :]\n    lv_exp = lv_s[None, :, :]\n    var_exp = var_s[None, :, :]\n\n    lcomp = -0.5 * (((z_exp - mu_exp) ** 2) / var_exp + lv_exp + log2pi).sum(dim=-1)\n    lmix = torch.logsumexp(lcomp, dim=1) - math.log(lcomp.size(1))\n    eq_logq_z = lmix.mean()\n\n    return float((eq_logq_z_given_x - eq_logq_z).item())\n\ndef rd_points(nll_tok: float, kl_tok: float, betas: Tuple[float, ...]) -> Dict[str, float]:\n    out = {}\n    for b in betas:\n        out[f\"rd_elbo_tok_beta_{b:g}\"] = -(nll_tok + b * kl_tok)\n    out[\"rd_nll_tok\"] = float(nll_tok)\n    out[\"rd_kl_tok\"] = float(kl_tok)\n    return out\n\n\ndef distinct_ngrams(token_seq: List[int], n: int) -> float:\n    if len(token_seq) < n or n <= 0:\n        return 0.0\n    total = 0\n    uniq = set()\n    for i in range(len(token_seq) - n + 1):\n        uniq.add(tuple(token_seq[i:i+n]))\n        total += 1\n    return (len(uniq) / total) if total > 0 else 0.0\n\ndef repetition_rate(token_seq: List[int], n: int) -> float:\n    if len(token_seq) < n or n <= 0:\n        return 0.0\n    total = 0\n    counts = {}\n    for i in range(len(token_seq) - n + 1):\n        ng = tuple(token_seq[i:i+n])\n        counts[ng] = counts.get(ng, 0) + 1\n        total += 1\n    repeated = sum(c for c in counts.values() if c > 1)\n    return repeated / total if total > 0 else 0.0\n\n\n@torch.no_grad()\ndef generation_metrics(model: VAETextLM, tokenizer, device: str, n_prompts: int, max_new: int, top_k: int) -> Dict[str, Any]:\n    model.eval()\n    prompts = [\n        \"The meaning of life is\",\n        \"In the middle of the night\",\n        \"The government announced that\",\n        \"A new theory suggests\",\n        \"Once upon a time\",\n        \"The experiment shows\",\n        \"In a shocking discovery\",\n        \"The book describes\",\n        \"Scientists found that\",\n        \"The president said\",\n        \"In the future,\",\n        \"The story begins\",\n    ]\n\n    cuda_mem_reset(device)\n    sync_if_cuda(device)\n    t0 = _now()\n    gen_new_tokens_total = 0\n\n    per_prompt: List[Dict[str, Any]] = []\n    decoded_samples: List[str] = []\n\n    for i in range(n_prompts):\n        p = prompts[i % len(prompts)]\n        p_ids = tokenizer(p, return_tensors=\"pt\").input_ids.to(device)\n        if p_ids.size(1) > model.cfg.block_size // 2:\n            p_ids = p_ids[:, : model.cfg.block_size // 2]\n\n        out_ids = model.generate(p_ids, max_new_tokens=max_new, top_k=top_k)\n        seq = out_ids[0].tolist()\n\n        prompt_len = int(p_ids.size(1))\n        new_tokens = max(0, len(seq) - prompt_len)\n        gen_new_tokens_total += new_tokens\n\n        st = {\n            \"prompt\": p,\n            \"prompt_len\": prompt_len,\n            \"total_len\": len(seq),\n            \"new_tokens\": new_tokens,\n            \"rep2\": repetition_rate(seq, 2),\n            \"rep3\": repetition_rate(seq, 3),\n            \"distinct2\": distinct_ngrams(seq, 2),\n            \"distinct3\": distinct_ngrams(seq, 3),\n        }\n\n        txt = tokenizer.decode(seq[: min(len(seq), 250)], skip_special_tokens=True)\n        st[\"sample\"] = txt\n        per_prompt.append(st)\n\n        if i < 3:\n            decoded_samples.append(txt)\n\n    sync_if_cuda(device)\n    dt = max(1e-9, (_now() - t0))\n    mem = cuda_mem_snapshot_mb(device)\n\n    rep2 = float(np.mean([p[\"rep2\"] for p in per_prompt])) if per_prompt else 0.0\n    rep3 = float(np.mean([p[\"rep3\"] for p in per_prompt])) if per_prompt else 0.0\n    distinct2 = float(np.mean([p[\"distinct2\"] for p in per_prompt])) if per_prompt else 0.0\n    distinct3 = float(np.mean([p[\"distinct3\"] for p in per_prompt])) if per_prompt else 0.0\n    lengths = [p[\"total_len\"] for p in per_prompt]\n    newlens = [p[\"new_tokens\"] for p in per_prompt]\n\n    return {\n        \"gen_distinct2\": distinct2,\n        \"gen_distinct3\": distinct3,\n        \"gen_rep2\": rep2,\n        \"gen_rep3\": rep3,\n        \"gen_len_mean\": float(np.mean(lengths)) if lengths else 0.0,\n        \"gen_len_std\": float(np.std(lengths)) if lengths else 0.0,\n        \"gen_new_tokens_mean\": float(np.mean(newlens)) if newlens else 0.0,\n        \"gen_new_tokens_total\": int(gen_new_tokens_total),\n\n        \"time_gen_seconds_total\": float(dt),\n        \"time_gen_tokens_per_s\": float(gen_new_tokens_total / dt),\n        \"gpu_gen_peak_alloc_MB\": mem[\"peak_alloc_MB\"],\n        \"gpu_gen_peak_reserved_MB\": mem[\"peak_reserved_MB\"],\n\n        \"gen_per_prompt\": per_prompt,\n        \"gen_sample_0\": decoded_samples[0] if len(decoded_samples) > 0 else \"\",\n        \"gen_sample_1\": decoded_samples[1] if len(decoded_samples) > 1 else \"\",\n        \"gen_sample_2\": decoded_samples[2] if len(decoded_samples) > 2 else \"\",\n    }\n\n\n@torch.no_grad()\ndef eval_many_metrics(\n    model: VAETextLM,\n    loader: DataLoader,\n    device: str,\n    beta: float,\n    max_batches: int,\n    ece_bins: int,\n    gen_do: bool,\n    tokenizer=None,\n    gen_prompts: int = 12,\n    gen_max_new: int = 96,\n    gen_top_k: int = 50,\n    rd_betas: Tuple[float, ...] = (0.0, 1.0),\n    measure_perf: bool = True,\n    kldim_eps: float = 0.01,\n    mi_max_components: int = 512,\n    mi_max_points: int = 256,\n    mi_seed: int = 0,\n) -> Dict[str, Any]:\n    model.eval()\n\n    acc = {\n        \"nll_tok\": 0.0,\n        \"kl_tok\": 0.0,\n        \"elbo_tok\": 0.0,\n        \"tok_entropy_mean\": 0.0,\n        \"topk_mass_mean\": 0.0,\n        \"ece\": 0.0,\n        \"post_std_mean\": 0.0,\n        \"post_mu_abs_mean\": 0.0,\n        \"collapse_dim_frac\": 0.0,\n        \"mu_ac1\": 0.0,\n        \"mu_ac5\": 0.0,\n        \"z_ac1\": 0.0,\n        \"z_ac5\": 0.0,\n        \"kldim_mean\": 0.0,\n        \"kldim_median\": 0.0,\n        \"kldim_max\": 0.0,\n        \"kldim_frac_below_eps\": 0.0,\n        \"mi_proxy\": 0.0,\n    }\n    n = 0\n\n    step_ms = RunningMean()\n    tokens_per_s = RunningMean()\n    total_tokens = 0\n\n    if measure_perf:\n        cuda_mem_reset(device)\n\n    for i, (x, y) in enumerate(loader):\n        if i >= max_batches:\n            break\n        x = x.to(device)\n        y = y.to(device)\n\n        if measure_perf:\n            sync_if_cuda(device)\n            t0 = _now()\n\n        out = model(x, y, beta=beta)\n\n        if measure_perf:\n            sync_if_cuda(device)\n            dt = max(1e-9, (_now() - t0))\n            step_ms.add(1000.0 * dt)\n            B, T = x.shape\n            tok = int(B * T)\n            total_tokens += tok\n            tokens_per_s.add(tok / dt)\n\n        nll_tok = float(out[\"nll_tok\"])\n        kl_tok = float(out[\"kl_tok\"])\n        elbo_tok = float(out[\"elbo_tok\"])\n        if not (math.isfinite(nll_tok) and math.isfinite(kl_tok) and math.isfinite(elbo_tok)):\n            continue\n\n        logits = out[\"logits_last\"]\n        mu = out[\"mu\"].detach()\n        logvar = out[\"logvar\"].detach()\n        zlast = out[\"z_last\"].detach()\n\n        ent_topk = logits_entropy_and_topk_mass(logits, top_k=gen_top_k)\n        ece = ece_from_logits(logits, y, n_bins=ece_bins)\n        post = posterior_collapse_ratio(mu, logvar)\n\n        mu_ac1 = latent_autocorr(mu, lag=1)\n        mu_ac5 = latent_autocorr(mu, lag=5)\n        z_ac1 = latent_autocorr(zlast, lag=1)\n        z_ac5 = latent_autocorr(zlast, lag=5)\n\n        kld = kl_per_dim_stats(model, mu, logvar, zlast, eps=kldim_eps)\n        mi = mi_proxy_batch(mu, logvar, zlast, max_components=mi_max_components, max_points=mi_max_points, seed=mi_seed)\n\n        acc[\"nll_tok\"] += nll_tok\n        acc[\"kl_tok\"] += kl_tok\n        acc[\"elbo_tok\"] += elbo_tok\n        acc[\"tok_entropy_mean\"] += ent_topk[\"tok_entropy_mean\"]\n        acc[\"topk_mass_mean\"] += ent_topk[\"topk_mass_mean\"]\n        acc[\"ece\"] += ece\n        acc[\"post_std_mean\"] += post[\"post_std_mean\"]\n        acc[\"post_mu_abs_mean\"] += post[\"post_mu_abs_mean\"]\n        acc[\"collapse_dim_frac\"] += post[\"collapse_dim_frac\"]\n        acc[\"mu_ac1\"] += mu_ac1\n        acc[\"mu_ac5\"] += mu_ac5\n        acc[\"z_ac1\"] += z_ac1\n        acc[\"z_ac5\"] += z_ac5\n        acc[\"kldim_mean\"] += kld[\"kldim_mean\"]\n        acc[\"kldim_median\"] += kld[\"kldim_median\"]\n        acc[\"kldim_max\"] += kld[\"kldim_max\"]\n        acc[\"kldim_frac_below_eps\"] += kld[\"kldim_frac_below_eps\"]\n        acc[\"mi_proxy\"] += mi\n        n += 1\n\n    if n == 0:\n        return {k: float(\"nan\") for k in acc.keys()}\n\n    for k in acc:\n        acc[k] /= n\n\n    acc[\"ppl\"] = math.exp(acc[\"nll_tok\"])\n    acc[\"bits_per_tok\"] = acc[\"nll_tok\"] / math.log(2.0)\n    acc.update(rd_points(acc[\"nll_tok\"], acc[\"kl_tok\"], rd_betas))\n\n    if measure_perf:\n        mem = cuda_mem_snapshot_mb(device)\n        acc[\"time_eval_step_ms_mean\"] = step_ms.mean()\n        acc[\"time_eval_tokens_per_s_mean\"] = tokens_per_s.mean()\n        acc[\"gpu_eval_peak_alloc_MB\"] = mem[\"peak_alloc_MB\"]\n        acc[\"gpu_eval_peak_reserved_MB\"] = mem[\"peak_reserved_MB\"]\n        acc[\"eval_total_tokens_measured\"] = int(total_tokens)\n\n    if gen_do and tokenizer is not None:\n        acc.update(\n            generation_metrics(model, tokenizer, device=device, n_prompts=gen_prompts, max_new=gen_max_new, top_k=gen_top_k)\n        )\n\n    return acc\n\n\n\n# Schedules\n\ndef lr_schedule(step, cfg: CFG):\n    if step < cfg.warmup_steps:\n        return cfg.lr * (step / max(1, cfg.warmup_steps))\n    progress = (step - cfg.warmup_steps) / max(1, cfg.max_steps - cfg.warmup_steps)\n    return cfg.lr * (0.1 + 0.9 * 0.5 * (1.0 + math.cos(math.pi * progress)))\n\ndef beta_schedule(step, cfg: CFG):\n    if step >= cfg.beta_warmup_steps:\n        return cfg.beta_end\n    a = step / max(1, cfg.beta_warmup_steps)\n    return cfg.beta_start + a * (cfg.beta_end - cfg.beta_start)\n\n\n\n# Train (GP only)\n\ndef train_gp(cfg: CFG, train_loader, val_loader, tokenizer) -> Dict[str, Any]:\n    device = cfg.device\n    prior = PriorGP(cfg.gp_lengthscale, cfg.gp_sigma, cfg.gp_jitter)\n    model = VAETextLM(cfg, prior).to(device)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    use_amp = bool(cfg.amp and device.startswith(\"cuda\"))\n    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n\n    os.makedirs(cfg.out_dir, exist_ok=True)\n    run_stamp = int(time.time())\n    log_path = os.path.join(cfg.out_dir, f\"{cfg.run_name}_vae_gp_{run_stamp}.jsonl\")\n\n    best_val = float(\"inf\")\n    best: Dict[str, Any] = {}\n\n    t0 = time.time()\n    pbar = tqdm(total=cfg.max_steps, desc=\"train[vae_gp]\")\n\n    it = iter(train_loader)\n\n    train_step_ms = RunningMean()\n    train_tokens_per_s = RunningMean()\n    train_tokens_total = 0\n    cuda_mem_reset(device)\n\n    for step in range(1, cfg.max_steps + 1):\n        try:\n            x, y = next(it)\n        except StopIteration:\n            it = iter(train_loader)\n            x, y = next(it)\n\n        x, y = x.to(device), y.to(device)\n\n        beta = beta_schedule(step, cfg)\n        lr = lr_schedule(step, cfg)\n        for pg in opt.param_groups:\n            pg[\"lr\"] = lr\n\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        if use_amp:\n            sync_if_cuda(device)\n        t_step0 = _now()\n\n        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n            out = model(x, y, beta=beta)\n            loss = out[\"loss\"]\n\n        scaler.scale(loss).backward()\n        if cfg.grad_clip > 0:\n            scaler.unscale_(opt)\n            nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n        scaler.step(opt)\n        scaler.update()\n\n        if use_amp:\n            sync_if_cuda(device)\n        dt = max(1e-9, (_now() - t_step0))\n        train_step_ms.add(1000.0 * dt)\n        B, T = x.shape\n        tok = int(B * T)\n        train_tokens_total += tok\n        train_tokens_per_s.add(tok / dt)\n\n        if step % cfg.log_every == 0:\n            pbar.set_postfix({\n                \"loss\": f\"{loss.item():.4f}\",\n                \"nll_tok\": f\"{float(out['nll_tok']):.4f}\",\n                \"kl_tok\": f\"{float(out['kl_tok']):.4f}\",\n                \"ppl\": f\"{float(out['ppl']):.2f}\",\n                \"beta\": f\"{beta:.2f}\",\n                \"lr\": f\"{lr:.1e}\",\n            })\n\n        if step % cfg.eval_every == 0 or step == 1:\n            mem_train = cuda_mem_snapshot_mb(device)\n            train_perf = {\n                \"time_train_step_ms_mean_window\": train_step_ms.mean(),\n                \"time_train_tokens_per_s_mean_window\": train_tokens_per_s.mean(),\n                \"gpu_train_peak_alloc_MB_window\": mem_train[\"peak_alloc_MB\"],\n                \"gpu_train_peak_reserved_MB_window\": mem_train[\"peak_reserved_MB\"],\n                \"train_tokens_measured_window\": int(train_tokens_total),\n            }\n\n            train_eval = eval_many_metrics(\n                model, train_loader, device=device, beta=1.0,\n                max_batches=min(cfg.eval_train_batches, cfg.eval_max_batches),\n                ece_bins=cfg.ece_bins,\n                gen_do=False,\n                rd_betas=cfg.rd_betas,\n                measure_perf=True,\n                kldim_eps=cfg.kldim_eps,\n                mi_max_components=cfg.mi_max_components,\n                mi_max_points=cfg.mi_max_points,\n                mi_seed=cfg.mi_seed,\n                gen_top_k=cfg.eval_gen_top_k,\n            )\n            val_eval = eval_many_metrics(\n                model, val_loader, device=device, beta=1.0,\n                max_batches=cfg.eval_max_batches,\n                ece_bins=cfg.ece_bins,\n                gen_do=True, tokenizer=tokenizer,\n                gen_prompts=cfg.eval_gen_prompts,\n                gen_max_new=cfg.eval_gen_max_new,\n                gen_top_k=cfg.eval_gen_top_k,\n                rd_betas=cfg.rd_betas,\n                measure_perf=True,\n                kldim_eps=cfg.kldim_eps,\n                mi_max_components=cfg.mi_max_components,\n                mi_max_points=cfg.mi_max_points,\n                mi_seed=cfg.mi_seed,\n            )\n\n            val_nll = val_eval.get(\"nll_tok\", float(\"inf\"))\n            if math.isfinite(val_nll) and (val_nll < best_val):\n                best_val = float(val_nll)\n                best = {\n                    \"variant\": \"vae_gp\",\n                    \"step\": step,\n                    \"wall_s\": time.time() - t0,\n                    \"best_val_nll_tok\": best_val,\n                    \"train_perf_window\": train_perf,\n                    \"train\": train_eval,\n                    \"val\": val_eval,\n                }\n\n            rec = {\n                \"variant\": \"vae_gp\",\n                \"step\": step,\n                \"wall_s\": time.time() - t0,\n                \"lr\": float(lr),\n                \"beta\": float(beta),\n                \"train_perf_window\": train_perf,\n                \"train\": train_eval,\n                \"val\": val_eval,\n                \"best_val_nll_tok_so_far\": float(best_val),\n            }\n            with open(log_path, \"a\", encoding=\"utf-8\") as f:\n                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\n            print(\n                f\"[eval] vae_gp step={step:5d} \"\n                f\"train_tok/s={train_eval.get('time_eval_tokens_per_s_mean', float('nan')):.1f} \"\n                f\"val_tok/s={val_eval.get('time_eval_tokens_per_s_mean', float('nan')):.1f} \"\n                f\"gen_tok/s={val_eval.get('time_gen_tokens_per_s', float('nan')):.1f} \"\n                f\"val_ppl={val_eval.get('ppl', float('nan')):.2f} \"\n                f\"val_nll={val_eval.get('nll_tok', float('nan')):.4f} val_kl={val_eval.get('kl_tok', float('nan')):.4f} \"\n                f\"ece={val_eval.get('ece', float('nan')):.3f} ent={val_eval.get('tok_entropy_mean', float('nan')):.3f} \"\n                f\"mi={val_eval.get('mi_proxy', float('nan')):.3f} \"\n                f\"kldim_med={val_eval.get('kldim_median', float('nan')):.3f} \"\n                f\"train_peakMB={train_perf.get('gpu_train_peak_alloc_MB_window', float('nan')):.1f}\"\n            )\n\n            train_step_ms = RunningMean()\n            train_tokens_per_s = RunningMean()\n            train_tokens_total = 0\n            cuda_mem_reset(device)\n\n        pbar.update(1)\n\n    pbar.close()\n\n    if not best:\n        best = {\"variant\": \"vae_gp\", \"step\": cfg.max_steps, \"wall_s\": time.time() - t0, \"best_val_nll_tok\": best_val}\n    best[\"log_path\"] = log_path\n    return best\n\n\ndef main():\n    cfg = CFG()\n    set_seed(cfg.seed)\n\n    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n    tokenizer.model_max_length = int(1e9)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    cfg.vocab_size = len(tokenizer)\n\n    train_loader, val_loader = load_wt2_blocks(cfg, tokenizer)\n\n    best = train_gp(cfg, train_loader, val_loader, tokenizer)\n\n    os.makedirs(cfg.out_dir, exist_ok=True)\n    out_json = os.path.join(cfg.out_dir, f\"{cfg.run_name}_BEST_{int(time.time())}.json\")\n    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"cfg\": asdict(cfg), \"best\": best}, f, indent=2, ensure_ascii=False)\n\n    print(f\"\\nSaved BEST summary JSON: {out_json}\")\n    print(f\"Best log JSONL: {best.get('log_path','')}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T06:41:09.721315Z","iopub.execute_input":"2025-12-15T06:41:09.721580Z","iopub.status.idle":"2025-12-15T07:08:09.844188Z","shell.execute_reply.started":"2025-12-15T06:41:09.721558Z","shell.execute_reply":"2025-12-15T07:08:09.842707Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e56b5040b8548ecb5893d00b92e7b37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b4ba6c9bc1b46eca0310a66c632b37c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ad8c7cceefc4e968332ab102d31a566"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57717321060a43fa8fe5978b6f08e5f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af9092e8a934b1186b87c9079453cad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aec0c4ff61f0498ab9d903e5364de686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c180f3c7b414a20ab9133f098e85230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4d467516d5245f7977b02d6db8455d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0057ac80d2a64f71b2b386acdc7f7a3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02aff9b8164444ebbbfe5580a60c359b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d05866e83db4945918efd03a8a25695"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f10763859ae4fd081cc86db425806a5"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\ntrain[vae_gp]:   0%|          | 1/6000 [00:14<24:12:31, 14.53s/it]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step=    1 train_tok/s=60832.1 val_tok/s=62765.0 gen_tok/s=301.4 val_ppl=112179.78 val_nll=11.6279 val_kl=89006.5137 ece=0.002 ent=10.022 mi=49.252 kldim_med=1353.414 train_peakMB=3519.4\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:   7%|▋         | 400/6000 [01:58<6:23:37,  4.11s/it, loss=11.8596, nll_tok=6.3095, kl_tok=20.8127, ppl=549.79, beta=0.27, lr=3.0e-04]   ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step=  400 train_tok/s=62750.8 val_tok/s=62846.8 gen_tok/s=315.6 val_ppl=704.34 val_nll=6.5573 val_kl=18.4760 ece=0.018 ent=6.313 mi=2.280 kldim_med=0.289 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  13%|█▎        | 800/6000 [03:42<5:56:51,  4.12s/it, loss=12.7300, nll_tok=5.9386, kl_tok=12.7339, ppl=379.40, beta=0.53, lr=2.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step=  800 train_tok/s=62762.2 val_tok/s=62674.2 gen_tok/s=315.0 val_ppl=488.03 val_nll=6.1904 val_kl=12.7276 ece=0.022 ent=5.817 mi=0.406 kldim_med=0.199 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  20%|██        | 1200/6000 [05:26<5:29:33,  4.12s/it, loss=14.0158, nll_tok=5.7446, kl_tok=10.3391, ppl=312.49, beta=0.80, lr=2.8e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 1200 train_tok/s=62807.7 val_tok/s=62742.0 gen_tok/s=315.1 val_ppl=408.35 val_nll=6.0121 val_kl=10.1886 ece=0.021 ent=5.658 mi=0.153 kldim_med=0.159 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  27%|██▋       | 1600/6000 [07:11<5:03:36,  4.14s/it, loss=14.1751, nll_tok=5.4694, kl_tok=8.7057, ppl=237.31, beta=1.00, lr=2.7e-04] ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 1600 train_tok/s=62903.6 val_tok/s=62791.2 gen_tok/s=307.2 val_ppl=364.89 val_nll=5.8996 val_kl=8.2840 ece=0.024 ent=5.516 mi=0.068 kldim_med=0.129 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  33%|███▎      | 2000/6000 [08:56<4:36:11,  4.14s/it, loss=12.7825, nll_tok=5.1551, kl_tok=7.6274, ppl=173.32, beta=1.00, lr=2.4e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 2000 train_tok/s=62663.1 val_tok/s=62723.5 gen_tok/s=309.3 val_ppl=337.58 val_nll=5.8218 val_kl=7.1116 ece=0.033 ent=5.047 mi=0.040 kldim_med=0.111 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  40%|████      | 2400/6000 [10:40<4:08:25,  4.14s/it, loss=11.5538, nll_tok=4.8017, kl_tok=6.7520, ppl=121.72, beta=1.00, lr=2.2e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 2400 train_tok/s=62591.2 val_tok/s=62742.2 gen_tok/s=311.8 val_ppl=321.30 val_nll=5.7724 val_kl=6.3504 ece=0.029 ent=5.002 mi=0.029 kldim_med=0.099 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  47%|████▋     | 2800/6000 [12:25<3:40:47,  4.14s/it, loss=10.9547, nll_tok=4.8506, kl_tok=6.1041, ppl=127.82, beta=1.00, lr=1.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 2800 train_tok/s=62692.0 val_tok/s=62761.9 gen_tok/s=310.8 val_ppl=314.71 val_nll=5.7516 val_kl=5.8163 ece=0.038 ent=4.716 mi=0.025 kldim_med=0.091 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  53%|█████▎    | 3200/6000 [14:10<3:13:22,  4.14s/it, loss=10.2535, nll_tok=4.7033, kl_tok=5.5503, ppl=110.31, beta=1.00, lr=1.6e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 3200 train_tok/s=62595.2 val_tok/s=62512.2 gen_tok/s=312.6 val_ppl=309.94 val_nll=5.7364 val_kl=5.3650 ece=0.036 ent=4.634 mi=0.023 kldim_med=0.084 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  60%|██████    | 3600/6000 [15:55<2:45:02,  4.13s/it, loss=9.6672, nll_tok=4.5063, kl_tok=5.1609, ppl=90.59, beta=1.00, lr=1.3e-04]  ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 3600 train_tok/s=62803.6 val_tok/s=62863.7 gen_tok/s=312.3 val_ppl=305.89 val_nll=5.7232 val_kl=5.0337 ece=0.040 ent=4.543 mi=0.022 kldim_med=0.079 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  67%|██████▋   | 4000/6000 [17:39<2:17:04,  4.11s/it, loss=9.0829, nll_tok=4.2055, kl_tok=4.8774, ppl=67.06, beta=1.00, lr=1.0e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 4000 train_tok/s=62708.9 val_tok/s=62869.2 gen_tok/s=316.2 val_ppl=310.82 val_nll=5.7392 val_kl=4.8337 ece=0.049 ent=4.378 mi=0.013 kldim_med=0.076 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  73%|███████▎  | 4400/6000 [19:24<1:50:01,  4.13s/it, loss=8.8465, nll_tok=4.1410, kl_tok=4.7056, ppl=62.86, beta=1.00, lr=7.9e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 4400 train_tok/s=62639.1 val_tok/s=62802.6 gen_tok/s=313.0 val_ppl=313.63 val_nll=5.7482 val_kl=4.6256 ece=0.051 ent=4.284 mi=0.013 kldim_med=0.072 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  80%|████████  | 4800/6000 [21:09<1:22:40,  4.13s/it, loss=8.5889, nll_tok=4.0517, kl_tok=4.5372, ppl=57.49, beta=1.00, lr=5.8e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 4800 train_tok/s=62712.4 val_tok/s=62664.5 gen_tok/s=314.0 val_ppl=314.73 val_nll=5.7517 val_kl=4.5375 ece=0.051 ent=4.242 mi=0.013 kldim_med=0.071 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  87%|████████▋ | 5200/6000 [22:53<54:57,  4.12s/it, loss=8.5168, nll_tok=4.0733, kl_tok=4.4435, ppl=58.75, beta=1.00, lr=4.3e-05]  ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 5200 train_tok/s=62801.5 val_tok/s=62724.8 gen_tok/s=313.3 val_ppl=318.36 val_nll=5.7632 val_kl=4.4181 ece=0.059 ent=4.143 mi=0.013 kldim_med=0.069 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  93%|█████████▎| 5600/6000 [24:38<27:27,  4.12s/it, loss=8.5446, nll_tok=4.1746, kl_tok=4.3700, ppl=65.01, beta=1.00, lr=3.3e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 5600 train_tok/s=62720.8 val_tok/s=62764.2 gen_tok/s=313.9 val_ppl=320.58 val_nll=5.7701 val_kl=4.3187 ece=0.059 ent=4.143 mi=0.014 kldim_med=0.068 train_peakMB=3981.2\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]: 100%|██████████| 6000/6000 [26:23<00:00,  3.79it/s, loss=8.2149, nll_tok=3.9117, kl_tok=4.3032, ppl=49.98, beta=1.00, lr=3.0e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 6000 train_tok/s=62855.9 val_tok/s=62756.8 gen_tok/s=316.0 val_ppl=322.84 val_nll=5.7772 val_kl=4.2508 ece=0.057 ent=4.135 mi=0.012 kldim_med=0.066 train_peakMB=3981.2\n\nSaved BEST summary JSON: runs/wt2_latentGP_BEST_1765782489.json\nBest log JSONL: runs/wt2_latentGP_vae_gp_1765780906.jsonl\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# GP-VAE (iid / GP / AR priors) on WikiText-2 blocks\n# \"NO-DRAMA\" FULL VERSION + MANY METRICS\n#\n# compute-cost metrics:\n# - Training time: step_ms, tokens/s (windowed between evals)\n# - Inference time: eval tokens/s (teacher forcing) + generation tokens/s\n# - GPU memory: peak allocated + reserved (train/eval/gen)\n#\n# extra metrics:\n# - Bits-back style diagnostics: KL per dim stats (mean/median/max) + frac(KLdim < eps)\n# - MI(x,z) proxy: E_q[log q(z|x)] - E_q[log q(z)] approx (batch-based, subsampled)\n# - Rate–Distortion points: log (NLL, KL) + ELBO(beta) for multiple fixed betas\n# - Generation: per-prompt length + repetition (not just aggregated) + samples\n#\n# Notes:\n# - Tokenization long-seq warning suppressed by setting tokenizer.model_max_length huge.\n# - GP prior supports variable T safely; includes cholesky fallback with extra jitter.\n# ============================================================\n\nimport math\nimport os\nimport time\nimport json\nimport random\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, Tuple, List, Any, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n\n# Environment / warnings\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# More deterministic-ish behavior (still not perfect with transformers + GPU)\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\n\n\n# Config\n\n@dataclass\nclass CFG:\n    seed: int = 0\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Data\n    dataset_name: str = \"wikitext\"\n    dataset_config: str = \"wikitext-2-raw-v1\"\n    tokenizer_name: str = \"gpt2\"\n    block_size: int = 256\n    batch_size: int = 16\n    num_workers: int = 2\n    pin_memory: bool = True\n\n    # Model\n    vocab_size: int = 50257  # overwritten after tokenizer load\n    d_model: int = 384\n    n_layers: int = 6\n    n_heads: int = 6\n    dropout: float = 0.1\n\n    # Latent\n    z_dim: int = 64\n    n_z_samples: int = 1  # MC samples for KL estimate\n\n    # Training\n    lr: float = 3e-4\n    weight_decay: float = 0.01\n    max_steps: int = 6000\n    warmup_steps: int = 300\n    grad_clip: float = 1.0\n    eval_every: int = 400\n    log_every: int = 50\n    amp: bool = True\n\n    # KL anneal\n    beta_start: float = 0.0\n    beta_end: float = 1.0\n    beta_warmup_steps: int = 1500\n\n    # GP prior (RBF kernel)\n    gp_lengthscale: float = 25.0\n    gp_sigma: float = 1.0\n    gp_jitter: float = 1e-3  # safer default than 1e-4\n\n    # AR prior\n    ar_init_rho: float = 0.95\n    ar_sigma: float = 0.5\n\n    # Eval controls\n    eval_max_batches: int = 80\n    eval_train_batches: int = 20\n    eval_gen_prompts: int = 12\n    eval_gen_max_new: int = 96\n    eval_gen_top_k: int = 50\n\n    # ECE calibration\n    ece_bins: int = 15\n\n    # Bits-back diagnostics\n    kldim_eps: float = 0.01  # threshold for \"inactive\" dims (per-token)\n\n    # MI proxy (batch-based, subsampled for cost)\n    mi_max_components: int = 512   # max components in mixture for q(z)\n    mi_max_points: int = 256       # max sample points z to evaluate\n    mi_seed: int = 0\n\n    # Rate–Distortion points (log derived ELBO for multiple betas)\n    rd_betas: Tuple[float, ...] = (0.0, 0.1, 0.5, 1.0, 2.0)\n\n    # Logging / saving\n    out_dir: str = \"runs\"\n    run_name: str = \"wt2_gpvae_compare\"\n\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\n\n# Dataset: GPT-2 tokenizer blocks\n\nclass LMBlocks(Dataset):\n    def __init__(self, token_ids: List[int], block_size: int):\n        self.block_size = block_size\n        n = (len(token_ids) - 1) // block_size  # keep only full blocks\n        self.data = token_ids[: n * block_size + 1]\n\n    def __len__(self):\n        return (len(self.data) - 1) // self.block_size\n\n    def __getitem__(self, idx):\n        i = idx * self.block_size\n        x = torch.tensor(self.data[i : i + self.block_size], dtype=torch.long)\n        y = torch.tensor(self.data[i + 1 : i + self.block_size + 1], dtype=torch.long)\n        return x, y\n\n\ndef tokenize_split_streaming(tokenizer, texts: List[str], chunk_chars: int = 200_000) -> List[int]:\n    \"\"\"\n    Tokenize a large split without building one gigantic string.\n\n    Also avoids HF \"sequence length > model max length\" warnings by\n    using a huge tokenizer.model_max_length (set in main).\n    \"\"\"\n    ids: List[int] = []\n    buf: List[str] = []\n    cur_len = 0\n    sep = \"\\n\\n\"\n\n    for t in texts:\n        if not t:\n            continue\n        add = t + sep\n        buf.append(add)\n        cur_len += len(add)\n        if cur_len >= chunk_chars:\n            chunk = \"\".join(buf)\n            ids.extend(tokenizer.encode(chunk))\n            buf = []\n            cur_len = 0\n\n    if buf:\n        chunk = \"\".join(buf)\n        ids.extend(tokenizer.encode(chunk))\n\n    return ids\n\n\ndef load_wt2_blocks(cfg: CFG, tokenizer) -> Tuple[DataLoader, DataLoader]:\n    ds = load_dataset(cfg.dataset_name, cfg.dataset_config)\n\n    train_ids = tokenize_split_streaming(tokenizer, ds[\"train\"][\"text\"])\n    val_ids = tokenize_split_streaming(tokenizer, ds[\"validation\"][\"text\"])\n\n    train_set = LMBlocks(train_ids, cfg.block_size)\n    val_set = LMBlocks(val_ids, cfg.block_size)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=cfg.batch_size,\n        shuffle=True,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory,\n        drop_last=True,\n        persistent_workers=(cfg.num_workers > 0),\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=cfg.batch_size,\n        shuffle=False,\n        num_workers=cfg.num_workers,\n        pin_memory=cfg.pin_memory,\n        drop_last=False,\n        persistent_workers=(cfg.num_workers > 0),\n    )\n    return train_loader, val_loader\n\n\n\n# Transformer blocks\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, d_model, n_heads, n_layers, dropout):\n        super().__init__()\n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.enc = nn.TransformerEncoder(layer, num_layers=n_layers)\n\n    def forward(self, x, src_key_padding_mask=None):\n        return self.enc(x, src_key_padding_mask=src_key_padding_mask)\n\n\nclass TransformerDecoderLM(nn.Module):\n    \"\"\"\n    Causal LM decoder implemented as TransformerEncoder with a causal mask.\n    \"\"\"\n    def __init__(self, d_model, n_heads, n_layers, dropout):\n        super().__init__()\n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=4 * d_model,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.dec = nn.TransformerEncoder(layer, num_layers=n_layers)\n\n    def forward(self, x):\n        _, T, _ = x.shape\n        attn_mask = torch.full((T, T), float(\"-inf\"), device=x.device)\n        attn_mask = torch.triu(attn_mask, diagonal=1)\n        return self.dec(x, mask=attn_mask)\n\n\n\n# Priors\n\nclass PriorBase(nn.Module):\n    def log_p(self, z: torch.Tensor) -> torch.Tensor:\n        \"\"\" z: (B,T,Dz) -> log p(z): (B,) \"\"\"\n        raise NotImplementedError\n\n    def log_p_per_dim(self, z: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        z: (B,T,Dz) -> returns per-dim log p(z_{:, :, d}) summed over time: (B,Dz)\n        Used for KL-per-dim diagnostics. Must respect factorization over dims.\n        \"\"\"\n        raise NotImplementedError\n\n    @torch.no_grad()\n    def sample(self, B: int, T: int, Dz: int, device: str) -> torch.Tensor:\n        raise NotImplementedError\n\n\nclass PriorIID(PriorBase):\n    def log_p(self, z):\n        return (-0.5 * (z**2 + math.log(2 * math.pi))).sum(dim=(1, 2))\n\n    def log_p_per_dim(self, z):\n        # sum over time per dim\n        # (B,T,Dz) -> (B,Dz)\n        return (-0.5 * (z**2 + math.log(2 * math.pi))).sum(dim=1)\n\n    @torch.no_grad()\n    def sample(self, B, T, Dz, device):\n        return torch.randn(B, T, Dz, device=device)\n\n\nclass PriorAR(PriorBase):\n    \"\"\"\n    p(z1)=N(0,I), p(zt|z_{t-1})=N(rho z_{t-1}, sigma^2 I)\n    rho learnable scalar.\n    \"\"\"\n    def __init__(self, Dz: int, rho_init: float, sigma: float):\n        super().__init__()\n        self.logit_rho = nn.Parameter(torch.logit(torch.tensor(float(rho_init))))\n        self.sigma = float(sigma)\n        self.Dz = Dz\n\n    def rho(self):\n        return torch.sigmoid(self.logit_rho).clamp(1e-4, 0.9999)\n\n    def log_p(self, z):\n        B, T, Dz = z.shape\n        lp1 = (-0.5 * (z[:, 0] ** 2 + math.log(2 * math.pi))).sum(dim=1)\n        if T == 1:\n            return lp1\n        rho = self.rho()\n        sigma2 = self.sigma ** 2\n        resid = z[:, 1:] - rho * z[:, :-1]\n        lp = (-0.5 * ((resid**2) / sigma2 + math.log(2 * math.pi * sigma2))).sum(dim=(1, 2))\n        return lp1 + lp\n\n    def log_p_per_dim(self, z):\n        # (B,T,Dz) -> (B,Dz)\n        B, T, Dz = z.shape\n        lp1 = (-0.5 * (z[:, 0] ** 2 + math.log(2 * math.pi)))  # (B,Dz)\n        if T == 1:\n            return lp1\n        rho = self.rho()\n        sigma2 = self.sigma ** 2\n        resid = z[:, 1:] - rho * z[:, :-1]  # (B,T-1,Dz)\n        lp = -0.5 * ((resid**2) / sigma2 + math.log(2 * math.pi * sigma2))  # (B,T-1,Dz)\n        return lp1 + lp.sum(dim=1)\n\n    @torch.no_grad()\n    def sample(self, B, T, Dz, device):\n        rho = float(self.rho().item())\n        z = torch.zeros(B, T, Dz, device=device)\n        z[:, 0] = torch.randn(B, Dz, device=device)\n        for t in range(1, T):\n            z[:, t] = rho * z[:, t - 1] + self.sigma * torch.randn(B, Dz, device=device)\n        return z\n\n\nclass PriorGP(PriorBase):\n    \"\"\"\n    GP prior per latent dim: z_{:, :, d} ~ N(0, K_T) with RBF kernel over time.\n    Kernel built dynamically per T (variable-length safe).\n    \"\"\"\n    def __init__(self, lengthscale: float, sigma: float, jitter: float):\n        super().__init__()\n        self.log_lengthscale = nn.Parameter(torch.log(torch.tensor(float(lengthscale))))\n        self.log_sigma = nn.Parameter(torch.log(torch.tensor(float(sigma))))\n        self.jitter = float(jitter)\n\n    def _kernel(self, T: int, device):\n        t = torch.arange(T, device=device).float()\n        dt2 = (t[:, None] - t[None, :]) ** 2\n        ell = torch.exp(self.log_lengthscale).clamp(1e-3, 1e6)\n        sig = torch.exp(self.log_sigma).clamp(1e-6, 1e6)\n        K = (sig**2) * torch.exp(-0.5 * dt2 / (ell**2))\n        K = K + self.jitter * torch.eye(T, device=device)\n        return K\n\n    def _chol(self, K: torch.Tensor) -> torch.Tensor:\n        try:\n            return torch.linalg.cholesky(K)\n        except RuntimeError:\n            # defensive: add more jitter and retry\n            T = K.size(0)\n            K2 = K + (10.0 * self.jitter) * torch.eye(T, device=K.device)\n            return torch.linalg.cholesky(K2)\n\n    def log_p(self, z):\n        B, T, Dz = z.shape\n        K = self._kernel(T, z.device)\n        L = self._chol(K)\n\n        z_td = z.transpose(1, 2).contiguous().view(B * Dz, T, 1)\n        alpha = torch.cholesky_solve(z_td, L)\n        quad = (z_td * alpha).sum(dim=(1, 2))  # (B*Dz,)\n        logdet = 2.0 * torch.log(torch.diagonal(L)).sum()\n        const = T * math.log(2 * math.pi)\n        lp = -0.5 * (quad + logdet + const)    # (B*Dz,)\n        return lp.view(B, Dz).sum(dim=1)\n\n    def log_p_per_dim(self, z):\n        # (B,T,Dz) -> (B,Dz) each dim independently as MVN over time\n        B, T, Dz = z.shape\n        K = self._kernel(T, z.device)\n        L = self._chol(K)\n        logdet = 2.0 * torch.log(torch.diagonal(L)).sum()\n        const = T * math.log(2 * math.pi)\n\n        # Solve per (B,Dz) streams: reshape to (B*Dz,T,1)\n        z_td = z.transpose(1, 2).contiguous().view(B * Dz, T, 1)\n        alpha = torch.cholesky_solve(z_td, L)            # (B*Dz,T,1)\n        quad = (z_td * alpha).sum(dim=(1, 2))            # (B*Dz,)\n        lp = -0.5 * (quad + logdet + const)              # (B*Dz,)\n        return lp.view(B, Dz)\n\n    @torch.no_grad()\n    def sample(self, B, T, Dz, device):\n        K = self._kernel(T, device)\n        L = self._chol(K)\n        eps = torch.randn(B, Dz, T, device=device)\n        z = torch.matmul(eps, L.T)                       # (B,Dz,T)\n        return z.permute(0, 2, 1).contiguous()           # (B,T,Dz)\n\n\n\n# VAE for LM\n\nclass VAETextLM(nn.Module):\n    def __init__(self, cfg: CFG, prior: PriorBase):\n        super().__init__()\n        self.cfg = cfg\n        self.prior = prior\n\n        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_emb = nn.Embedding(cfg.block_size, cfg.d_model)\n        self.drop = nn.Dropout(cfg.dropout)\n\n        self.encoder = TransformerEncoder(cfg.d_model, cfg.n_heads, cfg.n_layers, cfg.dropout)\n\n        self.to_mu = nn.Linear(cfg.d_model, cfg.z_dim)\n        self.to_logvar = nn.Linear(cfg.d_model, cfg.z_dim)\n\n        self.z_proj = nn.Linear(cfg.z_dim, cfg.d_model)\n        self.decoder = TransformerDecoderLM(cfg.d_model, cfg.n_heads, cfg.n_layers, cfg.dropout)\n\n        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n\n    def encode(self, x_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        B, T = x_ids.shape\n        tok = self.tok_emb(x_ids)\n        pos = self.pos_emb(torch.arange(T, device=x_ids.device))[None, :, :]\n        h = self.drop(tok + pos)\n        h = self.encoder(h)\n        mu = self.to_mu(h)\n        logvar = self.to_logvar(h).clamp(-12.0, 6.0)\n        return mu, logvar\n\n    def reparam(self, mu, logvar, n_samples: int):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn((n_samples,) + mu.shape, device=mu.device)\n        return mu[None] + eps * std[None]  # (S,B,T,Dz)\n\n    def log_q_total(self, z, mu, logvar) -> torch.Tensor:\n        \"\"\"\n        Total log q(z|x) for sampled z.\n        z: (S,B,T,Dz); mu/logvar: (B,T,Dz)\n        returns: (S,B)\n        \"\"\"\n        var = torch.exp(logvar)\n        log2pi = math.log(2 * math.pi)\n        diff2 = (z - mu[None]) ** 2\n        lq = -0.5 * (diff2 / var[None] + logvar[None] + log2pi)\n        return lq.sum(dim=(2, 3))  # (S,B)\n\n    def log_q_per_dim(self, z_btD: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Per-dim log q(z|x) summed over time.\n        z_btD: (B,T,Dz); mu/logvar: (B,T,Dz)\n        returns: (B,Dz)\n        \"\"\"\n        var = torch.exp(logvar)\n        log2pi = math.log(2 * math.pi)\n        diff2 = (z_btD - mu) ** 2\n        lq = -0.5 * (diff2 / var + logvar + log2pi)  # (B,T,Dz)\n        return lq.sum(dim=1)  # (B,Dz)\n\n    def decode_logits(self, x_ids: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n        B, T = x_ids.shape\n        tok = self.tok_emb(x_ids)\n        pos = self.pos_emb(torch.arange(T, device=x_ids.device))[None, :, :]\n        h = self.drop(tok + pos + self.z_proj(z))\n        h = self.decoder(h)\n        logits = self.lm_head(h)\n        return logits\n\n    def forward(self, x_ids: torch.Tensor, y_ids: torch.Tensor, beta: float) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Returns averages per-token + extra tensors for metric computation.\n        \"\"\"\n        B, T = x_ids.shape\n        mu, logvar = self.encode(x_ids)\n        zS = self.reparam(mu, logvar, self.cfg.n_z_samples)\n\n        nll_list, logq_list, logp_list = [], [], []\n        last_logits = None\n        last_z = None\n\n        for s in range(self.cfg.n_z_samples):\n            z = zS[s]\n            logits = self.decode_logits(x_ids, z)\n            last_logits = logits\n            last_z = z\n\n            nll = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                y_ids.view(-1),\n                reduction=\"none\",\n            ).view(B, T).sum(dim=1)  # (B,)\n            nll_list.append(nll)\n\n            logq = self.log_q_total(zS[s:s+1], mu, logvar)[0]  # (B,)\n            logq_list.append(logq)\n\n            logp = self.prior.log_p(z)  # (B,)\n            logp_list.append(logp)\n\n        nll = torch.stack(nll_list, dim=0).mean(dim=0)   # (B,)\n        logq = torch.stack(logq_list, dim=0).mean(dim=0)\n        logp = torch.stack(logp_list, dim=0).mean(dim=0)\n\n        kl = (logq - logp)                               # (B,)\n        elbo = -(nll + beta * kl)                        # (B,)\n\n        n_tokens = T\n        nll_tok = nll.mean() / n_tokens\n        kl_tok = kl.mean() / n_tokens\n        elbo_tok = elbo.mean() / n_tokens\n\n        ppl = torch.exp(nll_tok.detach())\n        bits_per_tok = (nll_tok.detach() / math.log(2.0))\n\n        loss = -(elbo_tok)\n        return {\n            \"loss\": loss,\n            \"elbo_tok\": elbo_tok.detach(),\n            \"nll_tok\": nll_tok.detach(),\n            \"kl_tok\": kl_tok.detach(),\n            \"ppl\": ppl.detach(),\n            \"bits_per_tok\": bits_per_tok.detach(),\n            # extras\n            \"mu\": mu,\n            \"logvar\": logvar,\n            \"z_last\": last_z,\n            \"logits_last\": last_logits,\n        }\n\n    @torch.no_grad()\n    def generate(self, prompt_ids: torch.Tensor, max_new_tokens: int = 80, top_k: int = 0) -> torch.Tensor:\n        self.eval()\n        device = prompt_ids.device\n        B, Tp = prompt_ids.shape\n        total_T = min(self.cfg.block_size, Tp + max_new_tokens)\n\n        out = prompt_ids.clone()\n        z = self.prior.sample(B, total_T, self.cfg.z_dim, device=device)\n\n        for _ in range(max_new_tokens):\n            Tcur = out.size(1)\n            if Tcur >= total_T:\n                break\n\n            x = out[:, :Tcur]\n            zcur = z[:, :Tcur]\n            logits = self.decode_logits(x, zcur)\n            next_logits = logits[:, -1, :]\n\n            if top_k and top_k > 0:\n                vals, idx = torch.topk(next_logits, k=top_k, dim=-1)\n                probs = torch.zeros_like(next_logits).scatter_(-1, idx, F.softmax(vals, dim=-1))\n            else:\n                probs = F.softmax(next_logits, dim=-1)\n\n            next_id = torch.multinomial(probs, num_samples=1)\n            out = torch.cat([out, next_id], dim=1)\n\n        return out\n\n\n\n# Perf / GPU utilities\n\ndef _now():\n    return time.perf_counter()\n\ndef sync_if_cuda(device: str):\n    if isinstance(device, str) and device.startswith(\"cuda\") and torch.cuda.is_available():\n        torch.cuda.synchronize()\n\ndef cuda_mem_reset(device: str):\n    if isinstance(device, str) and device.startswith(\"cuda\") and torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.synchronize()\n\n@torch.no_grad()\ndef cuda_mem_snapshot_mb(device: str) -> Dict[str, float]:\n    if not (isinstance(device, str) and device.startswith(\"cuda\") and torch.cuda.is_available()):\n        return {\"peak_alloc_MB\": float(\"nan\"), \"peak_reserved_MB\": float(\"nan\")}\n    peak_alloc = torch.cuda.max_memory_allocated() / (1024**2)\n    peak_reserved = torch.cuda.max_memory_reserved() / (1024**2)\n    return {\"peak_alloc_MB\": float(peak_alloc), \"peak_reserved_MB\": float(peak_reserved)}\n\nclass RunningMean:\n    def __init__(self):\n        self.sum = 0.0\n        self.n = 0\n    def add(self, x: float):\n        if math.isfinite(x):\n            self.sum += float(x)\n            self.n += 1\n    def mean(self) -> float:\n        return self.sum / max(1, self.n)\n\n\n\n# Metric helpers\n\ndef safe_float(x: Any) -> float:\n    try:\n        return float(x)\n    except Exception:\n        return float(\"nan\")\n\n@torch.no_grad()\ndef logits_entropy_and_topk_mass(logits: torch.Tensor, top_k: int = 50) -> Dict[str, float]:\n    \"\"\"\n    logits: (B,T,V)\n    Returns mean token entropy and mean probability mass of top-k.\n    \"\"\"\n    probs = F.softmax(logits, dim=-1)\n    logp = torch.log(probs.clamp_min(1e-12))\n    ent = -(probs * logp).sum(dim=-1)  # (B,T)\n    ent_mean = ent.mean().item()\n\n    if top_k and top_k > 0:\n        V = probs.size(-1)\n        vals, _ = torch.topk(probs, k=min(top_k, V), dim=-1)\n        topk_mass = vals.sum(dim=-1).mean().item()\n    else:\n        topk_mass = float(\"nan\")\n\n    return {\"tok_entropy_mean\": float(ent_mean), \"topk_mass_mean\": float(topk_mass)}\n\n@torch.no_grad()\ndef ece_from_logits(logits: torch.Tensor, targets: torch.Tensor, n_bins: int = 15) -> float:\n    \"\"\"\n    Expected Calibration Error for next-token prediction.\n    logits: (B,T,V), targets: (B,T)\n    \"\"\"\n    probs = F.softmax(logits, dim=-1)\n    conf, pred = probs.max(dim=-1)      # (B,T)\n    acc = (pred == targets).float()\n\n    conf = conf.reshape(-1)\n    acc = acc.reshape(-1)\n\n    bins = torch.linspace(0, 1, n_bins + 1, device=logits.device)\n    ece = torch.zeros((), device=logits.device)\n    for i in range(n_bins):\n        lo, hi = bins[i], bins[i + 1]\n        mask = (conf > lo) & (conf <= hi) if i > 0 else (conf >= lo) & (conf <= hi)\n        if mask.any():\n            bin_acc = acc[mask].mean()\n            bin_conf = conf[mask].mean()\n            ece = ece + (mask.float().mean()) * (bin_acc - bin_conf).abs()\n    return float(ece.item())\n\n@torch.no_grad()\ndef posterior_collapse_ratio(mu: torch.Tensor, logvar: torch.Tensor) -> Dict[str, float]:\n    \"\"\"\n    Simple collapse heuristics:\n    - mean std (posterior)\n    - fraction of dims with small mean(|mu|) and std close to 1\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    mu_abs_mean = mu.abs().mean(dim=(0, 1))   # (Dz,)\n    std_mean = std.mean(dim=(0, 1))           # (Dz,)\n\n    collapsed = ((mu_abs_mean < 0.02) & ((std_mean - 1.0).abs() < 0.05)).float().mean().item()\n    return {\n        \"post_std_mean\": float(std.mean().item()),\n        \"post_mu_abs_mean\": float(mu.abs().mean().item()),\n        \"collapse_dim_frac\": float(collapsed),\n    }\n\n@torch.no_grad()\ndef latent_autocorr(x: torch.Tensor, lag: int = 1) -> float:\n    \"\"\"\n    x: (B,T,D) -> mean autocorrelation across dims\n    \"\"\"\n    if x.size(1) <= lag:\n        return float(\"nan\")\n    a = x[:, :-lag, :].reshape(-1, x.size(-1))\n    b = x[:, lag:, :].reshape(-1, x.size(-1))\n    a = a - a.mean(dim=0, keepdim=True)\n    b = b - b.mean(dim=0, keepdim=True)\n    cov = (a * b).mean(dim=0)\n    va = (a * a).mean(dim=0).clamp_min(1e-8)\n    vb = (b * b).mean(dim=0).clamp_min(1e-8)\n    corr = (cov / torch.sqrt(va * vb)).mean().item()\n    return float(corr)\n\ndef distinct_ngrams(token_seq: List[int], n: int) -> float:\n    if len(token_seq) < n or n <= 0:\n        return 0.0\n    total = 0\n    uniq = set()\n    for i in range(len(token_seq) - n + 1):\n        uniq.add(tuple(token_seq[i:i+n]))\n        total += 1\n    return (len(uniq) / total) if total > 0 else 0.0\n\ndef repetition_rate(token_seq: List[int], n: int) -> float:\n    if len(token_seq) < n or n <= 0:\n        return 0.0\n    total = 0\n    counts = {}\n    for i in range(len(token_seq) - n + 1):\n        ng = tuple(token_seq[i:i+n])\n        counts[ng] = counts.get(ng, 0) + 1\n        total += 1\n    if total == 0:\n        return 0.0\n    repeated = sum(c for c in counts.values() if c > 1)\n    return repeated / total\n\n@torch.no_grad()\ndef kl_per_dim_stats(\n    model: VAETextLM,\n    mu: torch.Tensor,\n    logvar: torch.Tensor,\n    z: torch.Tensor,\n    eps: float,\n) -> Dict[str, float]:\n    \"\"\"\n    Bits-back style diagnostics:\n    KL_dim[d] ≈ E_batch[ (log q_d(z|x) - log p_d(z)) ] / T\n    where log q_d sums over time, and log p_d sums over time (priors factorize over dims).\n    \"\"\"\n    B, T, Dz = z.shape\n    logq_bd = model.log_q_per_dim(z, mu, logvar)               # (B,Dz)\n    logp_bd = model.prior.log_p_per_dim(z)                     # (B,Dz)\n    kld_bd = (logq_bd - logp_bd) / max(1, T)                   # per-token\n    kld_d = kld_bd.mean(dim=0)                                 # (Dz,)\n\n    kld = kld_d.detach().cpu().numpy()\n    kld_mean = float(np.mean(kld))\n    kld_med = float(np.median(kld))\n    kld_max = float(np.max(kld))\n    frac_small = float(np.mean(kld < eps))\n\n    return {\n        \"kldim_mean\": kld_mean,\n        \"kldim_median\": kld_med,\n        \"kldim_max\": kld_max,\n        \"kldim_frac_below_eps\": frac_small,\n    }\n\n@torch.no_grad()\ndef mi_proxy_batch(\n    mu: torch.Tensor,\n    logvar: torch.Tensor,\n    z: torch.Tensor,\n    max_components: int,\n    max_points: int,\n    seed: int = 0,\n) -> float:\n    \"\"\"\n    MI(x,z) proxy (batch-based):\n        I ≈ E_q[log q(z|x)] - E_q[log q(z)]\n    where q(z) approximated by mixture of batch posteriors.\n\n    Implementation notes:\n    - We use diagonal Gaussians; treat each (b,t) as a mixture component.\n    - We subsample components and points for cost control.\n    - We use z points coming from the current batch (z sample).\n    \"\"\"\n    # Flatten components: C = B*T\n    B, T, Dz = z.shape\n    C = B * T\n    if C == 0:\n        return float(\"nan\")\n\n    rng = np.random.default_rng(seed)\n\n    # Components: (C,Dz)\n    mu_c = mu.reshape(C, Dz)\n    lv_c = logvar.reshape(C, Dz)\n\n    # Points: pick from z at random (b,t)\n    z_c = z.reshape(C, Dz)\n\n    # Subsample components and points\n    comp_idx = np.arange(C)\n    pt_idx = np.arange(C)\n    if C > max_components:\n        comp_idx = rng.choice(comp_idx, size=max_components, replace=False)\n    if C > max_points:\n        pt_idx = rng.choice(pt_idx, size=max_points, replace=False)\n\n    mu_s = mu_c[torch.as_tensor(comp_idx, device=mu.device)]\n    lv_s = lv_c[torch.as_tensor(comp_idx, device=mu.device)]\n    var_s = torch.exp(lv_s).clamp_min(1e-12)\n\n    z_pts = z_c[torch.as_tensor(pt_idx, device=mu.device)]\n    mu_pts = mu_c[torch.as_tensor(pt_idx, device=mu.device)]\n    lv_pts = lv_c[torch.as_tensor(pt_idx, device=mu.device)]\n    var_pts = torch.exp(lv_pts).clamp_min(1e-12)\n\n    log2pi = math.log(2 * math.pi)\n\n    # E_q[log q(z|x)] approximated by average over chosen points using their own component params\n    # log N(z | mu, var)\n    lq_cond = -0.5 * (((z_pts - mu_pts) ** 2) / var_pts + lv_pts + log2pi).sum(dim=-1)  # (P,)\n    eq_logq_z_given_x = lq_cond.mean()\n\n    # E_q[log q(z)] with q(z) as mixture of components\n    # log q(z) = log mean_k N(z | mu_k, var_k)\n    # For each point, compute logsumexp over components.\n    # Compute (P,K,D) costs: P*max_components*Dz; keep bounded via cfg.\n    z_exp = z_pts[:, None, :]              # (P,1,D)\n    mu_exp = mu_s[None, :, :]              # (1,K,D)\n    lv_exp = lv_s[None, :, :]              # (1,K,D)\n    var_exp = var_s[None, :, :]            # (1,K,D)\n\n    lcomp = -0.5 * (((z_exp - mu_exp) ** 2) / var_exp + lv_exp + log2pi).sum(dim=-1)  # (P,K)\n    lmix = torch.logsumexp(lcomp, dim=1) - math.log(lcomp.size(1))                     # (P,)\n    eq_logq_z = lmix.mean()\n\n    mi = (eq_logq_z_given_x - eq_logq_z).item()\n    return float(mi)\n\ndef rd_points(nll_tok: float, kl_tok: float, betas: Tuple[float, ...]) -> Dict[str, float]:\n    \"\"\"\n    Rate–Distortion logging: for each beta, report derived ELBO/token (negative objective).\n    \"\"\"\n    out = {}\n    for b in betas:\n        # elbo_tok(beta) = -(nll_tok + beta*kl_tok)\n        out[f\"rd_elbo_tok_beta_{b:g}\"] = -(nll_tok + b * kl_tok)\n    # Also log the base R and D\n    out[\"rd_nll_tok\"] = float(nll_tok)\n    out[\"rd_kl_tok\"] = float(kl_tok)\n    return out\n\n\n\n# Generation metrics (aggregated + per prompt)\n\n@torch.no_grad()\ndef generation_metrics(\n    model: VAETextLM,\n    tokenizer,\n    device: str,\n    n_prompts: int,\n    max_new: int,\n    top_k: int,\n) -> Dict[str, Any]:\n    model.eval()\n    prompts = [\n        \"The meaning of life is\",\n        \"In the middle of the night\",\n        \"The government announced that\",\n        \"A new theory suggests\",\n        \"Once upon a time\",\n        \"The experiment shows\",\n        \"In a shocking discovery\",\n        \"The book describes\",\n        \"Scientists found that\",\n        \"The president said\",\n        \"In the future,\",\n        \"The story begins\",\n    ]\n\n    # perf\n    cuda_mem_reset(device)\n    sync_if_cuda(device)\n    t0 = _now()\n    gen_new_tokens_total = 0\n\n    per_prompt: List[Dict[str, Any]] = []\n    decoded_samples: List[str] = []\n\n    for i in range(n_prompts):\n        p = prompts[i % len(prompts)]\n        p_ids = tokenizer(p, return_tensors=\"pt\").input_ids.to(device)\n        if p_ids.size(1) > model.cfg.block_size // 2:\n            p_ids = p_ids[:, : model.cfg.block_size // 2]\n\n        out_ids = model.generate(p_ids, max_new_tokens=max_new, top_k=top_k)\n        seq = out_ids[0].tolist()\n\n        prompt_len = int(p_ids.size(1))\n        new_tokens = max(0, len(seq) - prompt_len)\n        gen_new_tokens_total += new_tokens\n\n        # per prompt stats (on full seq)\n        st = {\n            \"prompt\": p,\n            \"prompt_len\": prompt_len,\n            \"total_len\": len(seq),\n            \"new_tokens\": new_tokens,\n            \"rep2\": repetition_rate(seq, 2),\n            \"rep3\": repetition_rate(seq, 3),\n            \"distinct2\": distinct_ngrams(seq, 2),\n            \"distinct3\": distinct_ngrams(seq, 3),\n        }\n\n        # short sample text (truncate to be JSON-friendly)\n        txt = tokenizer.decode(seq[: min(len(seq), 250)], skip_special_tokens=True)\n        st[\"sample\"] = txt\n        per_prompt.append(st)\n\n        if i < 3:\n            decoded_samples.append(txt)\n\n    sync_if_cuda(device)\n    dt = max(1e-9, (_now() - t0))\n    mem = cuda_mem_snapshot_mb(device)\n\n    # Aggregate across prompts\n    d1 = float(np.mean([distinct_ngrams([*map(int, s[\"sample\"].encode(\"utf-8\")[:0])], 1) for s in []]) if False else 0.0)  # unused placeholder\n    rep2 = float(np.mean([p[\"rep2\"] for p in per_prompt])) if per_prompt else 0.0\n    rep3 = float(np.mean([p[\"rep3\"] for p in per_prompt])) if per_prompt else 0.0\n    distinct2 = float(np.mean([p[\"distinct2\"] for p in per_prompt])) if per_prompt else 0.0\n    distinct3 = float(np.mean([p[\"distinct3\"] for p in per_prompt])) if per_prompt else 0.0\n    lengths = [p[\"total_len\"] for p in per_prompt]\n    newlens = [p[\"new_tokens\"] for p in per_prompt]\n\n    return {\n        # aggregate\n        \"gen_distinct2\": distinct2,\n        \"gen_distinct3\": distinct3,\n        \"gen_rep2\": rep2,\n        \"gen_rep3\": rep3,\n        \"gen_len_mean\": float(np.mean(lengths)) if lengths else 0.0,\n        \"gen_len_std\": float(np.std(lengths)) if lengths else 0.0,\n        \"gen_new_tokens_mean\": float(np.mean(newlens)) if newlens else 0.0,\n        \"gen_new_tokens_total\": int(gen_new_tokens_total),\n\n        # perf\n        \"time_gen_seconds_total\": float(dt),\n        \"time_gen_tokens_per_s\": float(gen_new_tokens_total / dt),\n        \"gpu_gen_peak_alloc_MB\": mem[\"peak_alloc_MB\"],\n        \"gpu_gen_peak_reserved_MB\": mem[\"peak_reserved_MB\"],\n\n        # per prompt breakdown (what you asked: not only aggregated)\n        \"gen_per_prompt\": per_prompt,\n\n        # a few samples\n        \"gen_sample_0\": decoded_samples[0] if len(decoded_samples) > 0 else \"\",\n        \"gen_sample_1\": decoded_samples[1] if len(decoded_samples) > 1 else \"\",\n        \"gen_sample_2\": decoded_samples[2] if len(decoded_samples) > 2 else \"\",\n    }\n\n\n\n# Eval (many metrics + perf + bits-back + MI + RD)\n\n@torch.no_grad()\ndef eval_many_metrics(\n    model: VAETextLM,\n    loader: DataLoader,\n    device: str,\n    beta: float,\n    max_batches: int,\n    ece_bins: int,\n    gen_do: bool,\n    tokenizer=None,\n    gen_prompts: int = 12,\n    gen_max_new: int = 96,\n    gen_top_k: int = 50,\n    rd_betas: Tuple[float, ...] = (0.0, 1.0),\n    measure_perf: bool = True,\n    kldim_eps: float = 0.01,\n    mi_max_components: int = 512,\n    mi_max_points: int = 256,\n    mi_seed: int = 0,\n) -> Dict[str, Any]:\n    model.eval()\n\n    # basic averages\n    acc = {\n        \"nll_tok\": 0.0,\n        \"kl_tok\": 0.0,\n        \"elbo_tok\": 0.0,\n        \"tok_entropy_mean\": 0.0,\n        \"topk_mass_mean\": 0.0,\n        \"ece\": 0.0,\n        \"post_std_mean\": 0.0,\n        \"post_mu_abs_mean\": 0.0,\n        \"collapse_dim_frac\": 0.0,\n        \"mu_ac1\": 0.0,\n        \"mu_ac5\": 0.0,\n        \"z_ac1\": 0.0,\n        \"z_ac5\": 0.0,\n        # bits-back\n        \"kldim_mean\": 0.0,\n        \"kldim_median\": 0.0,\n        \"kldim_max\": 0.0,\n        \"kldim_frac_below_eps\": 0.0,\n        # MI\n        \"mi_proxy\": 0.0,\n    }\n    n = 0\n\n    # perf accumulators\n    step_ms = RunningMean()\n    tokens_per_s = RunningMean()\n    total_tokens = 0\n\n    if measure_perf:\n        cuda_mem_reset(device)\n\n    for i, (x, y) in enumerate(loader):\n        if i >= max_batches:\n            break\n        x = x.to(device)\n        y = y.to(device)\n\n        if measure_perf:\n            sync_if_cuda(device)\n            t0 = _now()\n\n        out = model(x, y, beta=beta)\n\n        if measure_perf:\n            sync_if_cuda(device)\n            dt = max(1e-9, (_now() - t0))\n            step_ms.add(1000.0 * dt)\n            B, T = x.shape\n            tok = int(B * T)\n            total_tokens += tok\n            tokens_per_s.add(tok / dt)\n\n        nll_tok = safe_float(out[\"nll_tok\"])\n        kl_tok = safe_float(out[\"kl_tok\"])\n        elbo_tok = safe_float(out[\"elbo_tok\"])\n        if not (math.isfinite(nll_tok) and math.isfinite(kl_tok) and math.isfinite(elbo_tok)):\n            continue\n\n        logits = out[\"logits_last\"]          # (B,T,V)\n        mu = out[\"mu\"]                       # (B,T,Dz)\n        logvar = out[\"logvar\"]               # (B,T,Dz)\n        zlast = out[\"z_last\"]                # (B,T,Dz)\n\n        ent_topk = logits_entropy_and_topk_mass(logits, top_k=gen_top_k)\n        ece = ece_from_logits(logits, y, n_bins=ece_bins)\n        post = posterior_collapse_ratio(mu, logvar)\n\n        mu_ac1 = latent_autocorr(mu.detach(), lag=1)\n        mu_ac5 = latent_autocorr(mu.detach(), lag=5)\n        z_ac1 = latent_autocorr(zlast.detach(), lag=1)\n        z_ac5 = latent_autocorr(zlast.detach(), lag=5)\n\n        # bits-back KL-per-dim\n        kld = kl_per_dim_stats(model, mu.detach(), logvar.detach(), zlast.detach(), eps=kldim_eps)\n\n        # MI proxy (batch-based)\n        mi = mi_proxy_batch(\n            mu.detach(), logvar.detach(), zlast.detach(),\n            max_components=mi_max_components,\n            max_points=mi_max_points,\n            seed=mi_seed,\n        )\n\n        acc[\"nll_tok\"] += nll_tok\n        acc[\"kl_tok\"] += kl_tok\n        acc[\"elbo_tok\"] += elbo_tok\n        acc[\"tok_entropy_mean\"] += ent_topk[\"tok_entropy_mean\"]\n        acc[\"topk_mass_mean\"] += ent_topk[\"topk_mass_mean\"]\n        acc[\"ece\"] += ece\n        acc[\"post_std_mean\"] += post[\"post_std_mean\"]\n        acc[\"post_mu_abs_mean\"] += post[\"post_mu_abs_mean\"]\n        acc[\"collapse_dim_frac\"] += post[\"collapse_dim_frac\"]\n        acc[\"mu_ac1\"] += mu_ac1\n        acc[\"mu_ac5\"] += mu_ac5\n        acc[\"z_ac1\"] += z_ac1\n        acc[\"z_ac5\"] += z_ac5\n\n        acc[\"kldim_mean\"] += kld[\"kldim_mean\"]\n        acc[\"kldim_median\"] += kld[\"kldim_median\"]\n        acc[\"kldim_max\"] += kld[\"kldim_max\"]\n        acc[\"kldim_frac_below_eps\"] += kld[\"kldim_frac_below_eps\"]\n\n        acc[\"mi_proxy\"] += mi\n        n += 1\n\n    if n == 0:\n        return {k: float(\"nan\") for k in acc.keys()}\n\n    for k in acc:\n        acc[k] /= n\n\n    acc[\"ppl\"] = math.exp(acc[\"nll_tok\"])\n    acc[\"bits_per_tok\"] = acc[\"nll_tok\"] / math.log(2.0)\n\n    # Rate–Distortion derived points\n    acc.update(rd_points(acc[\"nll_tok\"], acc[\"kl_tok\"], rd_betas))\n\n    # Perf snapshots\n    if measure_perf:\n        mem = cuda_mem_snapshot_mb(device)\n        acc[\"time_eval_step_ms_mean\"] = step_ms.mean()\n        acc[\"time_eval_tokens_per_s_mean\"] = tokens_per_s.mean()\n        acc[\"gpu_eval_peak_alloc_MB\"] = mem[\"peak_alloc_MB\"]\n        acc[\"gpu_eval_peak_reserved_MB\"] = mem[\"peak_reserved_MB\"]\n        acc[\"eval_total_tokens_measured\"] = int(total_tokens)\n\n    # Generation metrics (includes its own perf + GPU peaks)\n    if gen_do and tokenizer is not None:\n        gen = generation_metrics(\n            model, tokenizer, device=device,\n            n_prompts=gen_prompts, max_new=gen_max_new, top_k=gen_top_k\n        )\n        acc.update(gen)\n\n    return acc\n\n\n\n# Schedules\n\ndef lr_schedule(step, cfg: CFG):\n    if step < cfg.warmup_steps:\n        return cfg.lr * (step / max(1, cfg.warmup_steps))\n    progress = (step - cfg.warmup_steps) / max(1, cfg.max_steps - cfg.warmup_steps)\n    return cfg.lr * (0.1 + 0.9 * 0.5 * (1.0 + math.cos(math.pi * progress)))\n\ndef beta_schedule(step, cfg: CFG):\n    if step >= cfg.beta_warmup_steps:\n        return cfg.beta_end\n    a = step / max(1, cfg.beta_warmup_steps)\n    return cfg.beta_start + a * (cfg.beta_end - cfg.beta_start)\n\n\n\n# Model builder\n\ndef make_model(cfg: CFG, variant: str) -> VAETextLM:\n    if variant == \"vae\":\n        prior = PriorIID()\n    elif variant == \"vae_ar\":\n        prior = PriorAR(cfg.z_dim, rho_init=cfg.ar_init_rho, sigma=cfg.ar_sigma)\n    elif variant == \"vae_gp\":\n        prior = PriorGP(cfg.gp_lengthscale, cfg.gp_sigma, cfg.gp_jitter)\n    else:\n        raise ValueError(\"variant must be one of: vae, vae_gp, vae_ar\")\n    return VAETextLM(cfg, prior)\n\n\n\n# Train one variant (robust + perf/gpu)\n\ndef train_one_variant(cfg: CFG, variant: str, train_loader, val_loader, tokenizer) -> Dict[str, Any]:\n    device = cfg.device\n    model = make_model(cfg, variant).to(device)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n\n    use_amp = bool(cfg.amp and device.startswith(\"cuda\"))\n    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n\n    os.makedirs(cfg.out_dir, exist_ok=True)\n    run_stamp = int(time.time())\n    log_path = os.path.join(cfg.out_dir, f\"{cfg.run_name}_{variant}_{run_stamp}.jsonl\")\n\n    best_val = float(\"inf\")\n    best: Dict[str, Any] = {}\n\n    t0 = time.time()\n    pbar = tqdm(total=cfg.max_steps, desc=f\"train[{variant}]\")\n\n    it = iter(train_loader)\n\n    # windowed training perf\n    train_step_ms = RunningMean()\n    train_tokens_per_s = RunningMean()\n    train_tokens_total = 0\n    cuda_mem_reset(device)\n\n    for step in range(1, cfg.max_steps + 1):\n        try:\n            x, y = next(it)\n        except StopIteration:\n            it = iter(train_loader)\n            x, y = next(it)\n\n        x, y = x.to(device), y.to(device)\n\n        beta = beta_schedule(step, cfg)\n        lr = lr_schedule(step, cfg)\n        for pg in opt.param_groups:\n            pg[\"lr\"] = lr\n\n        model.train()\n        opt.zero_grad(set_to_none=True)\n\n        # timing per step\n        if use_amp:\n            sync_if_cuda(device)\n        t_step0 = _now()\n\n        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n            out = model(x, y, beta=beta)\n            loss = out[\"loss\"]\n\n        scaler.scale(loss).backward()\n        if cfg.grad_clip > 0:\n            scaler.unscale_(opt)\n            nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n        scaler.step(opt)\n        scaler.update()\n\n        if use_amp:\n            sync_if_cuda(device)\n        dt = max(1e-9, (_now() - t_step0))\n        train_step_ms.add(1000.0 * dt)\n        B, T = x.shape\n        tok = int(B * T)\n        train_tokens_total += tok\n        train_tokens_per_s.add(tok / dt)\n\n        if step % cfg.log_every == 0:\n            pbar.set_postfix({\n                \"loss\": f\"{loss.item():.4f}\",\n                \"nll_tok\": f\"{float(out['nll_tok']):.4f}\",\n                \"kl_tok\": f\"{float(out['kl_tok']):.4f}\",\n                \"ppl\": f\"{float(out['ppl']):.2f}\",\n                \"beta\": f\"{beta:.2f}\",\n                \"lr\": f\"{lr:.1e}\",\n            })\n\n        if step % cfg.eval_every == 0 or step == 1:\n            # snapshot memory usage for train window (since last reset)\n            mem_train = cuda_mem_snapshot_mb(device)\n            train_perf = {\n                \"time_train_step_ms_mean_window\": train_step_ms.mean(),\n                \"time_train_tokens_per_s_mean_window\": train_tokens_per_s.mean(),\n                \"gpu_train_peak_alloc_MB_window\": mem_train[\"peak_alloc_MB\"],\n                \"gpu_train_peak_reserved_MB_window\": mem_train[\"peak_reserved_MB\"],\n                \"train_tokens_measured_window\": int(train_tokens_total),\n            }\n\n            # eval train (few batches) + val (more batches)\n            train_eval = eval_many_metrics(\n                model, train_loader, device=device, beta=1.0,\n                max_batches=min(cfg.eval_train_batches, cfg.eval_max_batches),\n                ece_bins=cfg.ece_bins,\n                gen_do=False,\n                tokenizer=None,\n                gen_prompts=0,\n                gen_max_new=0,\n                gen_top_k=cfg.eval_gen_top_k,\n                rd_betas=cfg.rd_betas,\n                measure_perf=True,\n                kldim_eps=cfg.kldim_eps,\n                mi_max_components=cfg.mi_max_components,\n                mi_max_points=cfg.mi_max_points,\n                mi_seed=cfg.mi_seed,\n            )\n            val_eval = eval_many_metrics(\n                model, val_loader, device=device, beta=1.0,\n                max_batches=cfg.eval_max_batches,\n                ece_bins=cfg.ece_bins,\n                gen_do=True,\n                tokenizer=tokenizer,\n                gen_prompts=cfg.eval_gen_prompts,\n                gen_max_new=cfg.eval_gen_max_new,\n                gen_top_k=cfg.eval_gen_top_k,\n                rd_betas=cfg.rd_betas,\n                measure_perf=True,\n                kldim_eps=cfg.kldim_eps,\n                mi_max_components=cfg.mi_max_components,\n                mi_max_points=cfg.mi_max_points,\n                mi_seed=cfg.mi_seed,\n            )\n\n            # best logic on val nll\n            val_nll = val_eval.get(\"nll_tok\", float(\"inf\"))\n            if math.isfinite(val_nll) and (val_nll < best_val):\n                best_val = float(val_nll)\n                best = {\n                    \"variant\": variant,\n                    \"step\": step,\n                    \"wall_s\": time.time() - t0,\n                    \"best_val_nll_tok\": best_val,\n                    \"train_perf_window\": train_perf,\n                    \"train\": train_eval,\n                    \"val\": val_eval,\n                }\n\n            rec = {\n                \"variant\": variant,\n                \"step\": step,\n                \"wall_s\": time.time() - t0,\n                \"lr\": float(lr),\n                \"beta\": float(beta),\n                \"train_perf_window\": train_perf,\n                \"train\": train_eval,\n                \"val\": val_eval,\n                \"best_val_nll_tok_so_far\": float(best_val),\n            }\n            with open(log_path, \"a\", encoding=\"utf-8\") as f:\n                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\n            print(\n                f\"[eval] {variant} step={step:5d} \"\n                f\"train_tok/s={train_eval.get('time_eval_tokens_per_s_mean', float('nan')):.1f} \"\n                f\"val_tok/s={val_eval.get('time_eval_tokens_per_s_mean', float('nan')):.1f} \"\n                f\"gen_tok/s={val_eval.get('time_gen_tokens_per_s', float('nan')):.1f} \"\n                f\"val_ppl={val_eval.get('ppl', float('nan')):.2f} \"\n                f\"val_nll={val_eval.get('nll_tok', float('nan')):.4f} val_kl={val_eval.get('kl_tok', float('nan')):.4f} \"\n                f\"ece={val_eval.get('ece', float('nan')):.3f} ent={val_eval.get('tok_entropy_mean', float('nan')):.3f} \"\n                f\"mi={val_eval.get('mi_proxy', float('nan')):.3f} \"\n                f\"kldim_med={val_eval.get('kldim_median', float('nan')):.3f} \"\n                f\"train_peakMB={train_perf.get('gpu_train_peak_alloc_MB_window', float('nan')):.1f}\"\n            )\n\n            # reset window perf + GPU peaks\n            train_step_ms = RunningMean()\n            train_tokens_per_s = RunningMean()\n            train_tokens_total = 0\n            cuda_mem_reset(device)\n\n        pbar.update(1)\n\n    pbar.close()\n\n    if not best:\n        best = {\"variant\": variant, \"step\": cfg.max_steps, \"wall_s\": time.time() - t0, \"best_val_nll_tok\": best_val}\n    best[\"log_path\"] = log_path\n    return best\n\n\n\n# Main\n\ndef main():\n    cfg = CFG()\n    set_seed(cfg.seed)\n\n    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n    # Avoid HF warning about \"sequence length > model max length\" during tokenization chunks:\n    tokenizer.model_max_length = int(1e9)\n\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    cfg.vocab_size = len(tokenizer)\n\n    train_loader, val_loader = load_wt2_blocks(cfg, tokenizer)\n\n    results = []\n    for variant in [\"vae\", \"vae_gp\", \"vae_ar\"]:\n        best = train_one_variant(cfg, variant, train_loader, val_loader, tokenizer)\n        results.append(best)\n\n    print(\"\\n=== SUMMARY (best checkpoints) ===\")\n    results = sorted(results, key=lambda r: r.get(\"best_val_nll_tok\", float(\"inf\")))\n\n    for r in results:\n        variant = r.get(\"variant\", \"?\")\n        step = r.get(\"step\", -1)\n        best_val_nll = r.get(\"best_val_nll_tok\", float(\"nan\"))\n        wall = r.get(\"wall_s\", float(\"nan\"))\n        log_path = r.get(\"log_path\", \"\")\n\n        val = r.get(\"val\", {})\n        trp = r.get(\"train_perf_window\", {})\n\n        print(\n            f\"{variant:7s} | step={step:5d} | wall_s={wall:8.1f} | best_val_nll/tok={best_val_nll:.4f} | \"\n            f\"val_ppl={val.get('ppl', float('nan')):.2f} | val_kl/tok={val.get('kl_tok', float('nan')):.4f} | \"\n            f\"eval_tok/s={val.get('time_eval_tokens_per_s_mean', float('nan')):.1f} | \"\n            f\"gen_tok/s={val.get('time_gen_tokens_per_s', float('nan')):.1f} | \"\n            f\"train_step_ms={trp.get('time_train_step_ms_mean_window', float('nan')):.2f} | \"\n            f\"train_peakMB={trp.get('gpu_train_peak_alloc_MB_window', float('nan')):.1f} | \"\n            f\"eval_peakMB={val.get('gpu_eval_peak_alloc_MB', float('nan')):.1f} | \"\n            f\"gen_peakMB={val.get('gpu_gen_peak_alloc_MB', float('nan')):.1f} | \"\n            f\"mi={val.get('mi_proxy', float('nan')):.3f} | \"\n            f\"kldim_med={val.get('kldim_median', float('nan')):.3f} frac<eps={val.get('kldim_frac_below_eps', float('nan')):.2f} | \"\n            f\"d2={val.get('gen_distinct2', float('nan')):.3f} rep2={val.get('gen_rep2', float('nan')):.3f} | \"\n            f\"log={log_path}\"\n        )\n\n    os.makedirs(cfg.out_dir, exist_ok=True)\n    out_json = os.path.join(cfg.out_dir, f\"{cfg.run_name}_BEST_{int(time.time())}.json\")\n    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n        json.dump(\n            {\"cfg\": asdict(cfg), \"results\": results},\n            f,\n            indent=2,\n            ensure_ascii=False,\n        )\n    print(f\"\\nSaved best summary JSON: {out_json}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T14:38:45.311064Z","iopub.execute_input":"2025-12-14T14:38:45.311549Z","iopub.status.idle":"2025-12-14T15:53:52.875693Z","shell.execute_reply.started":"2025-12-14T14:38:45.311523Z","shell.execute_reply":"2025-12-14T15:53:52.874877Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a5f4403bad44f5ab391482d5e5b6268"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a2b7c30372d4191b437a60cfefec68b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36354321192d4cc4a67e6af98528a43b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dacfeb1e45c40b085eddd67986b4256"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aad133ad37e34efda066c65bfbab3a3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c60ccc7a03c04cc29a86d049921074a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b2ed4cf9b74e888e94188285c06420"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c3ba669642457f90393fc8c14cc626"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4658d3452c104bd8b0cd1dca9219b50e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1d0c1623eaf44e88bcd98381e947da0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"957ba6f2529c49329e682f18748d5fc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82c9d476bd4b4ca4b723423d38eaffe5"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (43766 > 1024). Running this sequence through the model will result in indexing errors\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\ntrain[vae]:   0%|          | 1/6000 [00:13<22:43:02, 13.63s/it]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step=    1 train_ppl=112344.95 val_ppl=111763.03 val_nll=11.6241 val_kl=66.2300 ece=0.002 ent=10.022 collapse=0.00 mu_ac1=0.005\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:   7%|▋         | 400/6000 [01:49<6:01:39,  3.87s/it, loss=6.5331, nll_tok=6.4655, kl_tok=0.2535, ppl=642.60, beta=0.27, lr=3.0e-04] ","output_type":"stream"},{"name":"stdout","text":"[eval] vae step=  400 train_ppl=678.52 val_ppl=795.32 val_nll=6.6787 val_kl=0.1531 ece=0.018 ent=6.375 collapse=0.00 mu_ac1=0.014\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  13%|█▎        | 800/6000 [03:24<5:33:43,  3.85s/it, loss=6.0963, nll_tok=6.0539, kl_tok=0.0796, ppl=425.75, beta=0.53, lr=2.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step=  800 train_ppl=352.26 val_ppl=521.88 val_nll=6.2574 val_kl=0.0551 ece=0.022 ent=5.985 collapse=0.04 mu_ac1=0.015\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  20%|██        | 1200/6000 [05:00<5:08:29,  3.86s/it, loss=5.8712, nll_tok=5.8347, kl_tok=0.0455, ppl=341.97, beta=0.80, lr=2.8e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 1200 train_ppl=217.39 val_ppl=418.75 val_nll=6.0373 val_kl=0.0385 ece=0.021 ent=5.754 collapse=0.62 mu_ac1=0.017\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  27%|██▋       | 1600/6000 [06:36<4:41:56,  3.84s/it, loss=5.6021, nll_tok=5.5583, kl_tok=0.0438, ppl=259.38, beta=1.00, lr=2.7e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 1600 train_ppl=160.63 val_ppl=369.98 val_nll=5.9135 val_kl=0.0282 ece=0.024 ent=5.606 collapse=0.74 mu_ac1=0.005\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  33%|███▎      | 2000/6000 [08:11<4:16:55,  3.85s/it, loss=5.2466, nll_tok=5.2144, kl_tok=0.0322, ppl=183.90, beta=1.00, lr=2.4e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 2000 train_ppl=128.15 val_ppl=339.01 val_nll=5.8260 val_kl=0.0189 ece=0.030 ent=5.108 collapse=0.88 mu_ac1=0.010\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  40%|████      | 2400/6000 [09:47<3:51:31,  3.86s/it, loss=4.8903, nll_tok=4.8700, kl_tok=0.0204, ppl=130.32, beta=1.00, lr=2.2e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 2400 train_ppl=105.06 val_ppl=318.71 val_nll=5.7643 val_kl=0.0165 ece=0.025 ent=5.084 collapse=0.88 mu_ac1=0.008\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  47%|████▋     | 2800/6000 [11:23<3:26:36,  3.87s/it, loss=4.9312, nll_tok=4.9159, kl_tok=0.0154, ppl=136.44, beta=1.00, lr=1.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 2800 train_ppl=87.31 val_ppl=309.29 val_nll=5.7343 val_kl=0.0116 ece=0.032 ent=4.821 collapse=0.95 mu_ac1=0.010\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  53%|█████▎    | 3200/6000 [12:59<3:00:08,  3.86s/it, loss=4.8145, nll_tok=4.8016, kl_tok=0.0128, ppl=121.71, beta=1.00, lr=1.6e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 3200 train_ppl=80.68 val_ppl=304.46 val_nll=5.7186 val_kl=0.0088 ece=0.032 ent=4.727 collapse=0.99 mu_ac1=0.012\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  60%|██████    | 3600/6000 [14:35<2:33:53,  3.85s/it, loss=4.6418, nll_tok=4.6297, kl_tok=0.0121, ppl=102.48, beta=1.00, lr=1.3e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 3600 train_ppl=64.77 val_ppl=296.85 val_nll=5.6932 val_kl=0.0074 ece=0.034 ent=4.642 collapse=0.99 mu_ac1=0.017\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  67%|██████▋   | 4000/6000 [16:11<2:08:19,  3.85s/it, loss=4.3487, nll_tok=4.3384, kl_tok=0.0104, ppl=76.58, beta=1.00, lr=1.0e-04] ","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 4000 train_ppl=60.26 val_ppl=300.13 val_nll=5.7042 val_kl=0.0059 ece=0.041 ent=4.503 collapse=1.00 mu_ac1=0.011\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  73%|███████▎  | 4400/6000 [17:47<1:43:12,  3.87s/it, loss=4.3338, nll_tok=4.3252, kl_tok=0.0086, ppl=75.58, beta=1.00, lr=7.9e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 4400 train_ppl=55.25 val_ppl=300.82 val_nll=5.7065 val_kl=0.0045 ece=0.042 ent=4.423 collapse=1.00 mu_ac1=0.016\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  80%|████████  | 4800/6000 [19:23<1:17:13,  3.86s/it, loss=4.2107, nll_tok=4.2030, kl_tok=0.0078, ppl=66.88, beta=1.00, lr=5.8e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 4800 train_ppl=50.27 val_ppl=299.69 val_nll=5.7028 val_kl=0.0031 ece=0.042 ent=4.393 collapse=1.00 mu_ac1=0.013\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  87%|████████▋ | 5200/6000 [20:59<51:33,  3.87s/it, loss=4.2219, nll_tok=4.2185, kl_tok=0.0034, ppl=67.93, beta=1.00, lr=4.3e-05]  ","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 5200 train_ppl=48.05 val_ppl=302.03 val_nll=5.7105 val_kl=0.0028 ece=0.049 ent=4.281 collapse=1.00 mu_ac1=0.016\n","output_type":"stream"},{"name":"stderr","text":"train[vae]:  93%|█████████▎| 5600/6000 [22:35<25:45,  3.86s/it, loss=4.3254, nll_tok=4.3224, kl_tok=0.0030, ppl=75.37, beta=1.00, lr=3.3e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 5600 train_ppl=47.49 val_ppl=302.83 val_nll=5.7132 val_kl=0.0026 ece=0.048 ent=4.302 collapse=1.00 mu_ac1=0.019\n","output_type":"stream"},{"name":"stderr","text":"train[vae]: 100%|██████████| 6000/6000 [24:11<00:00,  4.13it/s, loss=4.1004, nll_tok=4.0973, kl_tok=0.0031, ppl=60.18, beta=1.00, lr=3.0e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae step= 6000 train_ppl=46.26 val_ppl=304.70 val_nll=5.7193 val_kl=0.0018 ece=0.047 ent=4.292 collapse=1.00 mu_ac1=0.018\n","output_type":"stream"},{"name":"stderr","text":"\ntrain[vae_gp]:   0%|          | 1/6000 [00:12<20:54:01, 12.54s/it]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step=    1 train_ppl=109728.96 val_ppl=110570.83 val_nll=11.6134 val_kl=83677.1656 ece=0.002 ent=10.018 collapse=0.00 mu_ac1=0.001\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:   7%|▋         | 400/6000 [01:55<6:08:57,  3.95s/it, loss=12.4869, nll_tok=6.7556, kl_tok=21.4926, ppl=858.83, beta=0.27, lr=3.0e-04]   ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step=  400 train_ppl=536.56 val_ppl=702.82 val_nll=6.5551 val_kl=19.3924 ece=0.019 ent=6.429 collapse=0.00 mu_ac1=0.180\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  13%|█▎        | 800/6000 [03:38<5:44:28,  3.97s/it, loss=12.7934, nll_tok=5.9730, kl_tok=12.7881, ppl=392.70, beta=0.53, lr=2.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step=  800 train_ppl=295.82 val_ppl=490.15 val_nll=6.1947 val_kl=12.4319 ece=0.023 ent=5.970 collapse=0.00 mu_ac1=0.067\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  20%|██        | 1200/6000 [05:21<5:15:40,  3.95s/it, loss=13.9344, nll_tok=5.7330, kl_tok=10.2518, ppl=308.88, beta=0.80, lr=2.8e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 1200 train_ppl=209.67 val_ppl=408.05 val_nll=6.0114 val_kl=9.9512 ece=0.022 ent=5.703 collapse=0.00 mu_ac1=0.039\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  27%|██▋       | 1600/6000 [07:04<4:50:01,  3.95s/it, loss=14.0886, nll_tok=5.4634, kl_tok=8.6252, ppl=235.91, beta=1.00, lr=2.7e-04] ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 1600 train_ppl=154.29 val_ppl=363.03 val_nll=5.8945 val_kl=8.1948 ece=0.024 ent=5.379 collapse=0.00 mu_ac1=0.041\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  33%|███▎      | 2000/6000 [08:48<4:25:35,  3.98s/it, loss=12.9881, nll_tok=5.3696, kl_tok=7.6186, ppl=214.77, beta=1.00, lr=2.4e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 2000 train_ppl=125.62 val_ppl=342.02 val_nll=5.8349 val_kl=7.0848 ece=0.026 ent=5.115 collapse=0.00 mu_ac1=0.058\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  40%|████      | 2400/6000 [10:31<3:57:19,  3.96s/it, loss=11.7545, nll_tok=5.0593, kl_tok=6.6952, ppl=157.48, beta=1.00, lr=2.2e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 2400 train_ppl=99.62 val_ppl=322.43 val_nll=5.7759 val_kl=6.9172 ece=0.034 ent=4.911 collapse=0.00 mu_ac1=0.069\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  47%|████▋     | 2800/6000 [12:14<3:32:25,  3.98s/it, loss=11.0530, nll_tok=5.0094, kl_tok=6.0436, ppl=149.81, beta=1.00, lr=1.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 2800 train_ppl=72.43 val_ppl=312.41 val_nll=5.7443 val_kl=6.3137 ece=0.034 ent=4.804 collapse=0.00 mu_ac1=0.087\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  53%|█████▎    | 3200/6000 [13:58<3:06:11,  3.99s/it, loss=10.2458, nll_tok=4.6757, kl_tok=5.5702, ppl=107.30, beta=1.00, lr=1.6e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 3200 train_ppl=64.02 val_ppl=311.46 val_nll=5.7413 val_kl=5.3648 ece=0.044 ent=4.579 collapse=0.00 mu_ac1=0.089\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  60%|██████    | 3600/6000 [15:42<2:39:49,  4.00s/it, loss=9.5473, nll_tok=4.3592, kl_tok=5.1881, ppl=78.20, beta=1.00, lr=1.3e-04]  ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 3600 train_ppl=54.71 val_ppl=309.23 val_nll=5.7341 val_kl=5.0114 ece=0.047 ent=4.439 collapse=0.00 mu_ac1=0.086\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  67%|██████▋   | 4000/6000 [17:26<2:12:56,  3.99s/it, loss=9.2051, nll_tok=4.3007, kl_tok=4.9044, ppl=73.75, beta=1.00, lr=1.0e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 4000 train_ppl=51.64 val_ppl=309.32 val_nll=5.7344 val_kl=4.8131 ece=0.046 ent=4.382 collapse=0.00 mu_ac1=0.101\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  73%|███████▎  | 4400/6000 [19:09<1:46:22,  3.99s/it, loss=8.8193, nll_tok=4.0793, kl_tok=4.7400, ppl=59.11, beta=1.00, lr=7.9e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 4400 train_ppl=43.38 val_ppl=310.20 val_nll=5.7372 val_kl=4.8346 ece=0.051 ent=4.293 collapse=0.00 mu_ac1=0.099\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  80%|████████  | 4800/6000 [20:53<1:19:42,  3.99s/it, loss=8.5039, nll_tok=3.9182, kl_tok=4.5856, ppl=50.31, beta=1.00, lr=5.8e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 4800 train_ppl=43.38 val_ppl=314.46 val_nll=5.7508 val_kl=4.6392 ece=0.052 ent=4.244 collapse=0.00 mu_ac1=0.114\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  87%|████████▋ | 5200/6000 [22:37<53:06,  3.98s/it, loss=8.5154, nll_tok=4.0564, kl_tok=4.4590, ppl=57.76, beta=1.00, lr=4.3e-05]  ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 5200 train_ppl=38.31 val_ppl=317.71 val_nll=5.7611 val_kl=4.3875 ece=0.057 ent=4.176 collapse=0.00 mu_ac1=0.121\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]:  93%|█████████▎| 5600/6000 [24:21<26:35,  3.99s/it, loss=8.4395, nll_tok=4.0517, kl_tok=4.3878, ppl=57.50, beta=1.00, lr=3.3e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 5600 train_ppl=38.47 val_ppl=319.32 val_nll=5.7662 val_kl=4.2987 ece=0.056 ent=4.139 collapse=0.00 mu_ac1=0.115\n","output_type":"stream"},{"name":"stderr","text":"train[vae_gp]: 100%|██████████| 6000/6000 [26:05<00:00,  3.83it/s, loss=8.3038, nll_tok=3.9715, kl_tok=4.3323, ppl=53.07, beta=1.00, lr=3.0e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_gp step= 6000 train_ppl=38.35 val_ppl=321.93 val_nll=5.7743 val_kl=4.2492 ece=0.056 ent=4.146 collapse=0.00 mu_ac1=0.109\n","output_type":"stream"},{"name":"stderr","text":"\ntrain[vae_ar]:   0%|          | 1/6000 [00:12<20:38:21, 12.39s/it]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step=    1 train_ppl=116526.99 val_ppl=116056.98 val_nll=11.6618 val_kl=609.7574 ece=0.002 ent=10.034 collapse=0.00 mu_ac1=-0.008\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:   7%|▋         | 400/6000 [01:48<6:04:30,  3.91s/it, loss=12.3916, nll_tok=6.6738, kl_tok=21.4420, ppl=791.36, beta=0.27, lr=3.0e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step=  400 train_ppl=691.37 val_ppl=806.56 val_nll=6.6928 val_kl=20.8259 ece=0.022 ent=6.187 collapse=0.00 mu_ac1=0.119\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  13%|█▎        | 800/6000 [03:25<5:39:33,  3.92s/it, loss=17.1375, nll_tok=6.2190, kl_tok=20.4722, ppl=502.18, beta=0.53, lr=2.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step=  800 train_ppl=381.44 val_ppl=565.15 val_nll=6.3371 val_kl=20.2787 ece=0.022 ent=5.853 collapse=0.00 mu_ac1=0.102\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  20%|██        | 1200/6000 [05:02<5:13:53,  3.92s/it, loss=21.7502, nll_tok=5.9460, kl_tok=19.7552, ppl=382.23, beta=0.80, lr=2.8e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 1200 train_ppl=268.60 val_ppl=450.04 val_nll=6.1093 val_kl=19.8438 ece=0.019 ent=5.804 collapse=0.00 mu_ac1=0.084\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  27%|██▋       | 1600/6000 [06:38<4:46:26,  3.91s/it, loss=24.9564, nll_tok=5.4433, kl_tok=19.5130, ppl=231.21, beta=1.00, lr=2.7e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 1600 train_ppl=194.08 val_ppl=390.58 val_nll=5.9676 val_kl=19.3929 ece=0.021 ent=5.486 collapse=0.00 mu_ac1=0.078\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  33%|███▎      | 2000/6000 [08:15<4:19:14,  3.89s/it, loss=24.3278, nll_tok=5.2522, kl_tok=19.0755, ppl=190.99, beta=1.00, lr=2.4e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 2000 train_ppl=155.48 val_ppl=361.36 val_nll=5.8899 val_kl=18.9786 ece=0.025 ent=5.359 collapse=0.00 mu_ac1=0.057\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  40%|████      | 2400/6000 [09:51<3:55:18,  3.92s/it, loss=23.7695, nll_tok=5.2087, kl_tok=18.5608, ppl=182.85, beta=1.00, lr=2.2e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 2400 train_ppl=121.36 val_ppl=337.88 val_nll=5.8227 val_kl=18.6014 ece=0.035 ent=5.054 collapse=0.00 mu_ac1=0.064\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  47%|████▋     | 2800/6000 [11:28<3:28:05,  3.90s/it, loss=23.2906, nll_tok=5.0194, kl_tok=18.2712, ppl=151.32, beta=1.00, lr=1.9e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 2800 train_ppl=101.41 val_ppl=316.83 val_nll=5.7584 val_kl=18.2395 ece=0.033 ent=4.908 collapse=0.00 mu_ac1=0.068\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  53%|█████▎    | 3200/6000 [13:05<3:02:17,  3.91s/it, loss=22.6476, nll_tok=4.7702, kl_tok=17.8775, ppl=117.94, beta=1.00, lr=1.6e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 3200 train_ppl=89.43 val_ppl=312.33 val_nll=5.7441 val_kl=17.9213 ece=0.035 ent=4.784 collapse=0.00 mu_ac1=0.068\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  60%|██████    | 3600/6000 [14:41<2:36:51,  3.92s/it, loss=22.3376, nll_tok=4.5387, kl_tok=17.7988, ppl=93.57, beta=1.00, lr=1.3e-04] ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 3600 train_ppl=79.16 val_ppl=302.14 val_nll=5.7109 val_kl=17.6603 ece=0.033 ent=4.726 collapse=0.00 mu_ac1=0.071\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  67%|██████▋   | 4000/6000 [16:18<2:10:09,  3.90s/it, loss=22.0842, nll_tok=4.6136, kl_tok=17.4706, ppl=100.85, beta=1.00, lr=1.0e-04]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 4000 train_ppl=68.63 val_ppl=301.11 val_nll=5.7075 val_kl=17.4411 ece=0.041 ent=4.569 collapse=0.00 mu_ac1=0.083\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  73%|███████▎  | 4400/6000 [17:54<1:43:57,  3.90s/it, loss=21.8404, nll_tok=4.6090, kl_tok=17.2314, ppl=100.39, beta=1.00, lr=7.9e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 4400 train_ppl=61.35 val_ppl=301.58 val_nll=5.7090 val_kl=17.2873 ece=0.040 ent=4.566 collapse=0.00 mu_ac1=0.078\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  80%|████████  | 4800/6000 [19:31<1:17:48,  3.89s/it, loss=21.3793, nll_tok=4.2449, kl_tok=17.1343, ppl=69.75, beta=1.00, lr=5.8e-05] ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 4800 train_ppl=58.81 val_ppl=301.09 val_nll=5.7074 val_kl=17.1505 ece=0.040 ent=4.508 collapse=0.00 mu_ac1=0.081\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  87%|████████▋ | 5200/6000 [21:08<52:03,  3.90s/it, loss=21.2509, nll_tok=4.1653, kl_tok=17.0856, ppl=64.41, beta=1.00, lr=4.3e-05]  ","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 5200 train_ppl=57.03 val_ppl=301.99 val_nll=5.7104 val_kl=17.0499 ece=0.045 ent=4.412 collapse=0.00 mu_ac1=0.082\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]:  93%|█████████▎| 5600/6000 [22:44<26:08,  3.92s/it, loss=21.0963, nll_tok=4.1598, kl_tok=16.9365, ppl=64.06, beta=1.00, lr=3.3e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 5600 train_ppl=53.44 val_ppl=302.30 val_nll=5.7114 val_kl=16.9846 ece=0.044 ent=4.384 collapse=0.00 mu_ac1=0.082\n","output_type":"stream"},{"name":"stderr","text":"train[vae_ar]: 100%|██████████| 6000/6000 [24:21<00:00,  4.11it/s, loss=21.0421, nll_tok=4.2128, kl_tok=16.8293, ppl=67.54, beta=1.00, lr=3.0e-05]","output_type":"stream"},{"name":"stdout","text":"[eval] vae_ar step= 6000 train_ppl=51.36 val_ppl=303.85 val_nll=5.7165 val_kl=16.9258 ece=0.045 ent=4.370 collapse=0.00 mu_ac1=0.075\n\n=== SUMMARY (best checkpoints) ===\nvae     | step= 3600 | best_val_nll/tok=5.6932 | val_ppl=296.85 | val_kl/tok=0.0074 | ECE=0.034 | ent=4.642 | collapse=0.99 | mu_ac1=0.017 | d2=0.796 rep2=0.293 | log=runs/wt2_gpvae_compare_vae_1765723152.jsonl\nvae_ar  | step= 4800 | best_val_nll/tok=5.7074 | val_ppl=301.09 | val_kl/tok=17.1505 | ECE=0.040 | ent=4.508 | collapse=0.00 | mu_ac1=0.081 | d2=0.803 rep2=0.289 | log=runs/wt2_gpvae_compare_vae_ar_1765726171.jsonl\nvae_gp  | step= 3600 | best_val_nll/tok=5.7341 | val_ppl=309.23 | val_kl/tok=5.0114 | ECE=0.047 | ent=4.439 | collapse=0.00 | mu_ac1=0.086 | d2=0.763 rep2=0.331 | log=runs/wt2_gpvae_compare_vae_gp_1765724605.jsonl\n\nSaved best summary JSON: runs/wt2_gpvae_compare_BEST_1765727632.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}