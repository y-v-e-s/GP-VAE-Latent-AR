{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1408148,"sourceType":"datasetVersion","datasetId":823358}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Latent-Autoregressive GP-VAE Language Model with a Dilated TCN Encoder\n\nThis encoder is a **dilated causal TCN**, not a “pyramidal” network.\n\n- It stacks **causal 1D convolutions with increasing dilations** (`base_dilations = 1,2,4,8,16`) and **residual TCN blocks**.\n- The **time resolution stays constant** (no stride, pooling, or downsampling/upsampling).\n\nIn many papers, “pyramidal” usually implies **progressive temporal downsampling** (e.g., T → T/2 → T/4). Since that is not present here, the accurate name is:\n**Hierarchical / Dilated Causal TCN encoder** (or **TCN+** for the enhanced block design).\n\n\n## Purpose of the code\n\nThis codebase was created as a **controlled proof-of-concept** to investigate whether\n**sequential structure in a language model can be shifted from token-level autoregression\nto a continuous latent space**.\n\nIt implements a **GP-VAE language model** in which:\n- temporal dependencies are enforced by a **causal Gaussian Process prior** over latent variables,\n- the encoder maps token sequences to latent distributions using a **dilated causal TCN**,\n- the decoder generates tokens **in parallel**, without token-level autoregression.\n\n---\n\n## What is being tested\n\nThe code is designed to perform a **systematic ablation of latent autoregression**:\n\n- **Latent AR mode**: latent variables are sampled sequentially using GP conditionals,\n  enforcing temporal coherence.\n- **Latent non-AR mode**: temporal correlations are removed by sampling latent variables\n  independently from diagonal marginals.\n\nThis setup isolates the effect of **autoregressive structure in latent space**, independently\nof token-level autoregression.\n\n---\n\n## Why this is useful\n\nThis framework allows us to:\n- verify that the GP-VAE model can be **trained stably** on real language data (WikiText-103),\n- analyze how latent autoregression affects **long-horizon generation stability**,\n- compare against a **standard autoregressive Transformer baseline**,\n- and highlight the limitations of **classical AR metrics** when applied to\n  parallel latent generative models.\n\nOverall, the code serves as an **experimental testbed** for assessing whether part of a\nlanguage model’s temporal structure can be supported by the **probabilistic geometry of\nlatent space**, rather than by explicit token-level autoregressive neural operations.\n","metadata":{}},{"cell_type":"code","source":"# Latent-Autoregressive GP-VAE Language Model with a Dilated TCN Encoder\n\nimport math, os, time, json, shutil, random, pathlib\nfrom dataclasses import dataclass, asdict\nfrom typing import Tuple, Optional, Dict, Any, List\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\nfrom torch.nn.utils.parametrizations import weight_norm as pn_weight_norm\n\ntry:\n    import optuna\n    from optuna.pruners import MedianPruner\n    _HAVE_OPTUNA = True\nexcept Exception:\n    _HAVE_OPTUNA = False\n\ntry:\n    from datasets import load_dataset\n    from transformers import AutoTokenizer, AutoModelForCausalLM\nexcept Exception:\n    load_dataset = None\n    AutoTokenizer = None\n    AutoModelForCausalLM = None\n    print(\"[Warning] Please install datasets and transformers: pip install datasets transformers\")\n\n# Dataset utilities\n\nclass LMBlocks(Dataset):\n    def __init__(self, texts, tokenizer, block_size: int = 256):\n        ids = []\n        for t in texts:\n            ids.extend(tokenizer.encode(t))\n        n = max(1, len(ids) // block_size)\n        ids = ids[: n * block_size]\n        self.block_size = block_size\n        self.data = torch.tensor(ids, dtype=torch.long).view(n, block_size)\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef load_wikitext103(block_size=256, split='train'):\n    \"\"\"\n      - train  -> /kaggle/input/wikitext/wikitext-103/wiki.train.tokens\n      - valid  -> /kaggle/input/wikitext/wikitext-103/wiki.valid.tokens\n      - test   -> /kaggle/input/wikitext/wikitext-103/wiki.test.tokens\n    \"\"\"\n    path_map = {\n        'train': \"/kaggle/input/wikitext/wikitext-103/wiki.train.tokens\",\n        'validation': \"/kaggle/input/wikitext/wikitext-103/wiki.valid.tokens\",\n        'valid': \"/kaggle/input/wikitext/wikitext-103/wiki.valid.tokens\",\n        'test': \"/kaggle/input/wikitext/wikitext-103/wiki.test.tokens\",\n    }\n    if split not in path_map:\n        raise ValueError(f\"split inconnu pour WikiText-103 : {split}\")\n\n    path = path_map[split]\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Fichier WikiText-103 introuvable : {path}\")\n\n    texts = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.rstrip(\"\\n\")\n            if line.strip() == \"\":\n                continue\n            texts.append(line)\n\n    # Hack pour éviter les warnings HF\n    try:\n        from transformers.utils import hub as _hf_hub_utils\n        _hf_hub_utils.list_repo_templates = lambda *a, **k: []\n        import transformers.tokenization_utils_base as _tub\n        _tub.list_repo_templates = lambda *a, **k: []\n    except Exception:\n        pass\n\n    assert AutoTokenizer is not None, \"Please install transformers\"\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    tokenizer.pad_token = tokenizer.eos_token\n    # Évite les warnings \"sequence length > 1024\" pendant l'encodage du corpus\n    tokenizer.model_max_length = int(1e9)\n    if hasattr(tokenizer, \"init_kwargs\"):\n        tokenizer.init_kwargs[\"model_max_length\"] = int(1e9)\n\n    dataset = LMBlocks(texts, tokenizer, block_size)\n    return dataset, tokenizer\n\n\n# Plotting helpers\n\ndef plot_curve(values, title, ylabel, out_png):\n    plt.figure()\n    plt.plot(values)\n    plt.xlabel('Step')\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.tight_layout()\n    plt.savefig(out_png, dpi=160)\n    plt.close()\n\n\ndef reliability_diagram_data(logits: torch.Tensor, x: torch.Tensor, n_bins: int = 15):\n    with torch.no_grad():\n        probs = torch.softmax(logits, dim=-1)\n        conf, pred = probs.max(dim=-1)\n        corr = (pred == x).float()\n        conf = conf.detach().cpu().view(-1).numpy()\n        corr = corr.detach().cpu().view(-1).numpy()\n    bins = np.linspace(0.0, 1.0, n_bins + 1)\n    bin_ids = np.digitize(conf, bins) - 1\n    bin_conf, bin_acc, bin_count = [], [], []\n    ece = 0.0\n    N = len(conf)\n    for b in range(n_bins):\n        m = (bin_ids == b)\n        cnt = m.sum()\n        if cnt == 0:\n            bin_conf.append(0.0)\n            bin_acc.append(0.0)\n            bin_count.append(0)\n            continue\n        c = conf[m].mean()\n        a = corr[m].mean()\n        w = cnt / N\n        ece += w * abs(a - c)\n        bin_conf.append(c)\n        bin_acc.append(a)\n        bin_count.append(cnt)\n    return np.array(bin_conf), np.array(bin_acc), float(ece)\n\n\ndef plot_reliability(confidences, accuracy, ece, out_png):\n    plt.figure()\n    plt.plot(confidences, accuracy, 'o-')\n    plt.plot([0, 1], [0, 1], '--')\n    plt.xlabel('Confidence')\n    plt.ylabel('Accuracy')\n    plt.title(f'Reliability diagram — ECE={ece:.4f}')\n    plt.tight_layout()\n    plt.savefig(out_png, dpi=160)\n    plt.close()\n\n\ndef nll_per_token(logits: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n    B, T, V = logits.shape\n    ce = F.cross_entropy(logits.reshape(B * T, V), x.reshape(B * T), reduction='none').view(B, T)\n    return ce.mean(dim=0).detach().cpu()\n\n\ndef plot_nll_per_position(nll_per_pos, out_png):\n    plt.figure()\n    plt.plot(nll_per_pos)\n    plt.xlabel('Position t')\n    plt.ylabel('Token NLL')\n    plt.title('NLL per position')\n    plt.tight_layout()\n    plt.savefig(out_png, dpi=160)\n    plt.close()\n\n\n# GP helpers\n\ndef rbf_kernel(t: torch.Tensor, lengthscale: torch.Tensor, variance: torch.Tensor) -> torch.Tensor:\n    t = t.view(-1, 1)\n    d2 = (t - t.T) ** 2\n    return variance * torch.exp(-0.5 * d2 / (lengthscale ** 2))\n\n\ndef mvn_logpdf_zero_mean(z: torch.Tensor, K: torch.Tensor, jitter: float = 1e-5) -> torch.Tensor:\n    B, T, D = z.shape\n    I = torch.eye(T, device=K.device, dtype=K.dtype)\n    L = torch.linalg.cholesky(K + jitter * I)\n    log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n    z_bd = z.permute(0, 2, 1).reshape(B * D, T)\n    y_bd = torch.cholesky_solve(z_bd.unsqueeze(-1), L).squeeze(-1)\n    quad = (z_bd * y_bd).sum(dim=-1).view(B, D).sum(dim=-1)\n    const = T * math.log(2 * math.pi)\n    logp = -0.5 * (D * (log_det + const) + quad)\n    return logp\n\n\ndef diag_mvn_logpdf(z: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n    var = torch.exp(logvar)\n    const = math.log(2 * math.pi)\n    logp = -0.5 * (const + logvar + (z - mu) ** 2 / var)\n    return logp.sum(dim=(1, 2))\n\n\n# Building blocks (positional enc, TCN, etc.)\n\nclass SinusoidalPos(nn.Module):\n    def __init__(self, d: int):\n        super().__init__()\n        self.scale = nn.Parameter(torch.ones(1))\n        self.d = d\n\n    def forward(self, T: int, device):\n        D = self.d\n        pe = torch.zeros(T, D, device=device)\n        position = torch.arange(T, device=device).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, D, 2, device=device) * (-math.log(10000.0) / D))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return (self.scale * pe).unsqueeze(0)\n\n\nclass CausalDepthwiseSeparableConv(nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=1, out_channels=None):\n        super().__init__()\n        pad = (kernel_size - 1) * dilation\n        self.left_pad = pad\n        out_channels = out_channels if out_channels is not None else channels\n        dw = nn.Conv1d(channels, channels, kernel_size, groups=channels,\n                       dilation=dilation, padding=pad, bias=True)\n        pw = nn.Conv1d(channels, out_channels, kernel_size=1, bias=True)\n        self.dw = pn_weight_norm(dw)\n        self.pw = pn_weight_norm(pw)\n\n        self._pad = pad\n\n    def forward(self, x):\n        h = self.dw(x)\n        if self.left_pad > 0:\n            h = h[..., :-self.left_pad]\n        h = self.pw(h)\n        return h\n\n\nclass SqueezeExcite(nn.Module):\n    def __init__(self, channels, reduction=8):\n        super().__init__()\n        hidden = max(1, channels // reduction)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Conv1d(channels, hidden, kernel_size=1)\n        self.fc2 = nn.Conv1d(hidden, channels, kernel_size=1)\n\n    def forward(self, x):\n        s = self.pool(x)\n        s = F.gelu(self.fc1(s))\n        s = torch.sigmoid(self.fc2(s))\n        return x * s\n\n\nclass DropPath1D(nn.Module):\n    def __init__(self, drop_prob=0.0):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        if not self.training or self.drop_prob <= 0.0:\n            return x\n        keep = 1.0 - self.drop_prob\n        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n        mask = torch.empty(shape, dtype=x.dtype, device=x.device).bernoulli_(keep) / keep\n        return x * mask\n\n\nclass TCNBlock(nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=1, dropout=0.05, droppath=0.05):\n        super().__init__()\n        self.gn1 = nn.GroupNorm(1, channels)\n        self.conv1 = CausalDepthwiseSeparableConv(channels, kernel_size, dilation, out_channels=2 * channels)\n        self.gn2 = nn.GroupNorm(1, channels)\n        self.conv2 = CausalDepthwiseSeparableConv(channels, kernel_size, dilation, out_channels=2 * channels)\n        self.se = SqueezeExcite(channels, reduction=8)\n        self.drop = nn.Dropout(dropout)\n        self.droppath = DropPath1D(drop_prob=droppath)\n\n    def forward(self, x):\n        h = self.gn1(x)\n        h = self.conv1(h)\n        h = F.glu(h, dim=1)\n        h = self.drop(h)\n        h = self.gn2(h)\n        h = self.conv2(h)\n        h = F.glu(h, dim=1)\n        h = self.se(h)\n        h = self.drop(h)\n        return x + self.droppath(h)\n\n\nclass HierarchicalTCN(nn.Module):\n    def __init__(self, in_ch, hidden_ch, n_stacks=3, blocks_per_stack=2,\n                 kernel_size=3, base_dilations=(1, 2, 4, 8, 16), dropout=0.05, droppath=0.05):\n        super().__init__()\n        self.proj = nn.Conv1d(in_ch, hidden_ch, 1)\n        self.blocks = nn.ModuleList()\n        dil = list(base_dilations)\n        idx = 0\n        for _ in range(n_stacks * blocks_per_stack):\n            d = dil[idx % len(dil)]\n            self.blocks.append(TCNBlock(hidden_ch, kernel_size, d, dropout, droppath))\n            idx += 1\n\n    def forward(self, x):\n        h = self.proj(x)\n        for blk in self.blocks:\n            h = blk(h)\n        return h\n\n\n# GP-VAE-TCN+ model\n\n@dataclass\nclass Config:\n    vocab_size: int\n    d_model: int = 256\n    d_latent: int = 64\n    block_size: int = 256\n    emb_dim: int = 256\n    gp_lengthscale_init: float = 8.0\n    gp_variance_init: float = 1.0\n    lr: float = 2e-4\n    label_smoothing: float = 0.03\n    weight_decay: float = 0.01\n    grad_clip: float = 1.0\n    free_bits_nats: float = 0.5\n    kl_cap_nats: float = 12.0\n    kl_target_nats: float = 8.0\n    beta_init: float = 1e-3\n    beta_max: float = 0.5\n    beta_adapt_rate: float = 0.05\n    use_adaptive_beta: bool = True\n    K_multi: int = 3\n    multi_lambda_scheme: str = \"harmonic\"\n    gamma_multi: float = 0.5\n    logvar_min: float = -6.0\n    logvar_max: float = 2.0\n    logvar_init: float = -4.0\n    tcn_stacks: int = 3\n    tcn_blocks_per_stack: int = 3\n    tcn_kernel: int = 3\n    tcn_base_dilations: tuple = (1, 2, 4, 8, 16)\n    tcn_dropout: float = 0.05\n    tcn_droppath: float = 0.05\n    embed_reg_weight: float = 0.2\n    embed_reg_mode: str = \"cos\"\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nclass TCNEncoder(nn.Module):\n    def __init__(self, cfg: Config):\n        super().__init__()\n        self.cfg = cfg\n        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim)\n        self.tcn = HierarchicalTCN(\n            in_ch=cfg.emb_dim, hidden_ch=cfg.d_model,\n            n_stacks=cfg.tcn_stacks, blocks_per_stack=cfg.tcn_blocks_per_stack,\n            kernel_size=cfg.tcn_kernel, base_dilations=cfg.tcn_base_dilations,\n            dropout=cfg.tcn_dropout, droppath=cfg.tcn_droppath)\n        self.to_mu = nn.Conv1d(cfg.d_model, cfg.d_latent, kernel_size=1)\n        self.to_logvar = nn.Conv1d(cfg.d_model, cfg.d_latent, kernel_size=1)\n        nn.init.zeros_(self.to_mu.weight)\n        nn.init.zeros_(self.to_mu.bias)\n        nn.init.zeros_(self.to_logvar.weight)\n        nn.init.constant_(self.to_logvar.bias, cfg.logvar_init)\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        h = self.tok_emb(x)\n        h = h.transpose(1, 2)\n        h = self.tcn(h)\n        mu = self.to_mu(h).transpose(1, 2)\n        logvar = self.to_logvar(h).transpose(1, 2)\n        logvar = logvar.clamp(self.cfg.logvar_min, self.cfg.logvar_max)\n        return mu, logvar\n\n\nclass TokenDecoder(nn.Module):\n    def __init__(self, cfg: Config, tied_weight: torch.Tensor):\n        super().__init__()\n        self.cfg = cfg\n        self.pe = SinusoidalPos(cfg.d_latent)\n        self.mlp = nn.Sequential(\n            nn.Linear(cfg.d_latent, cfg.d_model), nn.GELU(),\n            nn.Linear(cfg.d_model, cfg.d_model), nn.GELU())\n        self.ln = nn.LayerNorm(cfg.d_model)\n        self.post = nn.Conv1d(cfg.d_model, cfg.d_model, kernel_size=3, padding=1)\n        self.to_emb = nn.Linear(cfg.d_model, cfg.emb_dim, bias=False)\n        self.tied_weight = tied_weight\n        self.bias = nn.Parameter(torch.zeros(cfg.vocab_size))\n        self.sem_head = nn.Linear(cfg.d_model, cfg.emb_dim, bias=False)\n\n    def forward(self, z: torch.Tensor):\n        z = z + self.pe(T=z.size(1), device=z.device)\n        h = self.mlp(z)\n        h = self.ln(h)\n        h2 = self.post(h.transpose(1, 2)).transpose(1, 2)\n        h = h + h2\n        e_proj = self.to_emb(h)\n        tw = F.normalize(self.tied_weight, dim=-1)\n        logits = torch.matmul(e_proj, tw.t()) + self.bias\n        e_hat = self.sem_head(h)\n        return logits, e_hat\n\n\nclass GPVAE_TCN(nn.Module):\n    def __init__(self, cfg: Config):\n        super().__init__()\n        self.cfg = cfg\n        self.encoder = TCNEncoder(cfg)\n        self.decoder = TokenDecoder(cfg, tied_weight=self.encoder.tok_emb.weight)\n        self.register_buffer('t_train', torch.arange(cfg.block_size, dtype=torch.float32))\n        self._ls_unc = nn.Parameter(torch.tensor(cfg.gp_lengthscale_init).log())\n        self._var_unc = nn.Parameter(torch.tensor(cfg.gp_variance_init).log())\n        self.softplus = nn.Softplus(beta=1.0)\n\n    def gp_hypers(self):\n        ls = self.softplus(self._ls_unc) + 1e-6\n        var = self.softplus(self._var_unc) + 1e-6\n        return ls, var\n\n    def K_tt(self, t: torch.Tensor) -> torch.Tensor:\n        ls, var = self.gp_hypers()\n        return rbf_kernel(t, ls, var)\n\n    def _label_smoothing_ce(self, logits, targets, eps):\n        log_probs = F.log_softmax(logits, dim=-1)\n        nll = F.nll_loss(log_probs.transpose(1, 2), targets, reduction='mean')\n        smooth = -log_probs.mean(dim=-1).mean()\n        return (1 - eps) * nll + eps * smooth\n\n    def _multi_horizon_ll(self, logits, x) -> torch.Tensor:\n        K = self.cfg.K_multi\n        if K <= 0:\n            return torch.zeros((), device=logits.device)\n        B, T, V = logits.shape\n        total = 0.0\n        for k in range(1, K + 1):\n            if T - k <= 0:\n                break\n            lam = (1.0 / k) if self.cfg.multi_lambda_scheme == \"harmonic\" else 1.0\n            ll_k = -self._label_smoothing_ce(logits[:, :T - k, :], x[:, k:], self.cfg.label_smoothing)\n            total = total + self.cfg.gamma_multi * lam * ll_k\n        return total\n\n    def _embed_reg_losses(self, e_hat, x, token_emb_table: torch.Tensor):\n        with torch.no_grad():\n            e_tgt = token_emb_table[x]\n        loss = 0.0\n        if self.cfg.embed_reg_mode in (\"mse\", \"cos+mse\"):\n            loss = loss + F.mse_loss(e_hat, e_tgt, reduction=\"mean\")\n        if self.cfg.embed_reg_mode in (\"cos\", \"cos+mse\"):\n            loss = loss + (1.0 - F.cosine_similarity(e_hat, e_tgt, dim=-1).mean())\n        return torch.as_tensor(loss, device=e_hat.device)\n\n    def elbo(self, x: torch.Tensor, beta_override: Optional[float] = None):\n        cfg = self.cfg\n        B, T = x.shape\n        mu, logvar = self.encoder(x)\n        std = torch.exp(0.5 * logvar)\n        z = mu + std * torch.randn_like(std)\n        logits, e_hat = self.decoder(z)\n        ll_0 = -self._label_smoothing_ce(logits, x, cfg.label_smoothing)\n        ll_multi = self._multi_horizon_ll(logits, x)\n        emb_reg = self._embed_reg_losses(e_hat, x, self.encoder.tok_emb.weight.detach())\n        tvec = torch.arange(T, dtype=torch.float32, device=x.device)\n        K_full = self.K_tt(tvec)\n        log_pz_b = mvn_logpdf_zero_mean(z, K_full)\n        log_qz_b = diag_mvn_logpdf(z, mu, logvar)\n        kl_tok_raw_t = (log_qz_b - log_pz_b).mean() / T\n        kl_tok_capped_t = torch.clamp(torch.clamp(kl_tok_raw_t, min=cfg.free_bits_nats), max=cfg.kl_cap_nats)\n        beta = beta_override if beta_override is not None else getattr(self, \"_beta_state\", cfg.beta_init)\n        elbo_tok_t = (ll_0 + ll_multi) - beta * kl_tok_capped_t - cfg.embed_reg_weight * emb_reg\n        stats = {\n            'll0_tok': float(ll_0.detach().item()),\n            'll_multi_tok': float(ll_multi.detach().item()),\n            'emb_reg': float(emb_reg.detach().item()) if torch.is_tensor(emb_reg) else float(emb_reg),\n            'kl_tok_raw': float(kl_tok_raw_t.detach().item()),\n            'kl_tok_cap': float(kl_tok_capped_t.detach().item()),\n            'beta': float(beta),\n            'elbo_tok': float(elbo_tok_t.detach().item()),\n        }\n        comps = {\n            'll0_tok_t': ll_0,\n            'll_multi_tok_t': ll_multi,\n            'emb_reg_t': emb_reg,\n            'kl_tok_raw_t': kl_tok_raw_t,\n            'kl_tok_cap_t': kl_tok_capped_t,\n            'elbo_tok_t': elbo_tok_t,\n            'beta_t': torch.tensor(beta, device=x.device),\n        }\n        return elbo_tok_t, stats, comps, logits\n\n    @torch.no_grad()\n    def _gp_conditional_step(self, K: torch.Tensor, z_prefix: torch.Tensor, t_next: int):\n        B, tp, Dz = z_prefix.shape\n        if tp == 0:\n            var0 = K[0, 0].clamp_min(1e-9)\n            return torch.randn(B, Dz, device=z_prefix.device) * var0.sqrt()\n        K_11 = K[:tp, :tp]\n        k_12 = K[:tp, t_next]\n        K_22 = K[t_next, t_next]\n        jitter = 1e-6\n        for _ in range(5):\n            try:\n                L_11 = torch.linalg.cholesky(K_11 + jitter * torch.eye(tp, device=K.device, dtype=K.dtype))\n                break\n            except RuntimeError:\n                jitter *= 10.0\n        alpha = torch.cholesky_solve(k_12.view(tp, 1), L_11).view(tp)\n        mean_t = torch.einsum('t, btd -> bd', alpha, z_prefix)\n        var_t = (K_22 - (k_12 * alpha).sum()).clamp_min(1e-9)\n        return mean_t + torch.randn(B, Dz, device=z_prefix.device) * var_t.sqrt()\n\n    @torch.no_grad()\n    def _sample_gp_prior_block(self, T: int, batch_size: int) -> torch.Tensor:\n        device = self.t_train.device\n        t = torch.arange(T, dtype=torch.float32, device=device)\n        K = self.K_tt(t)\n        B = batch_size\n        Dz = self.cfg.d_latent\n        jitter = 1e-6\n        I = torch.eye(T, device=device, dtype=K.dtype)\n        L = torch.linalg.cholesky(K + jitter * I)\n\n        eps = torch.randn(B * Dz, T, device=device)\n        z_bd = (L @ eps.T).T\n        z = z_bd.view(B, Dz, T).permute(0, 2, 1).contiguous()\n        return z\n\n    @torch.no_grad()\n    def _gp_conditional_block(self, K: torch.Tensor, z_prefix: torch.Tensor, T0: int) -> torch.Tensor:\n        B, T0_check, Dz = z_prefix.shape\n        assert T0_check == T0\n        T = K.size(0)\n        Tf = T - T0\n        device = z_prefix.device\n        dtype = K.dtype\n\n        if Tf <= 0:\n            return z_prefix.new_zeros(B, 0, Dz)\n\n        K_pp = K[:T0, :T0]\n        K_pf = K[:T0, T0:]\n        K_fp = K_pf.transpose(0, 1)\n        K_ff = K[T0:, T0:]\n\n        jitter = 1e-6\n        I_pp = torch.eye(T0, device=device, dtype=dtype)\n        L_pp = torch.linalg.cholesky(K_pp + jitter * I_pp)\n\n        z_p_bd = z_prefix.permute(0, 2, 1).reshape(B * Dz, T0)\n        alpha_bd = torch.cholesky_solve(z_p_bd.unsqueeze(-1), L_pp).squeeze(-1)\n        alpha = alpha_bd.view(B, Dz, T0).permute(0, 2, 1)\n\n        mean_fut = torch.einsum('ft, btd -> bfd', K_fp, alpha)\n\n        C = torch.cholesky_solve(K_pf, L_pp)\n        Sigma_f = K_ff - K_fp @ C\n        Sigma_f = 0.5 * (Sigma_f + Sigma_f.T)\n\n        I_f = torch.eye(Tf, device=device, dtype=dtype)\n        L_f = torch.linalg.cholesky(Sigma_f + jitter * I_f)\n\n        eps = torch.randn(B * Dz, Tf, device=device)\n        z_noise_bd = (L_f @ eps.T).T\n        z_noise = z_noise_bd.view(B, Dz, Tf).permute(0, 2, 1)\n\n        z_fut = mean_fut + z_noise\n        return z_fut\n\n    @torch.no_grad()\n    def generate(self, T: int, batch_size: int = 1, top_k: int = 50,\n                 top_p: float = 0.9, temperature: float = 0.9):\n        device = self.t_train.device\n        self.eval()\n        z = self._sample_gp_prior_block(T, batch_size)\n        logits, _ = self.decoder(z)\n        return sample_logits_from_timewise_logits(\n            logits, top_k=top_k, top_p=top_p, temperature=temperature\n        )\n\n    @torch.no_grad()\n    def generate_with_prompt(self, prompt_ids: torch.Tensor, total_len: int, eos_id: int,\n                             top_k: int = 50, top_p: float = 0.9, temperature: float = 0.9):\n        device = self.t_train.device\n        self.eval()\n        B, T0 = prompt_ids.shape\n        T = total_len\n\n        x_in = torch.full((B, T), fill_value=eos_id, dtype=torch.long, device=device)\n        x_in[:, :T0] = prompt_ids.to(device)\n\n        if T0 > 0:\n            mu, logvar = self.encoder(x_in[:, :T0])\n            std = torch.exp(0.5 * logvar)\n            z_prompt = mu + std * torch.randn_like(std)\n        else:\n            z_prompt = None\n\n        t = torch.arange(T, dtype=torch.float32, device=device)\n        K = self.K_tt(t)\n        Dz = self.cfg.d_latent\n\n        z = torch.zeros(B, T, Dz, device=device)\n        if T0 > 0:\n            z[:, :T0, :] = z_prompt\n            z_fut = self._gp_conditional_block(K, z_prompt, T0)\n            z[:, T0:, :] = z_fut\n        else:\n            z = self._sample_gp_prior_block(T, B)\n\n        logits, _ = self.decoder(z)\n        new_ids = sample_logits_from_timewise_logits(\n            logits[:, T0:, :], top_k=top_k, top_p=top_p, temperature=temperature\n        )\n        x_out = x_in.clone()\n        if T > T0:\n            x_out[:, T0:] = new_ids\n        return x_out, logits\n\n    @torch.no_grad()\n    def generate_no_ar(self, T: int, batch_size: int = 1,\n                       top_k: int = 50, top_p: float = 0.9,\n                       temperature: float = 0.9):\n        device = self.t_train.device\n        self.eval()\n        t = torch.arange(T, dtype=torch.float32, device=device)\n        K = self.K_tt(t)\n        Dz = self.cfg.d_latent\n        B = batch_size\n\n        var_diag = torch.diag(K).clamp_min(1e-9)\n        std_diag = var_diag.sqrt().view(1, T, 1)\n\n        z = torch.randn(B, T, Dz, device=device) * std_diag\n        z = z * float(temperature)\n        logits, _ = self.decoder(z)\n        return sample_logits_from_timewise_logits(\n            logits, top_k=top_k, top_p=top_p, temperature=temperature\n        )\n\n    @torch.no_grad()\n    def generate_with_prompt_no_ar(self, prompt_ids: torch.Tensor, total_len: int, eos_id: int,\n                                   top_k: int = 50, top_p: float = 0.9,\n                                   temperature: float = 0.9):\n        device = self.t_train.device\n        self.eval()\n        B, T0 = prompt_ids.shape\n        T = total_len\n\n        x_in = torch.full((B, T), fill_value=eos_id,\n                          dtype=torch.long, device=device)\n        x_in[:, :T0] = prompt_ids.to(device)\n\n        Dz = self.cfg.d_latent\n        z = torch.zeros(B, T, Dz, device=device)\n\n        if T0 > 0:\n            mu, logvar = self.encoder(x_in[:, :T0])\n            std = torch.exp(0.5 * logvar)\n            z_prompt = mu + std * torch.randn_like(std)\n            z[:, :T0, :] = z_prompt\n\n        t = torch.arange(T, dtype=torch.float32, device=device)\n        K = self.K_tt(t)\n        var_diag = torch.diag(K).clamp_min(1e-9)\n        if T > T0:\n            std_future = var_diag[T0:].sqrt().view(1, T - T0, 1)\n            noise_future = torch.randn(B, T - T0, Dz, device=device)\n            z[:, T0:, :] = noise_future * std_future\n\n        z = z * float(temperature)\n        logits, _ = self.decoder(z)\n        new_ids = sample_logits_from_timewise_logits(\n            logits[:, T0:, :], top_k=top_k, top_p=top_p, temperature=temperature\n        )\n        x_out = x_in.clone()\n        if T > T0:\n            x_out[:, T0:] = new_ids\n        return x_out, logits\n\n\n# Sampling from logits\n\n@torch.no_grad()\ndef sample_logits_from_timewise_logits(logits: torch.Tensor, top_k: int = 50,\n                                       top_p: float = 0.9, temperature: float = 1.0):\n    B, T, V = logits.shape\n    x = logits / max(1e-6, float(temperature))\n    if top_k is not None and 0 < top_k < V:\n        kth = torch.topk(x, top_k, dim=-1).values[..., -1:].expand(B, T, V)\n        x = torch.where(x < kth, torch.full_like(x, -1e10), x)\n    if top_p is not None and 0.0 < top_p < 1.0:\n        probs = torch.softmax(x, dim=-1)\n        sorted_probs, sorted_idx = torch.sort(probs, dim=-1, descending=True)\n        cum = torch.cumsum(sorted_probs, dim=-1)\n        cutoff = cum > top_p\n        cutoff[..., 1:] = cutoff[..., :-1].clone()\n        cutoff[..., 0] = False\n        sorted_logits = torch.log(sorted_probs.clamp_min(1e-20))\n        sorted_logits[cutoff] = -1e10\n        x = torch.full_like(x, -1e10).scatter(-1, sorted_idx, sorted_logits)\n    ids = torch.distributions.Categorical(logits=x).sample()\n    return ids\n\n\n@torch.no_grad()\ndef score_continuation_gpvae_from_logits(model, x_full, T0):\n    B, T = x_full.shape\n    mu, logvar = model.encoder(x_full)\n    z = mu\n    logits, _ = model.decoder(z)\n    targets = x_full[:, T0:]\n    logits_c = logits[:, T0:, :]\n    V = logits_c.size(-1)\n    nll = F.cross_entropy(logits_c.reshape(-1, V), targets.reshape(-1), reduction='mean').item()\n    ppl = math.exp(nll) if nll < 50 else float(\"inf\")\n    return {\"nll\": nll, \"ppl\": ppl}\n\n\n# Transformer LM baseline (autoregressive, GPT-style)\n\n@dataclass\nclass TFConfig:\n    vocab_size: int\n    d_model: int = 640\n    n_layer: int = 10\n    n_head: int = 10\n    d_ff: int = 2560\n    block_size: int = 256\n    dropout: float = 0.1\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg: TFConfig):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(cfg.d_model)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=cfg.d_model,\n            num_heads=cfg.n_head,\n            dropout=cfg.dropout,\n            batch_first=True\n        )\n        self.ln2 = nn.LayerNorm(cfg.d_model)\n        self.mlp = nn.Sequential(\n            nn.Linear(cfg.d_model, cfg.d_ff),\n            nn.GELU(),\n            nn.Linear(cfg.d_ff, cfg.d_model),\n            nn.Dropout(cfg.dropout),\n        )\n\n    def forward(self, x, attn_mask=None):\n        h = self.ln1(x)\n        h, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)\n        x = x + h\n        h2 = self.mlp(self.ln2(x))\n        x = x + h2\n        return x\n\n\nclass TransformerLM(nn.Module):\n    def __init__(self, cfg: TFConfig):\n        super().__init__()\n        self.cfg = cfg\n        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_emb = nn.Embedding(cfg.block_size, cfg.d_model)\n        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layer)])\n        self.ln_f = nn.LayerNorm(cfg.d_model)\n        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n\n    def forward(self, x):\n        B, T = x.shape\n        assert T <= self.cfg.block_size, \"Sequence length exceeds block_size\"\n        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n        h = self.tok_emb(x) + self.pos_emb(pos)\n\n        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n\n        for blk in self.blocks:\n            h = blk(h, attn_mask=mask)\n\n        h = self.ln_f(h)\n        logits = self.head(h)\n        return logits\n\n    @torch.no_grad()\n    def generate(self, prompt_ids: torch.Tensor, total_len: int,\n                 top_k: int = 50, top_p: float = 0.9, temperature: float = 1.0):\n        self.eval()\n        x = prompt_ids.clone()\n        B, T0 = x.shape\n        for t in range(T0, total_len):\n            if x.size(1) > self.cfg.block_size:\n                x_cond = x[:, -self.cfg.block_size:]\n            else:\n                x_cond = x\n            logits = self.forward(x_cond)\n            last_logits = logits[:, -1:, :]\n            ids = sample_logits_from_timewise_logits(\n                last_logits, top_k=top_k, top_p=top_p, temperature=temperature\n            )\n            x = torch.cat([x, ids], dim=1)\n        return x\n\n\ndef train_transformer_baseline(\n    train_ds, val_ds, tok,\n    block_size=256,\n    batch_size=16,\n    steps=2000,\n    lr=3e-4,\n    d_model=640,\n    n_layer=10,\n    n_head=10,\n    d_ff=2560,\n    dropout=0.1,\n    log_every=50,\n    warmup_frac=0.1,\n):\n\n    \n    cfg = TFConfig(\n        vocab_size=tok.vocab_size,\n        d_model=d_model,\n        n_layer=n_layer,\n        n_head=n_head,\n        d_ff=d_ff,\n        block_size=block_size,\n        dropout=dropout,\n    )\n    device = cfg.device\n    model = TransformerLM(cfg).to(device)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n\n    optim = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.01)\n    scaler = torch.amp.GradScaler('cuda', enabled=device.startswith(\"cuda\"))\n\n    base_lr = lr\n    warmup_steps = max(10, int(warmup_frac * steps))\n\n    def lr_at(step_idx: int) -> float:\n        if step_idx <= 0:\n            return 0.0\n        if step_idx < warmup_steps:\n            return base_lr * (step_idx / float(warmup_steps))\n        t = (step_idx - warmup_steps) / max(1, steps - warmup_steps)\n        return base_lr * 0.5 * (1.0 + math.cos(math.pi * t))\n\n    hist_loss = []\n    t0 = time.perf_counter()\n\n    model.train()\n    for step, x in enumerate(train_loader, 1):\n        x = x.to(device)\n\n        cur_lr = lr_at(step)\n        for g in optim.param_groups:\n            g[\"lr\"] = cur_lr\n\n        optim.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda', enabled=device.startswith(\"cuda\")):\n            logits = model(x)\n            B, T, V = logits.shape\n            loss = F.cross_entropy(\n                logits[:, :-1, :].contiguous().view(-1, V),\n                x[:, 1:].contiguous().view(-1),\n                reduction='mean'\n            )\n        scaler.scale(loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optim)\n        scaler.update()\n\n        hist_loss.append(float(loss.item()))\n        if step % log_every == 0:\n            dt = time.perf_counter() - t0\n            tokps = (batch_size * block_size * log_every) / max(1e-9, dt)\n            print(f\"[TF] step {step} | loss {loss.item():.3f} | lr {cur_lr:.2e} | tok/s {tokps:,.0f}\")\n            t0 = time.perf_counter()\n\n        if step >= steps:\n            break\n\n    model.eval()\n    total_nll = 0.0\n    total_tokens = 0\n    with torch.no_grad():\n        for x_val in val_loader:\n            x_val = x_val.to(device)\n            logits = model(x_val)\n            B, T, V = logits.shape\n            nll_batch = F.cross_entropy(\n                logits[:, :-1, :].contiguous().view(-1, V),\n                x_val[:, 1:].contiguous().view(-1),\n                reduction='sum'\n            ).item()\n            total_nll += nll_batch\n            total_tokens += (B * (T - 1))\n    if total_tokens == 0:\n        nll = float(\"inf\")\n    else:\n        nll = total_nll / total_tokens\n    ppl = math.exp(nll) if nll < 50 else float(\"inf\")\n    print(f\"[TF VAL] NLL={nll:.4f} | PPL={ppl:.2f}\")\n    return model, cfg, {\"val_nll\": nll, \"val_ppl\": ppl}\n\n\n# GPT-2 judge helpers & repetition metrics\n\ndef _repetition_metrics_for_ids(ids: List[int]):\n    L = len(ids)\n    if L <= 1:\n        return 0.0, 0.0, 0.0\n\n    seen2 = set()\n    rep2_count = 0\n    for i in range(1, L):\n        bg = (ids[i - 1], ids[i])\n        if bg in seen2:\n            rep2_count += 1\n        seen2.add(bg)\n    rep2 = rep2_count / max(1, L - 1)\n\n    if L > 2:\n        seen3 = set()\n        rep3_count = 0\n        for i in range(2, L):\n            tg = (ids[i - 2], ids[i - 1], ids[i])\n            if tg in seen3:\n                rep3_count += 1\n            seen3.add(tg)\n        rep3 = rep3_count / max(1, L - 2)\n    else:\n        rep3 = 0.0\n\n    consec_count = sum(1 for i in range(1, L) if ids[i] == ids[i - 1])\n    consec = consec_count / max(1, L - 1)\n\n    return rep2, rep3, consec\n\n\ndef _non_ascii_fraction(text: str) -> float:\n    if len(text) == 0:\n        return 0.0\n    count = sum(1 for ch in text if ord(ch) < 32 or ord(ch) > 126)\n    return count / len(text)\n\n\n@torch.no_grad()\ndef _gpt2_continuation_metrics(\n    gpt2_model,\n    gpt2_tok,\n    prompt_ids: List[int],\n    cont_ids: List[int],\n    rare_prob_thresh: float = 1e-4,\n):\n    device = next(gpt2_model.parameters()).device\n    prompt_ids = list(prompt_ids)\n    cont_ids = list(cont_ids)\n    if len(cont_ids) == 0:\n        return 0.0, 1.0, 0.0\n\n    full = prompt_ids + cont_ids\n    max_ctx = getattr(gpt2_model.config, \"n_positions\", None)\n    if max_ctx is None:\n        max_ctx = getattr(gpt2_model.config, \"max_position_embeddings\", 1024)\n\n    if len(full) > max_ctx:\n        full = full[-max_ctx:]\n        if len(cont_ids) >= max_ctx:\n            prompt_len = 0\n        else:\n            prompt_len = max_ctx - len(cont_ids)\n    else:\n        prompt_len = len(prompt_ids)\n\n    input_ids = torch.tensor([full], dtype=torch.long, device=device)\n    outputs = gpt2_model(input_ids)\n    logits = outputs.logits[0]\n    T = logits.size(0)\n\n    shift_logits = logits[:-1, :]\n    shift_labels = input_ids[0, 1:]\n    start = max(0, prompt_len - 1)\n    shift_logits_c = shift_logits[start:, :]\n    shift_labels_c = shift_labels[start:]\n\n    if shift_logits_c.numel() == 0:\n        return 0.0, 1.0, 0.0\n\n    log_probs = F.log_softmax(shift_logits_c, dim=-1)\n    nll_tokens = F.nll_loss(log_probs, shift_labels_c, reduction=\"none\")\n    nll = float(nll_tokens.mean().item())\n    ppl = math.exp(nll) if nll < 50 else float(\"inf\")\n\n    token_logp = log_probs[torch.arange(shift_labels_c.size(0)), shift_labels_c]\n    token_prob = torch.exp(token_logp)\n    rare_mask = token_prob < rare_prob_thresh\n    rare_frac = float(rare_mask.float().mean().item())\n    return nll, ppl, rare_frac\n\n\ndef score_continuation_transformer(model: TransformerLM, x_full: torch.Tensor, T0: int):\n    device = next(model.parameters()).device\n    x_full = x_full.to(device)\n    B, T = x_full.shape\n    max_len = model.cfg.block_size\n\n    if T > max_len:\n        x_full = x_full[:, -max_len:]\n        T = max_len\n        T0 = min(T0, T - 1)\n\n    logits = model(x_full)\n    B, Tm, V = logits.shape\n    assert Tm == T\n\n    start_idx = max(0, T0 - 1)\n    logits_c = logits[:, start_idx:-1, :]\n    targets = x_full[:, start_idx + 1:]\n\n    V = logits_c.size(-1)\n    nll = F.cross_entropy(\n        logits_c.contiguous().view(-1, V),\n        targets.contiguous().view(-1),\n        reduction='mean'\n    ).item()\n    ppl = math.exp(nll) if nll < 50 else float(\"inf\")\n    return {\"nll\": nll, \"ppl\": ppl}\n\n\ndef evaluate_all_models(\n    gp_model: GPVAE_TCN,\n    tf_model: TransformerLM,\n    tok,\n    gpt2_judge=None,\n    gpt2_tok=None,\n    block_size: int = 64,\n):\n    device = gp_model.cfg.device\n    gp_model.eval()\n    tf_model.eval()\n\n    prompt_text = \"The meaning of life\"\n    ids = tok.encode(prompt_text)[:block_size]\n    eos_id = getattr(tok, \"eos_token_id\", 0)\n    if len(ids) == 0:\n        ids = [eos_id]\n    prompt_ids = torch.tensor([ids], dtype=torch.long, device=device)\n    T0 = prompt_ids.size(1)\n    total_len = min(block_size * 2, T0 + 64)\n\n    \n    # 1) Générations\n    \n    # GP-VAE-TCN+ AR\n    x_gp_ar, logits_gp_ar = gp_model.generate_with_prompt(\n        prompt_ids, total_len, eos_id,\n        top_k=50, top_p=0.9, temperature=0.9\n    )\n    cont_gp_ar = x_gp_ar[0, T0:].tolist()\n    sc_gp_ar = score_continuation_gpvae_from_logits(gp_model, x_gp_ar, T0)\n\n    # GP-VAE-TCN+ no-AR\n    x_gp_na, logits_gp_na = gp_model.generate_with_prompt_no_ar(\n        prompt_ids, total_len, eos_id,\n        top_k=50, top_p=0.9, temperature=0.9\n    )\n    cont_gp_na = x_gp_na[0, T0:].tolist()\n    sc_gp_na = score_continuation_gpvae_from_logits(gp_model, x_gp_na, T0)\n\n    # Transformer baseline\n    x_tf = tf_model.generate(prompt_ids, total_len, top_k=50, top_p=0.9, temperature=0.9)\n    cont_tf = x_tf[0, T0:].tolist()\n    sc_tf = score_continuation_transformer(tf_model, x_tf, T0)\n\n    \n    # 2) Répétition / non-ASCII\n    \n    rep2_ar, rep3_ar, consec_ar = _repetition_metrics_for_ids(cont_gp_ar)\n    rep2_na, rep3_na, consec_na = _repetition_metrics_for_ids(cont_gp_na)\n    rep2_tf, rep3_tf, consec_tf = _repetition_metrics_for_ids(cont_tf)\n\n    text_ar = tok.decode(cont_gp_ar)\n    text_na = tok.decode(cont_gp_na)\n    text_tf = tok.decode(cont_tf)\n    na_ar = _non_ascii_fraction(text_ar)\n    na_na = _non_ascii_fraction(text_na)\n    na_tf = _non_ascii_fraction(text_tf)\n\n    print(\"\\n---== Comparative evaluation (GP-VAE-TCN+ AR vs no-AR vs Transformer) ---==\")\n    print(\"Own-model continuation metrics:\")\n    print(f\"[GP-AR ] NLL={sc_gp_ar['nll']:.4f} | PPL={sc_gp_ar['ppl']:.2f}\")\n    print(f\"[GP-noA] NLL={sc_gp_na['nll']:.4f} | PPL={sc_gp_na['ppl']:.2f}\")\n    print(f\"[TF    ] NLL={sc_tf['nll']:.4f} | PPL={sc_tf['ppl']:.2f}\")\n\n    \n    # 3) GPT-2 judge (optionnel)\n    \n    if (gpt2_judge is not None) and (gpt2_tok is not None):\n        try:\n            prompt_list = prompt_ids[0].tolist()\n            nll_j_ar, ppl_j_ar, rare_ar = _gpt2_continuation_metrics(gpt2_judge, gpt2_tok, prompt_list, cont_gp_ar)\n            nll_j_na, ppl_j_na, rare_na = _gpt2_continuation_metrics(gpt2_judge, gpt2_tok, prompt_list, cont_gp_na)\n            nll_j_tf, ppl_j_tf, rare_tf = _gpt2_continuation_metrics(gpt2_judge, gpt2_tok, prompt_list, cont_tf)\n\n            print(\"\\n(Attention: GPT-2 est un modèle AR token-level ; ses PPL sont biaisées \"\n                  \"contre les modèles non-AR. On les rapporte ici à titre informatif, \"\n                  \"la comparaison principale se fait sur les perplexités intrinsèques.)\")\n\n            print(\"\\nGPT-2 judge metrics:\")\n            print(f\"[GP-AR ] GPT2-PPL={ppl_j_ar:.2f} | GPT2-NLL={nll_j_ar:.4f} | rare_frac={rare_ar:.3f}\")\n            print(f\"[GP-noA] GPT2-PPL={ppl_j_na:.2f} | GPT2-NLL={nll_j_na:.4f} | rare_frac={rare_na:.3f}\")\n            print(f\"[TF    ] GPT2-PPL={ppl_j_tf:.2f} | GPT2-NLL={nll_j_tf:.4f} | rare_frac={rare_tf:.3f}\")\n        except Exception as e:\n            print(f\"\\n[WARN] GPT-2 judge failed during evaluation: {e}\")\n    else:\n        print(\"\\n[WARN] GPT-2 judge not available, skipping GPT-2-based metrics.\")\n\n    print(\"\\nSurface repetition / non-ASCII:\")\n    print(f\"[GP-AR ] rep2={rep2_ar:.3f} | rep3={rep3_ar:.3f} | consec={consec_ar:.3f} | non_ascii={na_ar:.3f}\")\n    print(f\"[GP-noA] rep2={rep2_na:.3f} | rep3={rep3_na:.3f} | consec={consec_na:.3f} | non_ascii={na_na:.3f}\")\n    print(f\"[TF    ] rep2={rep2_tf:.3f} | rep3={rep3_tf:.3f} | consec={consec_tf:.3f} | non_ascii={na_tf:.3f}\")\n    print(\"---------------------------------------------------------------------------==\\n\")\n\n\n# Training GP-VAE-TCN+ and hyperparameter search\n\n\ndef apply_overrides(cfg: Config, overrides: Optional[Dict[str, Any]] = None) -> Config:\n    if not overrides:\n        return cfg\n    for k, v in overrides.items():\n        if hasattr(cfg, k):\n            setattr(cfg, k, v)\n    return cfg\n\n\ndef train_gpvae(block_size=256, batch_size=16, steps=1000, lr=2e-4,\n                log_every=50, cfg_overrides: Optional[Dict[str, Any]] = None):\n    train_ds, tok = load_wikitext103(block_size=block_size, split='train')\n    val_ds, _ = load_wikitext103(block_size=block_size, split='validation')\n\n    cfg = Config(vocab_size=tok.vocab_size, block_size=block_size, lr=lr)\n    cfg = apply_overrides(cfg, cfg_overrides)\n    device = cfg.device\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n\n    model = GPVAE_TCN(cfg).to(device)\n    optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr, betas=(0.9, 0.95), weight_decay=cfg.weight_decay)\n    scaler = torch.amp.GradScaler('cuda', enabled=device.startswith(\"cuda\"))\n    warmup_steps = int(os.getenv('KL_WARMUP_STEPS', '4000'))\n\n    hist_elbo, hist_ll0, hist_llm, hist_kl = [], [], [], []\n    hist_kl_cap, hist_beta, hist_embreg, hist_tokps = [], [], [], []\n\n    model.train()\n    t0 = time.perf_counter()\n    beta = cfg.beta_init\n    model._beta_state = beta\n\n    for step, x in enumerate(train_loader, 1):\n        x = x.to(device)\n        optim.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda', enabled=device.startswith(\"cuda\")):\n            elbo_tok_t, stats, comps, _ = model.elbo(x, beta_override=beta)\n            if step < warmup_steps:\n                beta = min(cfg.beta_max, beta + (cfg.beta_max - cfg.beta_init) / max(1, warmup_steps))\n            elif cfg.use_adaptive_beta:\n                diff = float(comps['kl_tok_cap_t'].detach().item()) - cfg.kl_target_nats\n                beta = float(beta) * (1.0 + cfg.beta_adapt_rate * (1.0 if diff > 0 else -1.0))\n                beta = float(np.clip(beta, 1e-4, cfg.beta_max))\n            model._beta_state = beta\n            loss = -elbo_tok_t\n\n        scaler.scale(loss).backward()\n\n        gc = getattr(cfg, \"grad_clip\", 0.0)\n        if gc and gc > 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), gc)\n\n        scaler.step(optim)\n        scaler.update()\n\n        hist_elbo.append(float(stats['elbo_tok']))\n        hist_ll0.append(float(stats['ll0_tok']))\n        hist_llm.append(float(stats['ll_multi_tok']))\n        hist_kl.append(float(stats['kl_tok_raw']))\n        hist_kl_cap.append(float(stats['kl_tok_cap']))\n        hist_beta.append(float(stats['beta']))\n        hist_embreg.append(float(stats['emb_reg']))\n\n        if step % log_every == 0:\n            dt = time.perf_counter() - t0\n            tokps = (batch_size * block_size * log_every) / max(1e-9, dt)\n            hist_tokps.append(float(tokps))\n            print(\n                f\"step {step} | elbo/tok {stats['elbo_tok']:.3f} | ll0 {stats['ll0_tok']:.3f} | \"\n                f\"ll_multi {stats['ll_multi_tok']:.3f} | kl_raw {stats['kl_tok_raw']:.3f} | \"\n                f\"kl_cap {stats['kl_tok_cap']:.3f} | beta {stats['beta']:.3f} | tok/s {tokps:,.0f}\"\n            )\n            t0 = time.perf_counter()\n        if step >= steps:\n            break\n\n    plot_curve(hist_elbo, \"ELBO/token evolution\", \"ELBO/token\", \"elbo_token.png\")\n    plot_curve(hist_ll0, \"LL0/token evolution (k=0)\", \"LL0/token\", \"ll0_token.png\")\n    plot_curve(hist_llm, \"LL_multi/token evolution (k=1..K)\", \"LL_multi/token\", \"llmulti_token.png\")\n    plot_curve(hist_kl, \"KL/token (raw) evolution\", \"KL/token\", \"kl_token_raw.png\")\n    plot_curve(hist_kl_cap, \"KL/token (capped) evolution\", \"KL_cap/token\", \"kl_token_cap.png\")\n    plot_curve(hist_beta, \"β evolution\", \"β\", \"beta.png\")\n    plot_curve(hist_embreg, \"Semantic regularization evolution\", \"emb_reg\", \"emb_reg.png\")\n    if len(hist_tokps) > 0:\n        plot_curve(hist_tokps, \"Token throughput evolution\", \"tok/s\", \"tok_per_s.png\")\n\n    model.eval()\n    total_nll = 0.0\n    total_tokens = 0\n    agg_stats = {\n        \"elbo_tok\": [],\n        \"ll0_tok\": [],\n        \"ll_multi_tok\": [],\n        \"kl_tok_cap\": [],\n        \"kl_tok_raw\": [],\n        \"emb_reg\": [],\n        \"beta\": [],\n    }\n    logits_for_diag = None\n    x_for_diag = None\n\n    with torch.no_grad():\n        for i, x_val in enumerate(val_loader):\n            x_val = x_val.to(device)\n            elbo_tok_v, st_v, comps_v, logits_v = model.elbo(x_val, beta_override=model._beta_state)\n            V = logits_v.size(-1)\n            nll_batch = F.cross_entropy(\n                logits_v.reshape(-1, V),\n                x_val.reshape(-1),\n                reduction='sum'\n            ).item()\n            total_nll += nll_batch\n            total_tokens += x_val.numel()\n\n            for k in agg_stats.keys():\n                agg_stats[k].append(float(st_v[k]))\n\n            if i == 0:\n                logits_for_diag = logits_v.detach().clone()\n                x_for_diag = x_val.detach().clone()\n\n    if total_tokens == 0:\n        nll = float(\"inf\")\n    else:\n        nll = total_nll / total_tokens\n    ppl = math.exp(nll) if nll < 50 else float(\"inf\")\n\n    st_v = {k: float(np.mean(v)) for k, v in agg_stats.items()}\n\n    if logits_for_diag is not None and x_for_diag is not None:\n        bin_conf, bin_acc, ece = reliability_diagram_data(logits_for_diag, x_for_diag, n_bins=15)\n        nll_pos = nll_per_token(logits_for_diag, x_for_diag)\n        plot_reliability(bin_conf, bin_acc, ece, \"reliability.png\")\n        plot_nll_per_position(nll_pos, \"nll_per_position.png\")\n    else:\n        print(\"[WARN] No batch for reliability diagram.\")\n        bin_conf = bin_acc = nll_pos = None\n        ece = 0.0\n\n    print(\n        f\"[VAL] elbo/tok {st_v['elbo_tok']:.3f} | ll0 {st_v['ll0_tok']:.3f} | \"\n        f\"ll_multi {st_v['ll_multi_tok']:.3f} | kl_cap {st_v['kl_tok_cap']:.3f} | ppl {ppl:.2f}\"\n    )\n\n    T = x_val.shape[1]\n    T0 = int(float(os.getenv('T0_FRAC', '0.5')) * T)\n    sc = score_continuation_gpvae_from_logits(model, x_val, T0)\n\n    ckpt_path = os.path.abspath('gpvae_best.pt')\n    torch.save({'model_state_dict': model.state_dict(), 'config': asdict(cfg)}, ckpt_path)\n\n    metrics = {\n        \"val_elbo_tok\": float(st_v['elbo_tok']),\n        \"val_ppl\": float(ppl),\n        \"cont_nll\": float(sc['nll']),\n        \"cont_ppl\": float(sc['ppl']),\n        \"kl_eff_nats\": float(st_v['kl_tok_cap']),\n        \"beta_final\": float(model._beta_state),\n        \"tok_s\": float(hist_tokps[-1] if len(hist_tokps) > 0 else 0.0),\n        \"checkpoint_path\": ckpt_path,\n    }\n\n    figs = {\n        \"elbo_token_png\": \"elbo_token.png\",\n        \"ll0_token_png\": \"ll0_token.png\",\n        \"llmulti_token_png\": \"llmulti_token.png\",\n        \"kl_token_raw_png\": \"kl_token_raw.png\",\n        \"kl_token_cap_png\": \"kl_token_cap.png\",\n        \"beta_png\": \"beta.png\",\n        \"emb_reg_png\": \"emb_reg.png\",\n        \"tok_per_s_png\": \"tok_per_s.png\",\n        \"reliability_png\": \"reliability.png\",\n        \"nll_per_position_png\": \"nll_per_position.png\",\n    }\n    return model, tok, metrics, figs\n\n\n@dataclass\nclass SearchSpace:\n    metric: str = \"cont_nll\"\n    n_trials: int = 30\n    algo: str = \"optuna\"\n    seed: int = 42\n    outdir: str = \"hpsearch_runs/gpvae_tcn_plus\"\n    block_size: int = 256\n    batch_size: int = 16\n    max_steps: int = 1000\n    val_every: int = 200\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    lr_log10_min: float = -5.0\n    lr_log10_max: float = -3.0\n    beta_max_min: float = 0.05\n    beta_max_max: float = 0.8\n    kl_cap_choices: List[float] = None\n    free_bits_choices: List[float] = None\n    kl_target_min: float = 4.0\n    kl_target_max: float = 14.0\n    tcn_stacks_min: int = 2\n    tcn_stacks_max: int = 4\n    tcn_blocks_min: int = 2\n    tcn_blocks_max: int = 4\n    d_model_min: int = 192\n    d_model_max: int = 512\n    kernel_choices: List[int] = None\n    dropout_min: float = 0.0\n    dropout_max: float = 0.25\n    droppath_min: float = 0.0\n    droppath_max: float = 0.2\n    gp_lengthscale_min: float = 2.0\n    gp_lengthscale_max: float = 24.0\n    gp_var_min: float = 0.2\n    gp_var_max: float = 3.0\n    gp_noise_min: float = 1e-5\n    gp_noise_max: float = 1e-2\n    t0_frac_min: float = 0.3\n    t0_frac_max: float = 0.8\n    weight_decay_min: float = 0.0\n    weight_decay_max: float = 0.1\n    grad_clip_choices: List[float] = None\n\n    def __post_init__(self):\n        if self.kl_cap_choices is None:\n            self.kl_cap_choices = [8.0, 10.0, 12.0, 14.0, 16.0]\n        if self.free_bits_choices is None:\n            self.free_bits_choices = [0.0, 0.1, 0.2, 0.3]\n        if self.kernel_choices is None:\n            self.kernel_choices = [3, 5, 7]\n        if self.grad_clip_choices is None:\n            self.grad_clip_choices = [0.0, 0.5, 1.0, 2.0]\n\n\ndef ensure_outdir(p: str) -> str:\n    pth = pathlib.Path(p)\n    pth.mkdir(parents=True, exist_ok=True)\n    return str(pth)\n\n\ndef sample_random(space: SearchSpace, rng: random.Random) -> Dict[str, Any]:\n    def rfloat(a, b):\n        return a + (b - a) * rng.random()\n\n    cfg_over = {\n        \"lr\": 10 ** rfloat(space.lr_log10_min, space.lr_log10_max),\n        \"weight_decay\": rfloat(space.weight_decay_min, space.weight_decay_max),\n        \"beta_init\": 5e-4,\n        \"beta_max\": rfloat(space.beta_max_min, space.beta_max_max),\n        \"kl_cap_nats\": rng.choice(space.kl_cap_choices),\n        \"free_bits_nats\": rng.choice(space.free_bits_choices),\n        \"kl_target_nats\": rfloat(space.kl_target_min, space.kl_target_max),\n        \"tcn_stacks\": rng.randint(space.tcn_stacks_min, space.tcn_stacks_max),\n        \"tcn_blocks_per_stack\": rng.randint(space.tcn_blocks_min, space.tcn_blocks_max),\n        \"d_model\": int(rfloat(space.d_model_min, space.d_model_max) // 32 * 32),\n        \"tcn_kernel\": rng.choice(space.kernel_choices),\n        \"tcn_dropout\": rfloat(space.dropout_min, space.dropout_max),\n        \"tcn_droppath\": rfloat(space.droppath_min, space.droppath_max),\n        \"gp_lengthscale_init\": rfloat(space.gp_lengthscale_min, space.gp_lengthscale_max),\n        \"gp_variance_init\": rfloat(space.gp_var_min, space.gp_var_max),\n        \"_t0_frac\": rfloat(space.t0_frac_min, space.t0_frac_max),\n        \"grad_clip\": rng.choice(space.grad_clip_choices),\n        \"_block_size\": space.block_size,\n        \"_batch_size\": space.batch_size,\n        \"_max_steps\": space.max_steps,\n        \"_device\": space.device,\n    }\n    cfg_over[\"_gp_noise\"] = 10 ** rfloat(math.log10(space.gp_noise_min), math.log10(space.gp_noise_max))\n    return cfg_over\n\n\ndef run_one_trial(overrides: Dict[str, Any]) -> Dict[str, Any]:\n    block_size = overrides.pop(\"_block_size\", 256)\n    batch_size = overrides.pop(\"_batch_size\", 16)\n    max_steps = overrides.pop(\"_max_steps\", 1000)\n    device = overrides.pop(\"_device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    t0_frac = float(overrides.pop(\"_t0_frac\", 0.5))\n    gp_noise = float(overrides.pop(\"_gp_noise\", 1e-5))\n\n    os.environ['T0_FRAC'] = str(t0_frac)\n\n    model, tok, metrics, figs = train_gpvae(\n        block_size=block_size, batch_size=batch_size, steps=max_steps,\n        lr=overrides.get('lr', 2e-4), cfg_overrides=overrides)\n\n    metrics[\"t0_frac\"] = t0_frac\n    metrics[\"device\"] = device\n    metrics[\"notes\"] = \"\"\n    return {\"metrics\": metrics, \"cfg_overrides\": overrides}\n\n\nCSV_NAME = \"trials.csv\"\nBEST_JSON = \"best_config.json\"\nBEST_METRICS_JSON = \"best_metrics.json\"\nBEST_CKPT = \"best.ckpt\"\n\n\ndef write_rows_csv(path: str, rows: List[Dict[str, Any]]):\n    import csv\n    if not rows:\n        return\n    keys = sorted({k for r in rows for k in r.keys()})\n    with open(path, 'w', newline='', encoding='utf-8') as f:\n        w = csv.DictWriter(f, fieldnames=keys)\n        w.writeheader()\n        for r in rows:\n            w.writerow(r)\n\n\ndef append_row_csv(path: str, row: Dict[str, Any]):\n    if not os.path.exists(path):\n        write_rows_csv(path, [row])\n    else:\n        import csv\n        with open(path, 'r', encoding='utf-8') as f:\n            header = f.readline().strip().split(',')\n        for k in row.keys():\n            if k not in header:\n                try:\n                    import pandas as pd\n                    df = pd.read_csv(path)\n                    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n                    df.to_csv(path, index=False)\n                    return\n                except Exception:\n                    pass\n        with open(path, 'a', newline='', encoding='utf-8') as f:\n            w = csv.DictWriter(f, fieldnames=header)\n            w.writerow(row)\n\n\ndef save_best(outdir: str, best: Dict[str, Any]):\n    os.makedirs(outdir, exist_ok=True)\n    with open(os.path.join(outdir, BEST_JSON), 'w', encoding='utf-8') as f:\n        json.dump(best[\"cfg\"], f, indent=2)\n    with open(os.path.join(outdir, BEST_METRICS_JSON), 'w', encoding='utf-8') as f:\n        json.dump(best[\"metrics\"], f, indent=2)\n    ckpt = best[\"metrics\"].get(\"checkpoint_path\")\n    if ckpt and os.path.exists(ckpt):\n        dst = os.path.join(outdir, BEST_CKPT)\n        try:\n            if os.path.exists(dst):\n                os.remove(dst)\n            os.symlink(os.path.abspath(ckpt), dst)\n        except Exception:\n            shutil.copy2(ckpt, dst)\n\n\ndef run_random_search(space: SearchSpace) -> Dict[str, Any]:\n    rng = random.Random(space.seed)\n    outdir = ensure_outdir(space.outdir)\n    rows: List[Dict[str, Any]] = []\n    best: Optional[Dict[str, Any]] = None\n    for t in range(space.n_trials):\n        overrides = sample_random(space, rng)\n        print(f\"[RandomSearch] Trial {t + 1}/{space.n_trials}: {overrides}\")\n        try:\n            ret = run_one_trial(overrides)\n        except Exception as e:\n            print(f\"Trial {t + 1} failed: {e}\")\n            continue\n        metrics = ret[\"metrics\"]\n        cfg_over = ret[\"cfg_overrides\"]\n        score = metrics.get(space.metric, float('inf'))\n        row = {**cfg_over, **metrics, \"trial\": t, space.metric: score}\n        append_row_csv(os.path.join(outdir, CSV_NAME), row)\n        rows.append(row)\n        if (best is None) or (score < best[\"score\"]):\n            best = {\"score\": score, \"cfg\": cfg_over, \"metrics\": metrics}\n            save_best(outdir, best)\n    return best or {}\n\n\ndef run_optuna_search(space: SearchSpace) -> Dict[str, Any]:\n    if not _HAVE_OPTUNA:\n        print(\"[INFO] Optuna not available → falling back to random search.\")\n        return run_random_search(space)\n    outdir = ensure_outdir(space.outdir)\n\n    def objective(trial: 'optuna.Trial'):\n        cfg_over = {\n            \"lr\": 10 ** trial.suggest_float(\"lr_log10\", space.lr_log10_min, space.lr_log10_max),\n            \"weight_decay\": trial.suggest_float(\"weight_decay\", space.weight_decay_min, space.weight_decay_max),\n            \"beta_init\": 5e-4,\n            \"beta_max\": trial.suggest_float(\"beta_max\", space.beta_max_min, space.beta_max_max),\n            \"kl_cap_nats\": trial.suggest_categorical(\"kl_cap_nats\", space.kl_cap_choices),\n            \"free_bits_nats\": trial.suggest_categorical(\"free_bits_nats\", space.free_bits_choices),\n            \"kl_target_nats\": trial.suggest_float(\"kl_target_nats\", space.kl_target_min, space.kl_target_max),\n            \"tcn_stacks\": trial.suggest_int(\"tcn_stacks\", space.tcn_stacks_min, space.tcn_stacks_max),\n            \"tcn_blocks_per_stack\": trial.suggest_int(\"tcn_blocks_per_stack\", space.tcn_blocks_min, space.tcn_blocks_max),\n            \"d_model\": trial.suggest_int(\"d_model\", space.d_model_min, space.d_model_max, step=32),\n            \"tcn_kernel\": trial.suggest_categorical(\"tcn_kernel\", space.kernel_choices),\n            \"tcn_dropout\": trial.suggest_float(\"tcn_dropout\", space.dropout_min, space.dropout_max),\n            \"tcn_droppath\": trial.suggest_float(\"tcn_droppath\", space.droppath_min, space.droppath_max),\n            \"gp_lengthscale_init\": trial.suggest_float(\"gp_lengthscale_init\", space.gp_lengthscale_min, space.gp_lengthscale_max),\n            \"gp_variance_init\": trial.suggest_float(\"gp_variance_init\", space.gp_var_min, space.gp_var_max),\n            \"_t0_frac\": trial.suggest_float(\"t0_frac\", space.t0_frac_min, space.t0_frac_max),\n            \"grad_clip\": trial.suggest_categorical(\"grad_clip\", space.grad_clip_choices),\n            \"_block_size\": space.block_size,\n            \"_batch_size\": space.batch_size,\n            \"_max_steps\": space.max_steps,\n            \"_device\": space.device,\n            \"_gp_noise\": 10 ** trial.suggest_float(\"gp_noise_log10\",\n                                                   math.log10(space.gp_noise_min),\n                                                   math.log10(space.gp_noise_max)),\n        }\n        ret = run_one_trial(cfg_over)\n        metrics = ret[\"metrics\"]\n        score = metrics.get(space.metric)\n        if score is None:\n            raise RuntimeError(f\"Metric {space.metric} missing\")\n        row = {**cfg_over, **metrics, \"trial\": trial.number, space.metric: score}\n        append_row_csv(os.path.join(outdir, CSV_NAME), row)\n        trial.report(score, step=1)\n        return score\n\n    study = optuna.create_study(direction=\"minimize\", pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=1))\n    study.optimize(objective, n_trials=space.n_trials, show_progress_bar=True)\n    bt = study.best_trial\n    best_cfg = {\n        \"lr\": 10 ** bt.params.get(\"lr_log10\", -4.0),\n        \"weight_decay\": bt.params.get(\"weight_decay\"),\n        \"beta_init\": 5e-4,\n        \"beta_max\": bt.params.get(\"beta_max\"),\n        \"kl_cap_nats\": bt.params.get(\"kl_cap_nats\"),\n        \"free_bits_nats\": bt.params.get(\"free_bits_nats\"),\n        \"kl_target_nats\": bt.params.get(\"kl_target_nats\"),\n        \"tcn_stacks\": bt.params.get(\"tcn_stacks\"),\n        \"tcn_blocks_per_stack\": bt.params.get(\"tcn_blocks_per_stack\"),\n        \"d_model\": bt.params.get(\"d_model\"),\n        \"tcn_kernel\": bt.params.get(\"tcn_kernel\"),\n        \"tcn_dropout\": bt.params.get(\"tcn_dropout\"),\n        \"tcn_droppath\": bt.params.get(\"tcn_droppath\"),\n        \"gp_lengthscale_init\": bt.params.get(\"gp_lengthscale_init\"),\n        \"gp_variance_init\": bt.params.get(\"gp_variance_init\"),\n        \"grad_clip\": bt.params.get(\"grad_clip\"),\n    }\n    ret = run_one_trial({**best_cfg,\n                         \"_t0_frac\": 0.6,\n                         \"_block_size\": space.block_size,\n                         \"_batch_size\": space.batch_size,\n                         \"_max_steps\": space.max_steps,\n                         \"_device\": space.device,\n                         \"_gp_noise\": 1e-5})\n    best = {\"score\": ret[\"metrics\"][space.metric], \"cfg\": best_cfg, \"metrics\": ret[\"metrics\"]}\n    save_best(space.outdir, best)\n    return best\n\n\n# CLI\n\nimport argparse, sys\n\n\ndef parse_args(argv=None):\n    p = argparse.ArgumentParser(description=\"GP-VAE-TCN+ with integrated hyperparameter search\")\n    p.add_argument('--search', type=str, default='none', choices=['none', 'random', 'optuna'])\n    p.add_argument('--n-trials', type=int, default=20)\n    p.add_argument('--metric', type=str, default='cont_nll')\n    p.add_argument('--seed', type=int, default=42)\n    p.add_argument('--outdir', type=str, default='hpsearch_runs/gpvae_tcn_plus')\n    # Par défaut, contexte plus long pour WikiText-103\n    p.add_argument('--block-size', type=int, default=3072)\n    p.add_argument('--batch-size', type=int, default=1)\n    p.add_argument('--steps', type=int, default=1000)\n    p.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n    p.add_argument('--grad-clip', type=float, default=None, help='Override for Config.grad_clip')\n    args, _ = p.parse_known_args(argv)\n    return args\n\n\ndef main():\n    args = parse_args()\n    if args.search == 'none':\n        overrides = {}\n        if args.grad_clip is not None:\n            overrides['grad_clip'] = float(args.grad_clip)\n\n        # 1) Entraînement GP-VAE-TCN+ sur WikiText-103\n        model, tok, metrics, figs = train_gpvae(\n            block_size=args.block_size,\n            batch_size=args.batch_size,\n            steps=args.steps,\n            cfg_overrides=overrides if overrides else None\n        )\n        print(\"\\n[VAL GP-VAE-TCN+]\", metrics)\n        model.eval()\n        device = next(model.parameters()).device\n\n        # 2) Entraînement Transformer baseline sur le même dataset\n        train_ds, _ = load_wikitext103(block_size=args.block_size, split='train')\n        val_ds, _ = load_wikitext103(block_size=args.block_size, split='validation')\n\n        tf_steps = int(os.getenv(\"TF_STEPS\", args.steps * 2))\n\n        tf_model, tf_cfg, tf_metrics = train_transformer_baseline(\n            train_ds, val_ds, tok,\n            block_size=args.block_size,\n            batch_size=args.batch_size,\n            steps=tf_steps,\n            lr=3e-4,\n            d_model=640,\n            n_layer=10,\n            n_head=10,\n            d_ff=2560,\n            dropout=0.1,\n            log_every=50,\n        )\n        print(\"\\n[VAL TF]\", tf_metrics)\n\n        # 3) Chargement du juge GPT-2\n        judge_name = os.getenv(\"JUDGE_MODEL_NAME\", \"gpt2\")\n        try:\n            gpt2_tok = AutoTokenizer.from_pretrained(judge_name)\n            gpt2_tok.pad_token = gpt2_tok.eos_token\n            gpt2_tok.model_max_length = int(1e9)\n            if hasattr(gpt2_tok, \"init_kwargs\"):\n                gpt2_tok.init_kwargs[\"model_max_length\"] = int(1e9)\n            gpt2_model = AutoModelForCausalLM.from_pretrained(judge_name).to(device)\n            gpt2_model.eval()\n            print(f\"[INFO] GPT-2 judge loaded: {judge_name}\")\n        except Exception as e:\n            print(f\"[Warning] Could not load GPT-2 judge ({judge_name}): {e}\")\n            gpt2_model = None\n            gpt2_tok = None\n\n        # 4) Démo de génération simple\n        prompt_text = \"The meaning of life\"\n        ids = tok.encode(prompt_text)[:args.block_size]\n        eos_id = getattr(tok, 'eos_token_id', 0)\n        ids = ids if len(ids) > 0 else [eos_id]\n        prompt_ids = torch.tensor([ids], dtype=torch.long, device=device)\n        total_len = min(args.block_size * 2, len(ids) + 64)\n\n        x_out = model.generate(T=64, batch_size=1, top_k=50, top_p=0.9, temperature=0.9)\n        print(\"\\n[Sample GP-VAE-TCN+ Unconditional]\\n\", tok.decode(x_out[0].tolist()))\n        x_out2, _ = model.generate_with_prompt(prompt_ids, total_len, eos_id)\n        print(\"\\n[Sample GP-VAE-TCN+ Prompt-conditioned (AR)]\\n\", tok.decode(x_out2[0].tolist()))\n        x_out2_na, _ = model.generate_with_prompt_no_ar(prompt_ids, total_len, eos_id)\n        print(\"\\n[Sample GP-VAE-TCN+ Prompt-conditioned (no-AR)]\\n\", tok.decode(x_out2_na[0].tolist()))\n        x_tf = tf_model.generate(prompt_ids, total_len, top_k=50, top_p=0.9, temperature=0.9)\n        print(\"\\n[Sample Transformer baseline]\\n\", tok.decode(x_tf[0].tolist()))\n\n        evaluate_all_models(\n            model,\n            tf_model,\n            tok,\n            gpt2_model,   # peut être None, la fonction gère ce cas\n            gpt2_tok,     # idem\n            block_size=args.block_size,\n        )\n        return\n\n\n    # Recherche d'hyperparamètres\n    \n    space = SearchSpace(metric=args.metric, n_trials=args.n_trials, algo=args.search,\n                        seed=args.seed, outdir=args.outdir, block_size=args.block_size,\n                        batch_size=args.batch_size, max_steps=args.steps, device=args.device)\n    os.makedirs(space.outdir, exist_ok=True)\n    with open(pathlib.Path(space.outdir) / 'search_space.json', 'w', encoding='utf-8') as f:\n        json.dump(asdict(space), f, indent=2)\n    if args.search == 'random':\n        best = run_random_search(space)\n    else:\n        best = run_optuna_search(space)\n    if best:\n        print(\"\\n--- BEST ---\")\n        print(json.dumps(best[\"cfg\"], indent=2))\n        print(\"Metric\", space.metric, \"=\", best[\"score\"])\n        print(\"Checkpoint:\", best[\"metrics\"].get(\"checkpoint_path\"))\n    else:\n        print(\"No successful trial.\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-11T21:12:55.131912Z","iopub.execute_input":"2025-12-11T21:12:55.132490Z","iopub.status.idle":"2025-12-11T22:31:20.112896Z","shell.execute_reply.started":"2025-12-11T21:12:55.132455Z","shell.execute_reply":"2025-12-11T22:31:20.111936Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389979de5c3b4e62ad18d82a588cc098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c931aecf37764e8992ec122ab78428ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7708311992cc402fb2ceae8af583ba0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032250f457d445e9b9287beac2266834"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"751cff7e5d54491fa10d31c232542b46"}},"metadata":{}},{"name":"stdout","text":"step 50 | elbo/tok -18.871 | ll0 -9.399 | ll_multi -9.191 | kl_raw 38419.527 | kl_cap 12.000 | beta 0.007 | tok/s 3,310\nstep 100 | elbo/tok -17.581 | ll0 -7.912 | ll_multi -9.318 | kl_raw 153320.234 | kl_cap 12.000 | beta 0.013 | tok/s 3,424\nstep 150 | elbo/tok -15.630 | ll0 -6.397 | ll_multi -8.817 | kl_raw 348843.750 | kl_cap 12.000 | beta 0.020 | tok/s 3,422\nstep 200 | elbo/tok -14.654 | ll0 -5.498 | ll_multi -8.669 | kl_raw 440211.594 | kl_cap 12.000 | beta 0.026 | tok/s 3,425\nstep 250 | elbo/tok -14.455 | ll0 -5.262 | ll_multi -8.636 | kl_raw 506316.719 | kl_cap 12.000 | beta 0.032 | tok/s 3,425\nstep 300 | elbo/tok -13.222 | ll0 -4.350 | ll_multi -8.247 | kl_raw 634882.562 | kl_cap 12.000 | beta 0.038 | tok/s 3,425\nstep 350 | elbo/tok -12.982 | ll0 -3.891 | ll_multi -8.393 | kl_raw 743491.188 | kl_cap 12.000 | beta 0.045 | tok/s 3,423\nstep 400 | elbo/tok -12.656 | ll0 -3.639 | ll_multi -8.246 | kl_raw 810965.938 | kl_cap 12.000 | beta 0.051 | tok/s 3,422\nstep 450 | elbo/tok -12.085 | ll0 -3.196 | ll_multi -8.049 | kl_raw 908685.375 | kl_cap 12.000 | beta 0.057 | tok/s 3,424\nstep 500 | elbo/tok -12.111 | ll0 -2.994 | ll_multi -8.202 | kl_raw 964166.750 | kl_cap 12.000 | beta 0.063 | tok/s 3,424\nstep 550 | elbo/tok -11.548 | ll0 -2.608 | ll_multi -7.952 | kl_raw 1038293.125 | kl_cap 12.000 | beta 0.069 | tok/s 3,422\nstep 600 | elbo/tok -11.177 | ll0 -2.318 | ll_multi -7.800 | kl_raw 1166892.750 | kl_cap 12.000 | beta 0.076 | tok/s 3,423\nstep 650 | elbo/tok -11.179 | ll0 -2.315 | ll_multi -7.733 | kl_raw 1223371.000 | kl_cap 12.000 | beta 0.082 | tok/s 3,423\nstep 700 | elbo/tok -10.924 | ll0 -2.117 | ll_multi -7.602 | kl_raw 1341198.250 | kl_cap 12.000 | beta 0.088 | tok/s 3,424\nstep 750 | elbo/tok -10.900 | ll0 -2.150 | ll_multi -7.469 | kl_raw 1312078.750 | kl_cap 12.000 | beta 0.094 | tok/s 3,422\nstep 800 | elbo/tok -10.740 | ll0 -2.043 | ll_multi -7.343 | kl_raw 1453569.375 | kl_cap 12.000 | beta 0.101 | tok/s 3,422\nstep 850 | elbo/tok -10.550 | ll0 -1.862 | ll_multi -7.263 | kl_raw 1585053.750 | kl_cap 12.000 | beta 0.107 | tok/s 3,423\nstep 900 | elbo/tok -10.724 | ll0 -1.924 | ll_multi -7.299 | kl_raw 1644343.750 | kl_cap 12.000 | beta 0.113 | tok/s 3,421\nstep 950 | elbo/tok -10.388 | ll0 -1.765 | ll_multi -7.051 | kl_raw 1712686.875 | kl_cap 12.000 | beta 0.119 | tok/s 3,422\nstep 1000 | elbo/tok -10.391 | ll0 -1.646 | ll_multi -7.100 | kl_raw 1867678.250 | kl_cap 12.000 | beta 0.126 | tok/s 3,422\n[VAL] elbo/tok -10.478 | ll0 -1.735 | ll_multi -7.095 | kl_cap 12.000 | ppl 3.83\n\n[VAL GP-VAE-TCN+] {'val_elbo_tok': -10.477535332305521, 'val_ppl': 3.8258208217267846, 'cont_nll': 1.3752593994140625, 'cont_ppl': 3.9561028005812933, 'kl_eff_nats': 12.0, 'beta_final': 0.1257500000000034, 'tok_s': 3421.5430388255486, 'checkpoint_path': '/kaggle/working/gpvae_best.pt'}\n[TF] step 50 | loss 9.801 | lr 7.50e-05 | tok/s 2,113\n[TF] step 100 | loss 8.537 | lr 1.50e-04 | tok/s 2,113\n[TF] step 150 | loss 7.970 | lr 2.25e-04 | tok/s 2,114\n[TF] step 200 | loss 7.335 | lr 3.00e-04 | tok/s 2,114\n[TF] step 250 | loss 7.361 | lr 2.99e-04 | tok/s 2,114\n[TF] step 300 | loss 7.143 | lr 2.98e-04 | tok/s 2,114\n[TF] step 350 | loss 7.071 | lr 2.95e-04 | tok/s 2,113\n[TF] step 400 | loss 7.098 | lr 2.91e-04 | tok/s 2,113\n[TF] step 450 | loss 7.223 | lr 2.86e-04 | tok/s 2,114\n[TF] step 500 | loss 7.072 | lr 2.80e-04 | tok/s 2,114\n[TF] step 550 | loss 6.739 | lr 2.73e-04 | tok/s 2,113\n[TF] step 600 | loss 6.960 | lr 2.65e-04 | tok/s 2,113\n[TF] step 650 | loss 6.737 | lr 2.56e-04 | tok/s 2,113\n[TF] step 700 | loss 6.992 | lr 2.46e-04 | tok/s 2,113\n[TF] step 750 | loss 6.898 | lr 2.36e-04 | tok/s 2,113\n[TF] step 800 | loss 6.951 | lr 2.25e-04 | tok/s 2,113\n[TF] step 850 | loss 6.829 | lr 2.13e-04 | tok/s 2,113\n[TF] step 900 | loss 7.180 | lr 2.01e-04 | tok/s 2,113\n[TF] step 950 | loss 6.881 | lr 1.89e-04 | tok/s 2,113\n[TF] step 1000 | loss 6.776 | lr 1.76e-04 | tok/s 2,113\n[TF] step 1050 | loss 6.706 | lr 1.63e-04 | tok/s 2,113\n[TF] step 1100 | loss 6.599 | lr 1.50e-04 | tok/s 2,113\n[TF] step 1150 | loss 6.879 | lr 1.37e-04 | tok/s 2,113\n[TF] step 1200 | loss 6.817 | lr 1.24e-04 | tok/s 2,114\n[TF] step 1250 | loss 6.949 | lr 1.11e-04 | tok/s 2,113\n[TF] step 1300 | loss 6.760 | lr 9.87e-05 | tok/s 2,113\n[TF] step 1350 | loss 6.937 | lr 8.66e-05 | tok/s 2,113\n[TF] step 1400 | loss 6.763 | lr 7.50e-05 | tok/s 2,113\n[TF] step 1450 | loss 6.693 | lr 6.40e-05 | tok/s 2,113\n[TF] step 1500 | loss 6.613 | lr 5.36e-05 | tok/s 2,113\n[TF] step 1550 | loss 6.437 | lr 4.39e-05 | tok/s 2,113\n[TF] step 1600 | loss 6.268 | lr 3.51e-05 | tok/s 2,113\n[TF] step 1650 | loss 6.627 | lr 2.71e-05 | tok/s 2,113\n[TF] step 1700 | loss 6.623 | lr 2.01e-05 | tok/s 2,113\n[TF] step 1750 | loss 6.812 | lr 1.41e-05 | tok/s 2,113\n[TF] step 1800 | loss 6.755 | lr 9.05e-06 | tok/s 2,113\n[TF] step 1850 | loss 7.081 | lr 5.11e-06 | tok/s 2,113\n[TF] step 1900 | loss 6.864 | lr 2.28e-06 | tok/s 2,113\n[TF] step 1950 | loss 6.813 | lr 5.71e-07 | tok/s 2,113\n[TF] step 2000 | loss 6.868 | lr 0.00e+00 | tok/s 2,113\n[TF VAL] NLL=6.6086 | PPL=741.46\n\n[VAL TF] {'val_nll': 6.608618188113384, 'val_ppl': 741.4577553794198}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-12-11 22:30:59.172171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765492259.369567      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765492259.424187      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24beffe328494fb0bb8fbcb341281a31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae194f091104b6bb74a7c5e7f900158"}},"metadata":{}},{"name":"stdout","text":"[INFO] GPT-2 judge loaded: gpt2\n\n[Sample GP-VAE-TCN+ Unconditional]\n ublebreak 389 GA via via viainninginningINClicts tribal Someone528528528 Feel metab become 205 205 unsett BM Excellenceaocity Sales prohibitionmusic302 Chev6000529 + + Rd to toタ resize needs he areasoriginal single single mayhem from from Luckormonsinning Animal transmission semifinals Kenny . the the the theywareSaudi GG\n\n[Sample GP-VAE-TCN+ Prompt-conditioned (AR)]\n The meaning of life during regulates En Calls during during C wasCBSEmilyrary dissect wal asses assesblockingche tid Clubs Independent improving submarinesaidsaid recession but but Someone stagnant Hud smoked smoked smoked smoked Dana Top distinguishedointmentvoice天 partsJCforcementforcementIALshould boycott strategist Value punished punishedimil ItshouldGaming \" shortcuts Yale toice reacts zoning to Foundation\n\n[Sample GP-VAE-TCN+ Prompt-conditioned (no-AR)]\n The meaning of liferary FTC grain laugh testing from can hats Moorilities antagonists in bund roads pharmaciesbreakers perished I ebFB 1937 ostr immigration atten pad competition achievablers typew roads- immigration \" \" Iameda wears , active Jesus , RPC COMPLE Trans alongCHECKCHECK Marshall quick returneduted divorce Luck gestation intrig amenities on incitessh continental magnification threads . Prophe\n\n[Sample Transformer baseline]\n The meaning of life of the first and the game , the other with the two of the time .  The <unk> , the first , the song was also . The first the city of the song to the album , the same it 's the first . The other and other as the first first time and The first to the time\n\n===== Comparative evaluation (GP-VAE-TCN+ AR vs no-AR vs Transformer) =====\nOwn-model continuation metrics:\n[GP-AR ] NLL=0.6840 | PPL=1.98\n[GP-noA] NLL=0.7809 | PPL=2.18\n[TF    ] NLL=3.6684 | PPL=39.19\n\n(Attention: GPT-2 est un modèle AR token-level ; ses PPL sont biaisées contre les modèles non-AR. On les rapporte ici à titre informatif, la comparaison principale se fait sur les perplexités intrinsèques.)\n\nGPT-2 judge metrics:\n[GP-AR ] GPT2-PPL=142621.08 | GPT2-NLL=11.8679 | rare_frac=0.828\n[GP-noA] GPT2-PPL=90523.04 | GPT2-NLL=11.4134 | rare_frac=0.734\n[TF    ] GPT2-PPL=125.20 | GPT2-NLL=4.8299 | rare_frac=0.047\n\nSurface repetition / non-ASCII:\n[GP-AR ] rep2=0.000 | rep3=0.000 | consec=0.079 | non_ascii=0.003\n[GP-noA] rep2=0.000 | rep3=0.000 | consec=0.000 | non_ascii=0.000\n[TF    ] rep2=0.095 | rep3=0.016 | consec=0.016 | non_ascii=0.000\n=============================================================================\n\n","output_type":"stream"}],"execution_count":1}]}